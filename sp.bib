@article{Guan2015,
abstract = {—Cryptography plays an important role in comput-er and communication security. In practical implementations of cryptosystems, the cryptographic keys are usually loaded into the memory as plaintext, and then used in the cryptographic algorithms. Therefore, the private keys are subject to memory disclosure attacks that read unauthorized data from RAM. Such attacks could be performed through software methods (e.g., OpenSSL Heartbleed) even when the integrity of the victim system's executable binaries is maintained. They could also be performed through physical methods (e.g., cold-boot attacks on RAM chips) even when the system is free of software vulnerabilities. In this paper, we propose Mimosa that protects RSA private keys against the above software-based and physical memory attacks. When the Mimosa service is in idle, private keys are encrypted and reside in memory as ciphertext. During the cryptographic computing, Mimosa uses hardware transactional memory (HTM) to ensure that (a) whenever a malicious process other than Mimosa attempts to read the plaintext private key, the transaction aborts and all sensitive data are automatically cleared with hardware mechanisms, due to the strong atomicity guarantee of HTM; and (b) all sensitive data, including private keys and intermediate states, appear as plaintext only within CPU-bound caches, and are never loaded to RAM chips. To the best of our knowledge, Mimosa is the first solution to use transactional memory to protect sensitive data against memory disclosure attacks. We have implemented Mimosa on a commodity machine with Intel Core i7 Haswell CPUs. Through extensive experiments, we show that Mimosa effectively pro-tects cryptographic keys against various attacks that attempt to read sensitive data from memory, and it only introduces a small performance overhead.},
author = {Guan, Le and Lin, Jingqiang and Luo, Bo and Jing, Jiwu and Wang, Jing},
doi = {10.1109/SP.2015.8},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
pages = {3--19},
title = {{Protecting Private Keys against Memory Disclosure Attacks using Hardware Transactional Memory}},
year = {2015}
}
@article{Watson2015,
abstract = {—CHERI extends a conventional RISC Instruction-Set Architecture, compiler, and operating system to support fine-grained, capability-based memory protection to mitigate memory-related vulnerabilities in C-language TCBs. We describe how CHERI capabilities can also underpin a hardware-software object-capability model for application compartmentalization that can mitigate broader classes of attack. Prototyped as an extension to the open-source 64-bit BERI RISC FPGA soft-core processor, FreeBSD operating system, and LLVM compiler, we demonstrate multiple orders-of-magnitude improvement in scalability, simplified programmability, and resulting tangible security benefits as compared to compartmentalization based on pure Memory-Management Unit (MMU) designs. We evaluate incrementally deployable CHERI-based compartmentalization using several real-world UNIX libraries and applications.},
author = {Watson, Robert N M and Woodruff, Jonathan and Neumann, Peter G and Moore, Simon W and Anderson, Jonathan and Chisnall, David and Dave, Nirav and Davis, Brooks and Gudka, Khilan and Laurie, Ben and Murdoch, Steven J and Norton, Robert and Roe, Michael and Son, Stacey and Vadera, Munraj},
doi = {10.1109/SP.2015.9},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
pages = {20--36},
title = {{CHERI: A Hybrid Capability-System Architecture for Scalable Software Compartmentalization}},
year = {2015}
}
@article{Schuster2015,
author = {Schuster, F and Costa, M},
journal = {s{\&}p},
title = {{VC3: Trustworthy data analytics in the cloud}},
year = {2015}
}
@article{Ulrich2015,
author = {Ulrich, R and Xu, Xiaolin and Kraeh, Christian and Hilgers, Christian and Kononchuk, Dima and Finley, Jonathan J and Burleson, Wayne P},
doi = {10.1109/SP.2015.12},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {-virtual proofs,able functions,cryptography,interactive proof systems,keyless security sen-,of reality,physical cryptography,physical unclon-,physical zero-knowledge proofs,pufs,quantum,sors,vps},
title = {{Virtual Proofs of Reality and their Physical Implementation}},
year = {2015}
}
@article{Zhang2015,
author = {Zhang, Fengwei and Leach, Kevin and Stavrou, Angelos and Wang, Haining and Sun, Kun},
doi = {10.1109/SP.2015.11},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {-malware debugging,smm,transparency},
number = {Vm},
title = {{Using Hardware Features for Increased Debugging Transparency}},
year = {2015}
}
@article{Kanich2015,
author = {Kanich, Chris},
doi = {10.1109/SP.2015.16},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
title = {{Every Second Counts : Quantifying the Negative Externalities of Cybercrime via Typosquatting}},
year = {2015}
}
@article{Bonneau2015,
abstract = {Bitcoin has emerged as the most successful cryptographic currency in history. Within two years of its quiet launch in 2009, Bitcoin grew to comprise billions of dollars of economic value despite only cursory analysis of the system’s design. Since then a growing literature has identified hidden-but-important properties of the system, discovered attacks, proposed promising alternatives, and singled out difficult future challenges. Meanwhile a large and vibrant open-source community has proposed and deployed numerous modifications and extensions. We provide the first systematic exposition Bitcoin and the many related cryptocurrencies or ‘altcoins.’ Drawing from a scattered body of knowledge, we identify three key components of Bitcoin’s design that can be decoupled. This enables a more insightful analysis of Bitcoin’s properties and future stability. We map the design space for numerous proposed modifications, providing comparative analyses for alternative consensus mechanisms, currency allocation mechanisms, computational puzzles, and key management tools. We survey anonymity issues in Bitcoin and provide an evaluation framework for analyzing a variety of privacy-enhancing proposals. Finally we provide new insights on what we term disintermediation protocols, which absolve the need for trusted intermediaries in an interesting set of applications. We identify three general disintermediation strategies and provide a detailed comparison.},
author = {Bonneau, Joseph (Stanford University) and Miller, Andrew (University of Maryland) and Clark, Jeremy (Concordia University) and Narayanan, Arvind (Princeton University) and Kroll, Joshua (Princeton University) and Felten, Edward (Princeton University)},
doi = {10.1109/SP.2015.14},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {bitcoin,cryptocurrency},
title = {{SoK : Research Perspectives and Challenges for Bitcoin and Cryptocurrencies}},
year = {2015}
}
@article{Eyal2015,
archivePrefix = {arXiv},
arxivId = {1411.7099},
author = {Eyal, I},
doi = {10.1109/SP.2015.13},
eprint = {1411.7099},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {data mining;distributed processing;financial data},
pages = {89--103},
title = {{The Miner's Dilemma}},
year = {2015}
}
@article{Biryukov2015,
abstract = {Bitcoin is a decentralized P2P digital currency in which coins are generated by a distributed set of miners and transaction are broadcasted via a peer-to-peer network. While Bitcoin provides some level of anonymity (or rather pseudonymity) by encouraging the users to have any number of random-looking Bitcoin addresses, recent research shows that this level of anonymity is rather low. This encourages users to connect to the Bitcoin network through anonymizers like Tor and motivates development of default Tor functionality for popular mobile SPV clients. In this paper we show that combining Tor and Bitcoin creates an attack vector for the deterministic and stealthy man-in-the-middle attacks. A low-resource attacker can gain full control of information flows between all users who chose to use Bitcoin over Tor. In particular the attacker can link together user's transactions regardless of pseudonyms used, control which Bitcoin blocks and transactions are relayed to the user and can $\backslash$ delay or discard user's transactions and blocks. In collusion with a powerful miner double-spending attacks become possible and a totally virtual Bitcoin reality can be created for such set of users. Moreover, we show how an attacker can fingerprint users and then recognize them and learn their IP address when they decide to connect to the Bitcoin network directly.},
archivePrefix = {arXiv},
arxivId = {1410.6079},
author = {Biryukov, Alex and Pustogarov, Ivan},
doi = {10.1109/SP.2015.15},
eprint = {1410.6079},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
pages = {11},
title = {{Bitcoin over Tor isn't a good idea}},
url = {http://arxiv.org/abs/1410.6079},
year = {2015}
}
@article{Thomas2015,
author = {Thomas, Kurt and Bursztein, Elie and Grier, Chris and Ho, Grant and Jagpal, Nav and Kapravelos, Alexandros and Mccoy, Damon and Nappa, Antonio and Paxson, Vern and Pearce, Paul and Provos, Niels and Rajab, Moheeb Abu},
doi = {10.1109/SP.2015.17},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
title = {{Ad Injection at Scale: Assessing Deceptive Advertisement Modifications}},
url = {http://research.google.com/pubs/pub43346.html$\backslash$nhttps://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43346.pdf},
year = {2015}
}
@article{Zhu2015,
author = {Zhu, Liang and Hu, Zi and Heidemann, John and Wessels, Duane and Mankin, Allison and Somaiya, Nikita},
doi = {10.1109/SP.2015.18},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
title = {{Connection-Oriented DNS to Improve Privacy and Security}},
year = {2015}
}
@article{Unger2015,
author = {Unger, N and Dechand, S and Bonneau, J and Fahl, S and Perl, H and Goldberg, I and Smith, M},
doi = {10.1109/SP.2015.22},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {cryptography;data privacy;message passing;SoK;conv},
pages = {232--249},
title = {{SoK: Secure Messaging}},
year = {2015}
}
@article{Rasti2015,
author = {Rasti, Ryan and Murthy, Mukul and Weaver, Nicholas and Paxson, Vern},
doi = {10.1109/SP.2015.19},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
pages = {187--198},
title = {{Temporal Lensing and its Application in Pulsing Denial-of-Service Attacks}},
year = {2015}
}
@article{Lychev2015,
author = {Lychev, Robert and Jero, Samuel and Boldyreva, Alexandra and Nita-rotaru, Cristina},
doi = {10.1109/SP.2015.21},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
pages = {214--231},
title = {{How Secure and Quick is QUIC ? Provable Security and Performance Analyses}},
year = {2015}
}
@article{Sch2015,
author = {Sch, Matthias and Schmitt, Jens},
doi = {10.1109/SP.2015.20},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
title = {{Secure Track Verification}},
year = {2015}
}
@article{Corrigan-Gibbs2015,
abstract = {This paper presents Riposte, a new system for anonymous broadcast messaging. Riposte is the first such system, to our knowledge, that simultaneously protects against traffic-analysis attacks, prevents anonymous denial-of-service by malicious clients, and scales to million-user anonymity sets. To achieve these properties, Riposte makes novel use of techniques used in systems for private information retrieval and secure multi-party computation. For latency-tolerant workloads with many more readers than writers (e.g. Twitter, Wikileaks), we demonstrate that a three-server Riposte cluster can build an anonymity set of 2,895,216 users in 32 hours.},
archivePrefix = {arXiv},
arxivId = {1503.06115},
author = {Corrigan-Gibbs, Henry and Boneh, Dan and Mazieres, David},
doi = {10.1109/SP.2015.27},
eprint = {1503.06115},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
pages = {321--338},
title = {{Riposte: An Anonymous Messaging System Handling Millions of Users}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7163034},
year = {2015}
}
@article{Costello2015,
author = {Costello, C and Fournet, C and Howell, J and Kohlweiss, M and Kreuter, B and Naehrig, M and Parno, B and Zahur, S},
doi = {10.1109/SP.2015.23},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {cloud computing;computer bootstrapping;cryptograph},
pages = {253--270},
title = {{Geppetto: Versatile Verifiable Computation}},
year = {2015}
}
@article{Backes2015,
abstract = {We study the problem of privacy-preserving proofs on authenticated data, where a party receives data from a trusted source and is requested to prove computations over the data to third parties in a correct and private way, i.e., the third party learns no information on the data but is still assured that the claimed proof is valid. Our work particularly focuses on the challenging requirement that the third party should be able to verify the validity with respect to the specific data authenticated by the source — even without having access to that source. This problem is motivated by various scenarios emerging from several application areas such as wearable computing, smart metering, or general business-to-business interactions. Furthermore, these applications also demand any meaningful solution to satisfy additional properties related to usability and scalability.In this paper, we formalize the above three-party model, discuss concrete application scenarios, and then we design, build, and evaluate ADSNARK, a nearly practical system for proving arbitrary computations over authenticated data in a privacy-preserving manner. ADSNARK improves significantly over state-of-the-art solutions for this model. For instance, compared to corresponding solutions based on Pinocchio (Oakland’13), ADSNARK achieves up to 25× improvement in proof computation time and a 20× reduction in prover storage space.},
author = {Backes, Michael and Fiore, Dario and Reischuk, Raphael M},
doi = {10.1109/SP.2015.24},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
title = {{ADSNARK: Nearly Practical and Privacy-Preserving Proofs on Authenticated Data}},
volume = {617},
year = {2015}
}
@article{Ben-Sasson2015,
author = {Ben-Sasson, Eli and Chiesa, Alessandro and Green, Matthew and Tromer, Eran and Virza, Madars},
doi = {10.1109/SP.2015.25},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
pages = {287--304},
title = {{Secure Sampling of Public Parameters for Succinct Zero Knowledge Proofs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7163032},
year = {2015}
}
@article{Green2015,
author = {Green, Matthew D and Miers, Ian},
doi = {10.1109/SP.2015.26},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
pages = {305--320},
title = {{Forward Secure Asynchronous Messaging from Puncturable Encryption}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7163033},
year = {2015}
}
@article{Maffei2015,
author = {Maffei, Matteo and Malavolta, Giulio and Reinert, Manuel and Schr, Dominique},
doi = {10.1109/SP.2015.28},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
pages = {341--358},
title = {{Privacy and Access Control for Outsourced Personal Records}},
year = {2015}
}
@article{Songhori2015,
author = {Songhori, Ebrahim M and Hussain, Siam U and Sadeghi, Ahmad-Reza and Schneider, Thomas and Koushanfar, Farinaz},
doi = {10.1109/SP.2015.32},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {circuits,cryptography,garbled,mpc,tinygarble,yao},
title = {{TinyGarble: Highly Compressed and Scalable Sequential Garbled Circuits}},
url = {http://thomaschneider.de/papers/SHSSK15.pdf},
year = {2015}
}
@article{Nayak2015,
author = {Nayak, Kartik and Wang, Xiao Shaun and Ioannidis, Stratis and Weinsberg, Udi and Taft, Nina and Shi, Elaine},
doi = {10.1109/SP.2015.30},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
title = {{GraphSC: Parallel Secure Computation Made Easy}},
year = {2015}
}
@article{Fisch2015,
author = {Fisch, Ben A and Vo, Binh and Krell, Fernando and Kumarasubramanian, Abishek and Kolesnikov, Vladimir and Malkin, Tal and Bellovin, Steven M},
doi = {10.1109/SP.2015.31},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
title = {{Malicious-Client Security in Blind Seer : A Scalable Private DBMS}},
year = {2015}
}
@article{Liu2015,
author = {Liu, Chang and Wang, Xiao Shaun and Nayak, Kartik and Huang, Yan and Shi, Elaine},
doi = {10.1109/SP.2015.29},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
pages = {359--376},
title = {{ObliVM : A Programming Framework for Secure Computation}},
year = {2015}
}
@article{Vilk2015,
author = {Vilk, John and Molnar, David and Livshits, Benjamin and Ofek, Eyal and Rossbach, Chris and Moshchuk, Alexander and Wang, Helen J and Gal, Ran},
doi = {10.1109/SP.2015.33},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
title = {{SurroundWeb : Mitigating Privacy Concerns in a 3D Web Browser}},
year = {2015}
}
@article{Cao2015,
author = {Cao, Yinzhi and Yang, Junfeng},
doi = {10.1109/SP.2015.35},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
title = {{Towards Making Systems Forget with Machine Unlearning}},
year = {2015}
}
@article{Huang2015,
author = {Huang, Zhicong and Ayday, Erman and Fellay, Jacques and Hubaux, Jean-Pierre and Juels, Ari},
doi = {10.1109/SP.2015.34},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
pages = {447--462},
title = {{GenoGuard: Protecting genomic data against brute-force attacks}},
year = {2015}
}
@article{Bernhard2015,
author = {Bernhard, David and Cortier, Veronique and Galindo, David and Pereira, Olivier and Warinschi, Bogdan},
doi = {10.1109/SP.2015.37},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
pages = {499--516},
title = {{SoK: A Comprehensive Analysis of Game-Based Ballot Privacy Definitions}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7163044},
year = {2015}
}
@article{Chatterjee2015,
author = {Chatterjee, Rahul and Bonneau, Joseph and Juels, Ari and Ristenpart, Thomas},
doi = {10.1109/SP.2015.36},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
title = {{Cracking-Resistant Password Vaults using Natural Language Encoders}},
year = {2015}
}
@article{Abdalla2015,
abstract = {In the most strict formal definition of security for password-authenticated$\backslash$nkey exchange, an adversary can test at most one password per impersonation$\backslash$nattempt. We propose a slightly relaxed definition which restricts$\backslash$nan adversary to testing at most a constant number of passwords per$\backslash$nimpersonation attempt. This definition seems useful, since there$\backslash$nis currently a popular password-authenticated key exchange protocol$\backslash$ncalled SRP that seems resistant to off-line dictionary attack, yet$\backslash$ndoes allow an adversary to test two passwords per impersonation attempt.$\backslash$nIn this paper we prove (in the random oracle model) that a certain$\backslash$ninstantiation of the SPEKE protocol that uses hashed passwords instead$\backslash$nof non-hashed passwords is a secure password-authenticated key exchange$\backslash$nprotocol (using our relaxed definition) based on a new assumption,$\backslash$nthe Decision Inverted-Additive Diffie-Hellman assumption. Since this$\backslash$nis a new security assumption, we investigate its security and relation$\backslash$nto other assumptions; specifically we prove a lower bound for breaking$\backslash$nthis new assumption in the generic model, and we show that the computational$\backslash$nversion of this new assumption is equivalent to the Computational$\backslash$nDiffie-Hellman assumption.},
author = {Abdalla, Michel and Benhamouda, Fabrice and MacKenzie, Philip},
doi = {10.1109/SP.2015.41},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
pages = {571--587},
title = {{Security of the J-PAKE Password-Authenticated Key Exchange Protocol}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7163048},
year = {2015}
}
@article{Bos2015,
author = {Bos, Joppe W and Costello, Craig and Naehrig, Michael and Stebila, Douglas},
doi = {10.1109/SP.2015.40},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
pages = {1--26},
title = {{Post-quantum key exchange for the TLS protocol from the ring learning with errors problem}},
year = {2015}
}
@article{Beurdouche2015,
abstract = {Implementations of the Transport Layer Security (TLS) protocol must handle a variety of protocol versions and extensions, authentication modes, and key exchange methods. Confusingly, each combination may prescribe a different message sequence between the client and the server. We address the problem of designing a robust composite state machine that correctly multiplexes between these different protocol modes. We systematically test popular open-source TLS implementations for state machine bugs and discover several critical security vulnerabilities that have lain hidden in these libraries for years, and have now finally been patched due to our disclosures. Several of these vulnerabilities, including the recently publicized FREAK flaw, enable a network attacker to break into TLS connections between authenticated clients and servers. We argue that state machine bugs stem from incorrect compositions of individually correct state machines. We present the first verified implementation of a composite TLS state machine in C that can be embedded into OpenSSL and accounts for all its supported ciphersuites. Our attacks expose the need for the formal verification of core components in cryptographic protocol libraries; our implementation demonstrates that such mechanized proofs are within reach, even for mainstream TLS implementations.},
author = {Beurdouche, Benjamin and Bhargavan, Karthikeyan and Delignat-lavaud, Antoine and Fournet, Cedric and Kohlweiss, Markulf},
doi = {10.1109/SP.2015.39},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
pages = {1--19},
title = {{A Messy State of the Union: Taming the Composite State Machines of TLS}},
year = {2015}
}
@article{He2015,
author = {He, Boyuan and Rastogi, Vaibhav and Cao, Yinzhi and Chen, Yan},
doi = {10.1109/SP.2015.38},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
title = {{Vetting SSL Usage in Applications with SSL INT}},
year = {2015}
}
@article{Xu2015,
author = {Xu, Yuanzhong and Cui, Weidong and Peinado, Marcus},
doi = {10.1109/SP.2015.45},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
title = {{Controlled-Channel Attacks : Deterministic Side Channels for Untrusted Operating Systems}},
year = {2015}
}
@article{Irazoqui2015,
author = {Irazoqui, Gorka and Eisenbarth, Thomas and Sunar, Berk},
doi = {10.1109/SP.2015.42},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
title = {{S{\{}{\$}{\}}A: A Shared Cache Attack that Works Across Cores and Defies VM Sandboxing—and its Application to AES}},
year = {2015}
}
@article{Liu2015a,
abstract = {We present an effective implementation of the PRIME+PROBE side-channel attack against the last-level cache. We measure the capacity of the covert channel the attack creates and demonstrate a cross-core, cross-VM attack on multiple versions of GnuPG. Our technique achieves a high attack resolution without relying on weaknesses in the OS or virtual machine monitor or on sharing memory between attacker and victim.},
author = {Liu, Fangfei and Yarom, Yuval and Ge, Qian and Heiser, Gernot and Lee, Ruby B},
doi = {10.1109/SP.2015.43},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {ElGamal,covert channel,cross-VM side channel,last-level cache,—Side-channel attack},
title = {{Last-Level Cache Side-Channel Attacks are Practical}},
year = {2015}
}
@article{Andrysco2015,
author = {Andrysco, Marc and Kohlbrenner, David and Mowery, Keaton and Jhala, Ranjit and Lerner, Sorin and Shacham, Hovav},
doi = {10.1109/SP.2015.44},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
pages = {623--639},
title = {{On Subnormal Floating Point and Abnormal Timing}},
year = {2015}
}
@article{Pewny2015,
author = {Pewny, Jannik and Garmany, Behrad and Gawlik, Robert and Rossow, Christian and Holz, Thorsten and Horst, G},
doi = {10.1109/SP.2015.49},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
title = {{Cross-Architecture Bug Search in Binary Executables}},
year = {2015}
}
@article{Nappa2015,
abstract = {Vulnerability exploits remain an important mechanism for malware delivery, despite efforts to speed up the creation of patches and improvements in software updating mechanisms. Vulnerabilities in client applications (e.g., browsers, multimedia players, document readers and editors) are often exploited in spear phishing attacks and are difficult to characterize using network vulnerability scanners. Analyzing their lifecycle requires observing the deployment of patches on hosts around the world. Using data collected over 5 years on 8.4 million hosts, available through Symantec’s WINE platform, we present the first systematic study of patch deployment in client-side vulnerabilities. We analyze the patch deployment process of 1,593 vulnerabilities from 10 popular client applications, and we identify several new threats presented by multiple installations of the same program and by shared libraries distributed with several applications. For the 80 vulnerabilities in our dataset that affect code shared by two applications, the time between patch releases in the different applications is up to 118 days (with a median of 11 days). Furthermore, as the patching rates differ considerably among applications, many hosts patch the vulnerability in one application but not in the other one. We demonstrate two novel attacks that enable exploitation by invoking old versions of applications that are used infrequently, but remain installed. We also find that the median fraction of vulnerable hosts patched when exploits are released is at most 14{\{}{\%}{\}}. Finally, we show that the patching rate is affected by user-specific and application- specific factors; for example, hosts belonging to security analysts and applications with an automated updating mechanism have significantly lower median times to patch},
author = {Nappa, Antonio and Johnson, Richard and Bilge, Leyla and Caballero, Juan and Dumitras, Tudor},
doi = {10.1109/SP.2015.48},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
pages = {692--708},
title = {{The Attack of the Clones : A Study of the Impact of Shared Code on Vulnerability Patching}},
year = {2015}
}
@article{Ugarte-pedrero2015,
abstract = {—Run-time packers are often used by malware-writers to obfuscate their code and hinder static analysis. The packer problem has been widely studied, and several solutions have been proposed in order to generically unpack protected binaries. Nevertheless, these solutions commonly rely on a number of assumptions that may not necessarily reflect the reality of the packers used in the wild. Moreover, previous solutions fail to provide useful information about the structure of the packer or its complexity. In this paper, we describe a framework for packer analysis and we propose a taxonomy to measure the runtime complexity of packers. We evaluated our dynamic analysis system on two datasets, composed of both off-the-shelf packers and custom packed binaries. Based on the results of our experiments, we present several statistics about the packers complexity and their evolution over time.},
author = {Ugarte-pedrero, Xabier and Balzarotti, Davide and Santos, Igor and Bringas, Pablo G},
doi = {10.1109/SP.2015.46},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
pages = {659--673},
title = {{SoK: Deep Packer Inspection: A Longitudinal Study of the Complexity of Run-Time Packers}},
url = {http://www.ieee-security.org/TC/SP2015/papers-archived/6949a659.pdf},
year = {2015}
}
@article{Johannesmeyer2015,
author = {Johannesmeyer, Brian},
doi = {10.1109/SP.2015.47},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
title = {{A Generic Approach to Automatic Deobfuscation of Executable Code}},
year = {2015}
}
@article{Cha2015,
author = {Cha, Sang Kil and Woo, Maverick and Brumley, David},
doi = {10.1109/SP.2015.50},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
pages = {725--741},
title = {{Program-Adaptive Mutational Fuzzing}},
year = {2015}
}
@article{DeAmorim2015,
author = {{De Amorim}, A A and Denes, M and Giannarakis, N and Hritcu, C and Pierce, B C and Spector-Zabusky, A and Tolmach, A},
doi = {10.1109/SP.2015.55},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {cache storage;inference mechanisms;meta data;secur},
pages = {813--830},
title = {{Micro-Policies: Formally Verified, Tag-Based Security Monitors}},
year = {2015}
}
@article{Schuster2015a,
abstract = {—Code reuse attacks such as return-oriented program-ming (ROP) have become prevalent techniques to exploit memory corruption vulnerabilities in software programs. A variety of corresponding defenses has been proposed, of which some have already been successfully bypassed—and the arms race continues. In this paper, we perform a systematic assessment of recently proposed CFI solutions and other defenses against code reuse attacks in the context of C++. We demonstrate that many of these defenses that do not consider object-oriented C++ semantics pre-cisely can be generically bypassed in practice. Our novel attack technique, denoted as counterfeit object-oriented programming (COOP), induces malicious program behavior by only invoking chains of existing C++ virtual functions in a program through corresponding existing call sites. COOP is Turing complete in realistic attack scenarios and we show its viability by developing sophisticated, real-world exploits for Internet Explorer 10 on Windows and Firefox 36 on Linux. Moreover, we show that even recently proposed defenses (CPS, T-VIP, vfGuard, and VTint) that specifically target C++ are vulnerable to COOP. We observe that constructing defenses resilient to COOP that do not require access to source code seems to be challenging. We believe that our investigation and results are helpful contributions to the design and implementation of future defenses against control-flow hijacking attacks.},
author = {Schuster, Felix and Tendyck, Thomas and Liebchen, Christopher and Davi, Lucas and Sadeghi, Ahmad-Reza and Holz, Thorsten},
doi = {10.1109/SP.2015.51},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
title = {{Counterfeit Object-oriented Programming On the Difficulty of Preventing Code Reuse Attacks in C++ Applications}},
year = {2015}
}
@article{Yamaguchi2015,
abstract = {Taint-style vulnerabilities are a persistent problem in software development, as the recently discovered “Heartbleed” vulnerability strikingly illustrates. In this class of vulnerabil- ities, attacker-controlled data is passed unsanitized from an input source to a sensitive sink. While simple instances of this vulnerability class can be detected automatically, more subtle defects involving data flow across several functions or project- specific APIs are mainly discovered by manual auditing. Different techniques have been proposed to accelerate this process by searching for typical patterns of vulnerable code. However, all of these approaches require a security expert to manually model and specify appropriate patterns in practice. In this paper, we propose a method for automatically inferring search patterns for taint-style vulnerabilities in C code. Given a security-sensitive sink, such as a memory function, our method automatically identifies corresponding source-sink systems and constructs patterns that model the data flow and sanitization in these systems. The inferred patterns are expressed as traversals in a code property graph and enable efficiently searching for unsanitized data flows—across several functions as well as with project-specific APIs. We demonstrate the efficacy of this approach in different experiments with 5 open-source projects. The inferred search patterns reduce the amount of code to inspect for finding known vulnerabilities by 94.9{\{}{\%}{\}} and also enable us to uncover 8 previously unknown vulnerabilities.},
author = {Yamaguchi, Fabian and Maier, Alwin and Gascon, Hugo and Rieck, Konrad},
doi = {10.1109/SP.2015.54},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Clustering,Graph Databases,Vulnerabilities},
title = {{Automatic Inference of Search Patterns for Taint-Style Vulnerabilities}},
url = {http://user.informatik.uni-goettingen.de/{\{}{~}{\}}fyamagu/pdfs/2015-oakland.pdf},
year = {2015}
}
@article{Crane2015,
author = {Crane, Stephen and Liebchen, Christopher and Homescu, Andrei and Davi, Lucas and Larsen, Per and Sadeghi, Ahmad-reza and Brunthaler, Stefan and Franz, Michael},
doi = {10.1109/SP.2015.52},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
pages = {18},
title = {{Readactor : Practical Code Randomization Resilient to Memory Disclosure}},
year = {2015}
}
@article{Evans2015,
author = {Evans, I and Fingeret, S and Gonzalez, J and Otgonbaatar, U and Tang, T and Shrobe, H and Sidiroglou-Douskos, S and Rinard, M and Okhravi, H},
doi = {10.1109/SP.2015.53},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {data protection;security of data;ARM;C-C++;CPI saf},
pages = {781--796},
title = {{Missing the Point(er): On the Effectiveness of Code Pointer Integrity}},
year = {2015}
}
@article{Chen2015,
author = {Chen, Eric Y and Chen, Shuo and Qadeer, Shaz and Wang, Rui},
doi = {10.1109/SP.2015.56},
isbn = {9781467369497},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {-multiparty protocol,cst,online payment,rui wang,shaz qadeer,shuo chen,single-sign-on,symbolic transaction,verification},
title = {{Securing Multiparty Online Services via Certification of Symbolic Transactions}},
year = {2015}
}
@article{Kim2015,
author = {Kim, Beom Heyn and Lie, David},
doi = {10.1109/SP.2015.59},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
pages = {880--896},
title = {{Caelus : Verifying the Consistency of Cloud Services with Battery-Powered Devices}},
year = {2015}
}
@article{Wagner2015,
author = {Wagner, Jonas and Kuznetsov, Volodymyr and Candea, George and Kinder, Johannes},
doi = {10.1109/SP.2015.58},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
title = {{High System-Code Security with Low Overhead}},
year = {2015}
}
@article{Zhou2015,
author = {Zhou, Yuchen},
doi = {10.1109/SP.2015.57},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
title = {{Understanding and Monitoring Embedded Web Scripts}},
year = {2015}
}
@article{Xia2015,
author = {Xia, Mingyuan},
doi = {10.1109/SP.2015.60},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {-approximated execution,mobile application,pri-,program analysis,vacy},
title = {{Effective Real-time Android Application Auditing}},
year = {2015}
}
@article{Bianchi2015,
abstract = {Mobile applications are part of the everyday lives of billions of people, who often trust them with sensitive information. These users identify the currently focused app solely by its visual appearance, since the GUIs of the most popular mobile OSes do not show any trusted indication of the app origin. In this paper, we analyze in detail the many ways in which Android users can be confused into misidentifying an app, thus, for instance, being deceived into giving sensitive information to a malicious app. Our analysis of the Android platform APIs, assisted by an automated state-exploration tool, led us to identify and categorize a variety of attack vectors (some previously known, others novel, such as a non-escapable fullscreen overlay) that allow a malicious app to surreptitiously replace or mimic the GUI of other apps and mount phishing and click-jacking attacks. Limitations in the system GUI make these attacks significantly harder to notice than on a desktop machine, leaving users completely defenseless against them. To mitigate GUI attacks, we have developed a two-layer defense. To detectmalicious apps at themarket level, we developed a tool that uses static analysis to identify code that could launch GUI confusion attacks. We show how this tool detects apps that might launch GUI attacks, such as ransomware programs. Since these attacks are meant to confuse humans, we have also designed and implemented an on-device defense that addresses the underlying issue of the lack of a security indicator in the Android GUI. We add such an indicator to the system navigation bar; this indicator securely informs users about the origin of the app with which they are interacting (e.g., the PayPal app is backed by “PayPal, Inc.”). We demonstrate the effectiveness of our attacks and the proposed on-device defense with a user study involving 308 human subjects, whose ability to detect the attacks increased significantly when using a system equipped with our defense.},
author = {Bianchi, Antonio and Corbetta, Jacopo and Invernizzi, Luca and Fratantonio, Yanick and Kruegel, Christopher and Vigna, Giovanni},
doi = {10.1109/SP.2015.62},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
title = {{What the App is That? Deception and Countermeasures in the Android User Interface}},
year = {2015}
}
@article{Zhang2015a,
author = {Zhang, Nan and Yuan, Kan and Naveed, Muhammad and Zhou, Xiaoyong and Wang, Xiaofeng},
doi = {10.1109/SP.2015.61},
isbn = {978-1-4673-6949-7},
issn = {1081-6011},
journal = {s{\&}p},
title = {{Leave Me Alone: App-level Protection Against Runtime Information Gathering on Android}},
year = {2015}
}
@article{Li2014,
abstract = {Compromised websites that redirect web traffic to malicious hosts play a critical role in organized web crimes, serving as doorways to all kinds of malicious web activities (e.g., drive-by downloads, phishing etc.). They are also among the most elusive components of a malicious web infrastructure and extremely difficult to hunt down, due to the simplicity of redirect operations, which also happen on legitimate sites, and extensive use of cloaking techniques. Making the detection even more challenging is the recent trend of injecting redirect scripts into JavaScript (JS) files, as those files are not indexed by search engines and their infections are therefore more difficult to catch. In our research, we look at the problem from a unique angle: the adversary's strategy and constraints for deploying redirect scripts quickly and stealthily. Specifically, we found that such scripts are often blindly injected into both JS and HTML files for a rapid deployment, changes to the infected JS files are often made minimum to evade detection and also many JS files are actually JS libraries (JS-libs) whose uninfected versions are publicly available. Based upon those observations, we developed JsRED, a new technique for the automatic detection of unknown redirect-script injections. Our approach analyzes the difference between a suspicious JS-lib file and its clean counterpart to identify malicious redirect scripts and further searches for similar scripts in other JS and HTML files. This simple, lightweight approach is found to work effectively against redirect injection campaigns: our evaluation shows that JsRED captured most of compromised websites with almost no false positives, significantly outperforming a commercial detection service in terms of finding unknown JS infections. Based upon the compromised websites reported by JsRED, we further conducted a measurement study that reveals interesting features of redirect payloads and a new Peer-to-Peer network the adversary const- ucted to evade detection.},
author = {Li, Zhou and Alrwais, Sumayah and Wang, XiaoFeng and Alowaisheq, Eihal},
doi = {10.1109/SP.2014.8},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Browsers,Compromised Web Sites,Differential Analysis,Feeds,HTML,Libraries,Payloads,Security,Servers,Web Redirection},
pages = {3--18},
title = {{Hunting the Red Fox Online: Understanding and Detection of Mass Redirect-Script Injections}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956553},
year = {2014}
}
@article{Lee2014,
abstract = {Graphics processing units (GPUs) are important components of modern computing devices for not only graphics rendering, but also efficient parallel computations. However, their security problems are ignored despite their importance and popularity. In this paper, we first perform an in-depth security analysis on GPUs to detect security vulnerabilities. We observe that contemporary, widely-used GPUs, both NVIDIA's and AMD's, do not initialize newly allocated GPU memory pages which may contain sensitive user data. By exploiting such vulnerabilities, we propose attack methods for revealing a victim program's data kept in GPU memory both during its execution and right after its termination. We further show the high applicability of the proposed attacks by applying them to the Chromium and Firefox web browsers which use GPUs for accelerating webpage rendering. We detect that both browsers leave rendered webpage textures in GPU memory, so that we can infer which web pages a victim user has visited by analyzing the remaining textures. The accuracy of our advanced inference attack that uses both pixel sequence matching and RGB histogram matching is up to 95.4{\{}{\%}{\}}.},
author = {Lee, Sangho and Kim, Youngsok and Kim, Jong Jangwoo},
doi = {10.1109/SP.2014.9},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {AMD GPU,Browsers,Chromium,Context,GPU memory pages,GPU vulnerability,Graphics processing units,Kernel,Memory management,NVIDIA GPU,RGB histogram matching,Security,Web page,graphics processing unit,graphics processing units,image matching,image texture,pixel sequence matching,red-green-blue histogram matching,rendering,rendering (computer graphics)},
number = {1},
pages = {19--33},
title = {{Stealing Webpages Rendered on Your Browser by Exploiting GPU Vulnerabilities}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956554},
year = {2014}
}
@article{Tian2014,
abstract = {HTML5 changes many aspects in the browser world by introducing numerous new concepts, in particular, the new HTML5 screen sharing API impacts the security implications of browsers tremendously. One of the core assumptions on which browser security is built is that there is no cross-origin feedback loop from the client to the server. However, the screen sharing API allows creating a cross-origin feedback loop. Consequently, websites will potentially be able to see all visible content from the user's screen, irrespective of its origin. This cross-origin feedback loop, when combined with human vision limitations, can introduce new vulnerabilities. An attacker can capture sensitive information from victim's screen using the new API without the consensus of the victim. We investigate the security implications of the screen sharing API and discuss how existing defenses against traditional web attacks fail during screen sharing. We show that several attacks are possible with the help of the screen sharing API: cross-site request forgery, history sniffing, and information stealing. We discuss how popular websites such as Amazon and Wells Fargo can be attacked using this API and demonstrate the consequences of the attacks such as economic losses, compromised account and information disclosure. The objective of this paper is to present the attacks using the screen sharing API, analyze the fundamental cause and motivate potential defenses to design a more secure screen sharing API.},
author = {Tian, Yuan and Liu, Ying Chuan and Bhosale, Amar and Huang, Lin Shung and Tague, Patrick and Jackson, Collin},
doi = {10.1109/SP.2014.10},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Browser Security,Browsers,Feedback loop,Google,HTML5,HTML5 screen sharing API,History,Receivers,Screen Sharing,Security,Servers,Web attacks,Web sites,application program interfaces,history sniffing,hypermedia markup languages,information stealing,security of data},
pages = {34--48},
title = {{All Your Screens Are Belong to Us: Attacks Exploiting the HTML5 Screen Sharing API}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956555},
year = {2014}
}
@article{Bond2014,
abstract = {EMV, also known as "Chip and PIN", is the leading system for card payments worldwide. It is used throughout Europe and much of Asia, and is starting to be introduced in North America too. Payment cards contain a chip so they can execute an authentication protocol. This protocol requires point-of-sale (POS) terminals or ATMs to generate a nonce, called the unpredictable number, for each transaction to ensure it is fresh. We have discovered that some EMV implementers have merely used counters, timestamps or home-grown algorithms to supply this number. This exposes them to a "pre-play" attack which is indistinguishable from card cloning from the standpoint of the logs available to the card-issuing bank, and can be carried out even if it is impossible to clone a card physically (in the sense of extracting the key material and loading it into another card). Card cloning is the very type of fraud that EMV was supposed to prevent. We describe how we detected the vulnerability, a survey methodology we developed to chart the scope of the weakness, evidence from ATM and terminal experiments in the field, and our implementation of proof-of-concept attacks. We found flaws in widely-used ATMs from the largest manufacturers. We can now explain at least some of the increasing number of frauds in which victims are refused refunds by banks which claim that EMV cards cannot be cloned and that a customer involved in a dispute must therefore be mistaken or complicit. Pre-play attacks may also be carried out by malware in an ATM or POS terminal, or by a man-in-the-middle between the terminal and the acquirer. We explore the design and implementation mistakes that enabled the flaw to evade detection until now: shortcomings of the EMV specification, of the EMV kernel certification process, of implementation testing, formal analysis, or monitoring customer complaints. Finally we discuss countermeasures.},
archivePrefix = {arXiv},
arxivId = {1209.2531},
author = {Bond, Mike and Choudary, Omar and Murdoch, Steven J and Skorobogatov, Sergei and Anderson, Ross},
doi = {10.1109/SP.2014.11},
eprint = {1209.2531},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
number = {May},
pages = {1--21},
title = {{Chip and Skim: cloning EMV cards with the pre-play attack}},
url = {http://arxiv.org/abs/1209.2531},
year = {2014}
}
@article{Liang2014,
abstract = {Content Delivery Network (CDN) and Hypertext Transfer Protocol Secure (HTTPS) are two popular but independent web technologies, each of which has been well studied individually and independently. This paper provides a systematic study on how these two work together. We examined 20 popular CDN providers and 10,721 of their customer web sites using HTTPS. Our study reveals various problems with the current HTTPS practice adopted by CDN providers, such as widespread use of invalid certificates, private key sharing, neglected revocation of stale certificates, and insecure back-end communication. While some of those problems are operational issues only, others are rooted in the fundamental semantic conflict between the end-to-end nature of HTTPS and the man-in-the-middle nature of CDN involving multiple parties in a delegated service. To address the delegation problem when HTTPS meets CDN, we proposed and implemented a lightweight solution based on DANE (DNS-based Authentication of Named Entities), an emerging IETF protocol complementing the current Web PKI model. Our implementation demonstrates that it is feasible for HTTPS to work with CDN securely and efficiently. This paper intends to provide a context for future discussion within security and CDN community on more preferable solutions.},
author = {Liang, Jinjin and Jiang, Jian and Duan, Haixin and Li, Kang and Wan, Tao and Wu, Jianping},
doi = {10.1109/SP.2014.12},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Authentication,Browsers,CDN,DANE protocol,DNS-based authentication of named entities,HTTPS,Internet,Protocols,Servers,Uniform resource locators,Web technology,computer network security,content delivery network,delegated service authentication,domain name system,hypertext transfer protocol secure,protocols,stale certificates},
pages = {67--82},
title = {{When HTTPS Meets CDN: A Case of Authentication in Delegated Service}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956557},
year = {2014}
}
@article{Huang2014,
abstract = {The SSL man-in-the-middle attack uses forged SSL certificates to intercept encrypted connections between clients and servers. However, due to a lack of reliable indicators, it is still unclear how commonplace these attacks occur in the wild. In this work, we have designed and implemented a method to detect the occurrence of SSL man-in-the-middle attack on a top global website, Facebook. Over 3 million real-world SSL connections to this website were analyzed. Our results indicate that 0.2{\{}{\%}{\}} of the SSL connections analyzed were tampered with forged SSL certificates, most of them related to antivirus software and corporate-scale content filters. We have also identified some SSL connections intercepted by malware. Limitations of the method and possible defenses to such attacks are also discussed.},
author = {Huang, Lin Shung and Rice, Alex and Ellingsen, Erling and Jackson, Collin},
doi = {10.1109/SP.2014.13},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Browsers,Cryptography,Facebook,Java,Protocols,SSL,SSL man-in-the-middle attack,Servers,Sockets,antivirus software,certificates,certification,corporate-scale content filters,encrypted connections,forged SSL certificate analysis,global Web site,man-in-the-middle attack,secure socket layer,security of data},
pages = {83--97},
title = {{Analyzing Forged SSL Certificates in the Wild}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956558},
year = {2014}
}
@article{Bhargavan2014,
abstract = {TLS was designed as a transparent channel abstraction to allow developers with no cryptographic expertise to protect their application against attackers that may control some clients, some servers, and may have the capability to tamper with network connections. However, the security guarantees of TLS fall short of those of a secure channel, leading to a variety of attacks. We show how some widespread false beliefs about these guarantees can be exploited to attack popular applications and defeat several standard authentication methods that rely too naively on TLS. We present new client impersonation attacks against TLS renegotiations, wireless networks, challenge-response protocols, and channel-bound cookies. Our attacks exploit combinations of RSA and Diffie-Hellman key exchange, session resumption, and renegotiation to bypass many recent countermeasures. We also demonstrate new ways to exploit known weaknesses of HTTP over TLS. We investigate the root causes for these attacks and propose new countermeasures. At the protocol level, we design and implement two new TLS extensions that strengthen the authentication guarantees of the handshake. At the application level, we develop an exemplary HTTPS client library that implements several mitigations, on top of a previously verified TLS implementation, and verify that their composition provides strong, simple application security.},
author = {Bhargavan, Karthikeyan and Lavaud, Antoine Delignat and Fournet, Cedric and Pironti, Alfredo and Strub, Pierre Yves},
doi = {10.1109/SP.2014.14},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Authentication,Browsers,Cryptography,Diffie-Hellman key exchange,HTTPS client library,Libraries,Protocols,RSA,Servers,TLS implementation,TLS renegotiations,TLS security guarantees,application protection,application security,authentication,authorisation,challenge-response protocols,channel-bound cookies,client impersonation attacks,client-server systems,computer network security,cookie cutters,cryptographic protocols,data protection,network connections,public key cryptography,secure channel,session resumption,transparent channel abstraction,transport protocols,triple-handshakes,wireless networks},
pages = {98--113},
title = {{Triple Handshakes and Cookie Cutters: Breaking and Fixing Authentication over TLS}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956559},
year = {2014}
}
@article{Brubaker2014,
abstract = {Modern network security rests on the Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols. Distributed systems, mobile and desktop applications, embedded devices, and all of secure Web rely on SSL/TLS for protection against network attacks. This protection critically depends on whether SSL/TLS clients correctly validate X.509 certificates presented by servers during the SSL/TLS handshake protocol. We design, implement, and apply the first methodology for large-scale testing of certificate validation logic in SSL/TLS implementations. Our first ingredient is "frankencerts," synthetic certificates that are randomly mutated from parts of real certificates and thus include unusual combinations of extensions and constraints. Our second ingredient is differential testing: if one SSL/TLS implementation accepts a certificate while another rejects the same certificate, we use the discrepancy as an oracle for finding flaws in individual implementations. Differential testing with frankencerts uncovered 208 discrepancies between popular SSL/TLS implementations such as OpenSSL, NSS, CyaSSL, GnuTLS, PolarSSL, MatrixSSL, etc. Many of them are caused by serious security vulnerabilities. For example, any server with a valid X.509 version1 certificate can act as a rogue certificate authority and issue fake certificates for any domain, enabling man-in-the-middle attacks against MatrixSSL and GnuTLS. Several implementations also accept certificate authorities created by unauthorized issuers, as well as certificates not intended for server authentication. We also found serious vulnerabilities in how users are warned about certificate validation errors. When presented with an expired, self-signed certificate, NSS, Safari, and Chrome (on Linux) report that the certificate has expired - a low-risk, often ignored error - but not that the connection is insecure against a man-in-the-middle attack. These results demonstrate that automated adversarial testing with frankencert- is a powerful methodology for discovering security flaws in SSL/TLS implementations.},
author = {Brubaker, Chad and Jana, Suman and Ray, Baishakhi and Khurshid, Sarfraz and Shmatikov, Vitaly},
doi = {10.1109/SP.2014.15},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Authentication,Browsers,Chrome,Computer bugs,CyaSSL,Frankencerts,GnuTLS,MatrixSSL,NSS,OpenSSL,PolarSSL,Protocols,SSL,SSL-TLS clients,SSL-TLS handshake protocol,SSL-TLS implementations,Safari,Servers,Testing,X.509 certificates,authorisation,automated adversarial testing,automated testing,certificate validation,certificate validation errors,certificate validation logic,computer network security,differential testing,man-in-the-middle attacks,modern network security,network attacks,online front-ends,oracle,protocols,secure sockets layer protocols,security vulnerabilities,self-signed certificate,server authentication,synthetic certificates,transport layer security protocols},
pages = {114--129},
title = {{Using Frankencerts for Automated Adversarial Testing of Certificate Validation in SSL/TLS Implementations}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956560},
year = {2014}
}
@article{Blankstein2014,
abstract = {In many client-facing applications, a vulnerability in any part can compromise the entire application. This paper describes the design and implementation of Passe, a system that protects a data store from unintended data leaks and unauthorized writes even in the face of application compromise. Passe automatically splits (previously shared-memory-space) applications into sandboxed processes. Passe limits communication between those components and the types of accesses each component can make to shared storage, such as a backend database. In order to limit components to their least privilege, Passe uses dynamic analysis on developer-supplied end-to-end test cases to learn data and control-flow relationships between database queries and previous query results, and it then strongly enforces those relationships. Our prototype of Passe acts as a drop-in replacement for the Django web framework. By running eleven unmodified, off-the-shelf applications in Passe, we demonstrate its ability to provide strong security guarantees-Passe correctly enforced 96{\{}{\%}{\}} of the applications' policies-with little additional overhead. Additionally, in the web-specific setting of the prototype, we also mitigate the cross-component effects of cross-site scripting (XSS) attacks by combining browser HTML5 sandboxing techniques with our automatic component separation.},
author = {Blankstein, Aaron and Freedman, Michael J},
doi = {10.1109/SP.2014.16},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Browsers,Databases,Django web framework,HTML5 sandboxing techniques,Libraries,Passe system,Prototypes,Runtime,Security,Servers,Web services,XSS attack,capabilities,client-facing applications,control-flow relationship,cross-site scripting attack,data-flow relationship,database queries,isolation,principle of least privilege,query results,sandboxed process,security guarantee,security of data,security policy inference,shared-memory-space application,web security},
pages = {133--148},
title = {{Automating Isolation and Least Privilege in Web Services}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956561},
year = {2014}
}
@article{Mulliner2014,
abstract = {Graphical user interfaces (GUIs) are the predominant means by which users interact with modern programs. GUIs contain a number of common visual elements or widgets such as labels, text fields, buttons, and lists, and GUIs typically provide the ability to set attributes on these widgets to control their visibility, enabled status, and whether they are writable. While these attributes are extremely useful to provide visual cues to users to guide them through an application's GUI, they can also be misused for purposes they were not intended. In particular, in the context of GUI-based applications that include multiple privilege levels within the application, GUI element attributes are often misused as a mechanism for enforcing access control policies. In this work, we introduce GEMs, or instances of GUI element misuse, as a novel class of access control vulnerabilities in GUI-based applications. We present a classification of different GEMs that can arise through misuse of widget attributes, and describe a general algorithm for identifying and confirming the presence of GEMs in vulnerable applications. We then present GEM Miner, an implementation of our GEM analysis for the Windows platform. We evaluate GEM Miner over a test set of three complex, real-world GUI-based applications targeted at the small business and enterprise markets, and demonstrate the efficacy of our analysis by finding numerous previously unknown access control vulnerabilities in these applications. We have reported the vulnerabilities we discovered to the developers of each application, and in one case have received confirmation of the issue.},
author = {Mulliner, Collin and Robertson, William and Kirda, Engin and Xwrpdwhg, Lgghq and Ri, Lvfryhu},
doi = {10.1109/SP.2014.17},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {GEM Miner,GEM analysis,GEM classification,GUI,GUI element attributes,GUI element misuse,Licenses,Privacy,Security,Windows platform,access control,access control policies,access control vulnerabilities,authorisation,automated discovery,automation,business,buttons,enterprise markets,graphical user interfaces,labels,lists,pattern classification,real-world GUI-based applications,text fields,visual cues,visual elements,vulnerability analysis,widget,widget attributes},
pages = {149--162},
title = {{Hidden GEMs: Automated Discovery of Access Control Vulnerabilities in Graphical User Interfaces}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956562},
year = {2014}
}
@article{Kremer2014,
author = {Kremer, Steve},
doi = {10.1109/SP.2014.18},
journal = {s{\&}p},
title = {{Automated analysis of security protocols with global state}},
year = {2014}
}
@article{Schmidt2014,
abstract = {We advance the state-of-the-art in automated symbolic cryptographic protocol analysis by providing the first algorithm that can handle Diffie-Hellman exponentiation, bilinear pairing, and AC-operators. Our support for AC-operators enables protocol specifications to use multisets, natural numbers, and finite maps. We implement the algorithm in the TAMARIN prover and provide the first symbolic correctness proofs for group key agreement protocols that use Diffie-Hellman or bilinear pairing, loops, and recursion, while at the same time supporting advanced security properties, such as perfect forward secrecy and eCK-security. We automatically verify a set of protocols, including the STR, group Joux, and GDH protocols, thereby demonstrating the effectiveness of our approach.},
author = {Schmidt, Benedikt and Sasse, Ralf and Cremers, Cas and Basin, David},
doi = {10.1109/SP.2014.19},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {AC-operators,Cognition,Cryptography,DH-HEMTs,Diffie-Hellman exponentiation,GDH protocols,Mathematical model,Protocols,STR,Semantics,TAMARIN prover,advanced security properties,automated symbolic cryptographic protocol analysis,bilinear pairing,cryptographic protocols,eCK-security,finite maps,group Joux,group key agreement protocols,multisets,natural numbers,perfect forward secrecy,protocol specifications,public key cryptography,set theory,symbolic correctness proofs},
pages = {179--194},
title = {{Automated Verification of Group Key Agreement Protocols}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956564},
year = {2014}
}
@article{Rndic2014,
abstract = {Learning-based classifiers are increasingly used for detection of various forms of malicious data. However, if they are deployed online, an attacker may attempt to evade them by manipulating the data. Examples of such attacks have been previously studied under the assumption that an attacker has full knowledge about the deployed classifier. In practice, such assumptions rarely hold, especially for systems deployed online. A significant amount of information about a deployed classifier system can be obtained from various sources. In this paper, we experimentally investigate the effectiveness of classifier evasion using a real, deployed system, PDFrate, as a test case. We develop a taxonomy for practical evasion strategies and adapt known evasion algorithms to implement specific scenarios in our taxonomy. Our experimental results reveal a substantial drop of PDFrate's classification scores and detection accuracy after it is exposed even to simple attacks. We further study potential defense mechanisms against classifier evasion. Our experiments reveal that the original technique proposed for PDFrate is only effective if the executed attack exactly matches the anticipated one. In the discussion of the findings of our study, we analyze some potential techniques for increasing robustness of learning-based systems against adversarial manipulation of data.},
author = {Rndic, Nedim and Laskov, Pavel},
doi = {10.1109/SP.2014.20},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Feature extraction,Learning systems,Malware,PDFrate,Portable document format,Taxonomy,Training,adversarial data manipulation,learning (artificial intelligence),learning-based classifier evasion,learning-based systems,malicious data detection,pattern classification,security of data},
pages = {197--211},
title = {{Practical Evasion of a Learning-Based Classifier: A Case Study}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956565},
year = {2014}
}
@article{Afroz2014,
abstract = {Stylometry is a method for identifying anonymous authors of anonymous texts by analyzing their writing style. While stylometric methods have produced impressive results in previous experiments, we wanted to explore their performance on a challenging dataset of particular interest to the security research community. Analysis of underground forums can provide key information about who controls a given bot network or sells a service, and the size and scope of the cybercrime underworld. Previous analyses have been accomplished primarily through analysis of limited structured metadata and painstaking manual analysis. However, the key challenge is to automate this process, since this labor intensive manual approach clearly does not scale. We consider two scenarios. The first involves text written by an unknown cybercriminal and a set of potential suspects. This is standard, supervised stylometry problem made more difficult by multilingual forums that mix l33t-speak conversations with data dumps. In the second scenario, you want to feed a forum into an analysis engine and have it output possible doppelgangers, or users with multiple accounts. While other researchers have explored this problem, we propose a method that produces good results on actual separate accounts, as opposed to data sets created by artificially splitting authors into multiple identities. For scenario 1, we achieve 77{\{}{\%}{\}} to 84{\{}{\%}{\}} accuracy on private messages. For scenario 2, we achieve 94{\{}{\%}{\}} recall with 90{\{}{\%}{\}} precision on blogs and 85.18{\{}{\%}{\}} precision with 82.14{\{}{\%}{\}} recall for underground forum users. We demonstrate the utility of our approach with a case study that includes applying our technique to the Carders forum and manual analysis to validate the results, enabling the discovery of previously undetected doppelganger accounts.},
author = {Afroz, Sadia and Islam, Aylin Caliskan and Stolerman, Ariel and Greenstadt, Rachel and McCoy, Damon},
doi = {10.1109/SP.2014.21},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Accuracy,Blogs,Carders forum,Detectors,Doppelganger Finder,Electronic mail,Internet,Manuals,Social network services,Stylometry,anonymous author identification,anonymous texts,computer crime,cybercrime,cybercrime underworld,cybercriminal,data dumps,l33t-speak conversations,meta data,multilingual forums,security research community,standards,structured metadata,stylometric methods,supervised stylometry problem,underground forum,underground forum analysis,writing style analysis},
pages = {212--226},
title = {{Doppelganger Finder: Taking Stylometry to the Underground}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956566},
year = {2014}
}
@article{Bittau2014,
author = {Bittau, Andrea and Belay, Adam},
doi = {10.1109/SP.2014.22},
isbn = {9781479946860},
issn = {10816011},
journal = {s{\&}p},
title = {{Hacking blind}},
url = {http://www.scs.stanford.edu/brop/bittau-brop.pdf},
year = {2014}
}
@article{Bosman2014,
abstract = {Signal handling has been an integral part of UNIX systems since the earliest implementation in the 1970s. Nowadays, we find signals in all common flavors of UNIX systems, including BSD, Linux, Solaris, Android, and Mac OS. While each flavor handles signals in slightly different ways, the implementations are very similar. In this paper, we show that signal handling can be used as an attack method in exploits and backdoors. The problem has been a part of UNIX from the beginning, and now that advanced security measures like ASLR, DEP and stack cookies have made simple exploitation much harder, our technique is among the lowest hanging fruit available to an attacker. Specifically, we describe Sigreturn Oriented Programming (SROP), a novel technique for exploits and backdoors in UNIX-like systems. Like return-oriented programming (ROP), sigreturn oriented programming constructs what is known as a 'weird machine' that can be programmed by attackers to change the behavior of a process. To program the machine, attackers set up fake signal frames and initiate returns from signals that the kernel never really delivered. This is possible, because UNIX stores signal frames on the process' stack. Sigreturn oriented programming is interesting for attackers, OS developers and academics. For attackers, the technique is very versatile, with pre-conditions that are different from those of existing exploitation techniques like ROP. Moreover, unlike ROP, sigreturn oriented programming programs are portable. For OS developers, the technique presents a problem that has been present in one of the two main operating system families from its inception, while the fixes (which we also present) are non-trivial. From a more academic viewpoint, it is also interesting because we show that sigreturn oriented programming is Turing complete. We demonstrate the usefulness of the technique in three applications. First, we describe the exploitation of a vulnerable web server on different Linux dist- ibutions. Second, we build a very stealthy proof-of-concept backdoor. Third, we use SROP to bypass Apple's code signing and security vetting process by building an app that can execute arbitrary system calls. Finally, we discuss mitigation techniques.},
author = {Bosman, Erik and Bos, Herbert},
doi = {10.1109/SP.2014.23},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {ASLR,Android,Apples code signing,BSD,Context,DEP,Kernel,Linux,Linux distributions,Mac OS,OS developers,Operatings system security,Program processors,Programming,Registers,SROP,Security,Solaris,Turing complete,UNIX systems,advanced security measures,attack method,attackers,backdoors,computer crime,exploitation techniques,exploits,fake signal frames,mitigation techniques,portable shellcode,process behavior,proof-of-concept backdoor,security vetting process,signal handling,sigreturn oriented programming,stack cookies,vulnerable Web server,weird machine},
pages = {243--258},
title = {{Framing Signals - A Return to Portable Shellcode}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956568},
year = {2014}
}
@article{Mickens2014,
abstract = {Pivot is a new JavaScript isolation framework for web applications. Pivot uses iframes as its low-level isolation containers, but it uses code rewriting to implement synchronous cross-domain interfaces atop the asynchronous cross-frame postMessage( ) primitive. Pivot layers a distributed scheduling abstraction across the frames, essentially treating each frame as a thread which can invoke RPCs that are serviced by external threads. By rewriting JavaScript call sites, Pivot can detect RPC invocations, Pivot exchanges RPC requests and responses via postMessage( ), and it pauses and restarts frames using a novel rewriting technique that translates each frame's JavaScript code into a restart able generator function. By leveraging both iframes and rewriting, Pivot does not need to rewrite all code, providing an order-of-magnitude performance improvement over rewriting-only solutions. Compared to iframe-only approaches, Pivot provides synchronous RPC semantics, which developers typically prefer over asynchronous RPCs. Pivot also allows developers to use the full, unrestricted JavaScript language, including powerful statements like eval( ).},
author = {Mickens, James},
doi = {10.1109/SP.2014.24},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Browsers,Generators,Java,JavaScript call sites,JavaScript code,JavaScript isolation framework,Libraries,Pivot,RPC invocations,RPC requests,Reactive power,Runtime,Satellites,Security,Web applications,Web sites,asynchronous cross-frame postMessage() primitive,code rewriting,distributed scheduling abstraction,eval(),generator chains,iframe-only approaches,iframes,low-level isolation containers,order-of-magnitude performance improvement,program compilers,restartable generator function,rewriting systems,rewriting technique,rewriting-only solutions,synchronous cross-domain interfaces,synchronous mashup isolation,unrestricted JavaScript language},
pages = {261--275},
title = {{Pivot: Fast, Synchronous Mashup Isolation Using Generator Chains}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956569},
year = {2014}
}
@article{Larsen2014,
abstract = {The idea of automatic software diversity is at least two decades old. The deficiencies of currently deployed defenses and the transition to online software distribution (the "App store" model) for traditional and mobile computers has revived the interest in automatic software diversity. Consequently, the literature on diversity grew by more than two dozen papers since 2008. Diversity offers several unique properties. Unlike other defenses, it introduces uncertainty in the target. Precise knowledge of the target software provides the underpinning for a wide range of attacks. This makes diversity a broad rather than narrowly focused defense mechanism. Second, diversity offers probabilistic protection similar to cryptography-attacks may succeed by chance so implementations must offer high entropy. Finally, the design space of diversifying program transformations is large. As a result, researchers have proposed multiple approaches to software diversity that vary with respect to threat models, security, performance, and practicality. In this paper, we systematically study the state-of-the-art in software diversity and highlight fundamental trade-offs between fully automated approaches. We also point to open areas and unresolved challenges. These include "hybrid solutions", error reporting, patching, and implementation disclosure attacks on diversified software.},
author = {Larsen, Per and Homescu, Andrei and Brunthaler, Stefan and Franz, Michael},
doi = {10.1109/SP.2014.25},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {App store model,Encoding,Layout,Monitoring,Operating systems,Registers,Security,SoK,automated software diversity,cryptography,error reporting,implementation disclosure attacks,mobile computers,mobile computing,online software distribution,patching attacks,probabilistic protection,probability,program transformations,software attacks,software performance evaluation},
pages = {276--291},
title = {{SoK: Automated Software Diversity}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956570},
year = {2014}
}
@article{JohnCriswellNathanDautenhahn2014,
abstract = {We present a new system, KCoFI, that is the first we know of to provide complete Control-Flow Integrity protection for commodity operating systems without using heavyweight complete memory safety. Unlike previous systems, KCoFI protects commodity operating systems from classical control- flow hijack attacks, return-to-user attacks, and code segment modification attacks. We formally verify a subset of KCoFI’s design by modeling several features in small-step semantics and providing a partial proof that the semantics maintain control-flow integrity. The model and proof account for oper- ations such as page table management, trap handlers, context switching, and signal delivery. Our evaluation shows that KCoFI prevents all the gadgets found by an open-source Return Oriented Programming (ROP) gadget-finding tool in the FreeBSD kernel from being used; it also reduces the number of indirect control-flow targets by 98.18{\{}{\%}{\}}. Our evaluation also shows that the performance impact of KCoFI on web server bandwidth is negligible while file transfer bandwidth using OpenSSH is reduced by an average of 13{\{}{\%}{\}}, and at worst 27{\{}{\%}{\}}, across a wide range of file sizes. PostMark, an extremely file-system intensive benchmark, shows 2x overhead. Where comparable numbers are available, the overheads of KCoFI are far lower than heavyweight memory-safety techniques. I.},
author = {{John Criswell, Nathan Dautenhahn}, Vikram Adve},
doi = {10.1109/SP.2014.26},
isbn = {978-1-4799-4686-0},
issn = {10816011},
journal = {s{\&}p},
title = {{KCoFI: Complete control-flow integrity for commodity operating system kernels}},
url = {http://web.engr.illinois.edu/{\{}{~}{\}}dautenh1/downloads/publications/KCoFI-Oakland-2014.pdf},
year = {2014}
}
@article{Zhou2014,
abstract = {To be trustworthy, security-sensitive applications must be formally verified and hence small and simple, i.e., wimpy. Thus, they cannot include a variety of basic services available only in large and untrustworthy commodity systems, i.e., in giants. Hence, wimps must securely compose with giants to survive on commodity systems, i.e., rely on giants' services but only after efficiently verifying their results. This paper presents a security architecture based on a wimpy kernel that provides on-demand isolated I/O channels for wimp applications, without bloating the underlying trusted computing base. The size and complexity of the wimpy kernel are minimized by safely outsourcing I/O subsystem functions to an untrusted commodity operating system and exporting driver and I/O subsystem code to wimp applications. Using the USB subsystem as a case study, this paper illustrates the dramatic reduction of wimpy-kernel size and complexity, e.g., over 99{\{}{\%}{\}} of the USB code base is removed. Performance measurements indicate that the wimpy-kernel architecture exhibits the desired execution efficiency.},
author = {Zhou, Zongwei and Yu, Miao and Gligor, Virgil D},
doi = {10.1109/SP.2014.27},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Complexity theory,Hardware,I/O subsystem functions,Kernel,Linux,Security,USB code base,USB subsystem,Universal Serial Bus,commodity systems,formal verification,giants services,on-demand isolated I/O channels,operating systems (computers),peripheral interfaces,security architecture,security-sensitive applications,software architecture,trusted computing,trustworthy,untrusted commodity operating system,wimp applications,wimpy kernel complexity,wimpy kernel size,wimpy-kernel architecture},
pages = {308--323},
title = {{Dancing with Giants: Wimpy Kernels for On-Demand Isolated I/O}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956572},
year = {2014}
}
@article{Sen2014,
abstract = {With the rapid increase in cloud services collecting and using user data to offer personalized experiences, ensuring that these services comply with their privacy policies has become a business imperative for building user trust. However, most compliance efforts in industry today rely on manual review processes and audits designed to safeguard user data, and therefore are resource intensive and lack coverage. In this paper, we present our experience building and operating a system to automate privacy policy compliance checking in Bing. Central to the design of the system are (a) Legal ease-a language that allows specification of privacy policies that impose restrictions on how user data is handled, and (b) Grok-a data inventory for Map-Reduce-like big data systems that tracks how user data flows among programs. Grok maps code-level schema elements to data types in Legal ease, in essence, annotating existing programs with information flow types with minimal human input. Compliance checking is thus reduced to information flow analysis of Big Data systems. The system, bootstrapped by a small team, checks compliance daily of millions of lines of ever-changing source code written by several thousand developers.},
author = {Sen, Shayak and Guha, Saikat and Datta, Anupam and Rajamani, Sriram K and Tsai, Janice and Wing, Jeannette M},
doi = {10.1109/SP.2014.28},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Advertising,Big Data,Big data,Bing,Data privacy,Grok data inventory,IP networks,Lattices,Legal ease language,Map-Reduce-like Big Data systems,Privacy,Semantics,Web services,automatic privacy policy compliance checking,big data,bing,business imperative privacy policies,cloud computing,cloud services,code-level schema element mapping,compliance,computer bootstrapping,conformance testing,data privacy,datatypes,information flow,information flow types,minimal human input,parallel programming,personalized user experiences,policy,privacy,privacy compliance bootstrapping,privacy policy specification,program analysis,program annotation,search engines,source code,source code (software),user data handling,user trust},
pages = {327--342},
title = {{Bootstrapping Privacy Compliance in Big Data Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956573},
year = {2014}
}
@article{Kusters2014,
abstract = {Mix nets with randomized partial checking (RPC mix nets) have been introduced by Jakobsson, Juels, and Rivest as particularly simple and efficient verifiable mix nets. These mix nets have been used in several implementations of prominent e-voting systems to provide vote privacy and verifiability. In RPC mix nets, higher efficiency is traded for a lower level of privacy and verifiability. However, these mix nets have never undergone a rigorous formal analysis. Recently, Kahazei and Wikstroem even pointed out several severe problems in the original proposal and in implementations of RPC mix nets in e-voting systems, both for so-called re-encryption and Chaumian RPC mix nets. While Kahazei and Wikstroem proposed several fixes, the security status of Chaumian RPC mix nets (with the fixes applied) has been left open, re-encryption RPC mix nets, as they suggest, should not be used at all. In this paper, we provide the first formal security analysis of Chaumian RPC mix nets. We propose security definitions that allow one to measure the level of privacy and verifiability RPC mix nets offer, and then based on these definitions, carry out a rigorous analysis. Altogether, our results show that these mix nets provide a reasonable level of privacy and verifiability, and that they are still an interesting option for the use in e-voting systems.},
author = {Kusters, Ralf and Truderung, Tomasz and Vogt, Andreas},
doi = {10.1109/SP.2014.29},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {-mix nets,Accountability,Chaumian RPC mix nets,Cryptographic Analysis,Electronic voting,Mix Nets,Privacy,Protocols,Public key,Random Partial Checking,Servers,Verifiability,accountability,crypto-,cryptography,data privacy,e-voting systems,formal security analysis,government data processing,graphic analysis,politics,privacy,random partial checking,randomized partial checking,reencryption RPC mix nets,verifiability,vote privacy,vote verifiability},
pages = {343--358},
title = {{Formal Analysis of Chaumian Mix Nets with Randomized Partial Checking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956574},
year = {2014}
}
@article{Pappas2014,
abstract = {Query privacy in secure DBMS is an important feature, although rarely formally considered outside the theoretical community. Because of the high overheads of guaranteeing privacy in complex queries, almost all previous works addressing practical applications consider limited queries (e.g., just keyword search), or provide a weak guarantee of privacy. In this work, we address a major open problem in private DB: efficient sub linear search for arbitrary Boolean queries. We consider scalable DBMS with provable security for all parties, including protection of the data from both server (who stores encrypted data) and client (who searches it), as well as protection of the query, and access control for the query. We design, build, and evaluate the performance of a rich DBMS system, suitable for real-world deployment on today medium-to large-scale DBs. On a modern server, we are able to query a formula over 10TB, 100M-record DB, with 70 searchable index terms per DB row, in time comparable to (insecure) MySQL (many practical queries can be privately executed with work 1.2-3 times slower than MySQL, although some queries are costlier). We support a rich query set, including searching on arbitrary boolean formulas on keywords and ranges, support for stemming, and free keyword searches over text fields. We identify and permit a reasonable and controlled amount of leakage, proving that no further leakage is possible. In particular, we allow leakage of some search pattern information, but protect the query and data, provide a high level of privacy for individual terms in the executed search formula, and hide the difference between a query that returned no results and a query that returned a very small result set. We also support private and complex access policies, integrated in the search process so that a query with empty result set and a query that fails the policy are hard to tell apart.},
author = {Pappas, Vasilis and Krell, Fernando and Vo, Binh and Kolesnikov, Vladimir and Malkin, Tal and Choi, Seung Geol and George, Wesley and Keromytis, Angelos and Bellovin, Steve},
doi = {10.1109/SP.2014.30},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Blind Seer,Cryptography,Data privacy,Indexes,MySQL,Privacy,Private database,Servers,arbitrary Boolean queries,data privacy,data protection,database management systems,encrypted data,free keyword searches,medium-to large-scale DB,private search,query control,query privacy,query processing,scalable private DBMS,search pattern information,secure DBMS,stemming support,sublinear search,text fields},
pages = {359--374},
title = {{Blind Seer: A Scalable Private DBMS}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956575},
year = {2014}
}
@article{Hohenberger2014,
abstract = {A secure ad-hoc survey scheme enables a survey authority to independently (without any interaction) select an ad-hoc group of registered users based only on their identities (e.g., their email addresses), and create a survey where only selected users can anonymously submit exactly one response. We present a formalization of secure ad-hoc surveys and a provably-secure implementation in the random oracle model, called ANONIZE. Our performance analysis shows that ANONIZE enables securely implementing million-person anonymous surveys using a single modern workstation. As far as we know, ANONIZE constitutes the first implementation of a large-scale secure computation protocol (of non-trivial functionalities) that scales to millions of users.},
author = {Hohenberger, Susan and Myers, Steven and Pass, Rafael and abhi Shelat},
doi = {10.1109/SP.2014.31},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {ANONIZE,Abstracts,Educational institutions,Electronic mail,Protocols,Public key,Registers,ad-hoc group,authorisation,cryptographic protocols,email addresses,large-scale anonymous survey system,large-scale secure computation protocol,modern workstation,nontrivial functionalities,person anonymous surveys,provably-secure implementation,random oracle model,secure ad-hoc survey scheme,secure anonymous survey system,survey authority,user identities},
pages = {375--389},
title = {{ANONIZE: A Large-Scale Anonymous Survey System}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956576},
year = {2014}
}
@article{Xing2014,
abstract = {Android is a fast evolving system, with new updates coming out one after another. These updates often completely overhaul a running system, replacing and adding tens of thousands of files across Android's complex architecture, in the presence of critical user data and applications (apps for short). To avoid accidental damages to such data and existing apps, the upgrade process involves complicated program logic, whose security implications, however, are less known. In this paper, we report the first systematic study on the Android updating mechanism, focusing on its Package Management Service (PMS). Our research brought to light a new type of security-critical vulnerabilities, called Pileup flaws, through which a malicious app can strategically declare a set of privileges and attributes on a low-version operating system (OS) and wait until it is upgraded to escalate its privileges on the new system. Specifically, we found that by exploiting the Pileup vulnerabilities, the app can not only acquire a set of newly added system and signature permissions but also determine their settings (e.g., protection levels), and it can further substitute for new system apps, contaminate their data (e.g., cache, cookies of Android default browser) to steal sensitive user information or change security configurations, and prevent installation of critical system services. We systematically analyzed the source code of PMS using a program verification tool and confirmed the presence of those security flaws on all Android official versions and over 3000 customized versions. Our research also identified hundreds of exploit opportunities the adversary can leverage over thousands of devices across different device manufacturers, carriers and countries. To mitigate this threat without endangering user data and apps during an upgrade, we also developed a new detection service, called SecUP, which deploys a scanner on the user's device to capture the malicious apps designed to exploit Pileu- vulnerabilities, based upon the vulnerability-related information automatically collected from newly released Android OS images.},
author = {Xing, Luyi and Pan, Xiaorui and Wang, Rui and Yuan, Kan and Wang, XiaoFeng},
doi = {10.1109/SP.2014.32},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Android,Android (operating system),Android OS images,Androids,Google,Humanoid robots,Mobile communication,OS update,PMS,Package Manager Service,Pileup flaws,Privilege Escalation,Registers,Security,Smart phones,formal verification,invasive software,low-version operating system,malicious apps,malware,mobile OS updating,mobile computing,operating systems,package management service,program verification tool,security configuration,security-critical vulnerability,user information},
pages = {393--408},
title = {{Upgrading Your Android, Elevating My Malware: Privilege Escalation through Mobile OS Updating}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956577},
year = {2014}
}
@article{Zhou2014a,
abstract = {Android phone manufacturers are under the perpetual pressure to move quickly on their new models, continuously customizing Android to fit their hardware. However, the security implications of this practice are less known, particularly when it comes to the changes made to Android's Linux device drivers, e.g., those for camera, GPS, NFC etc. In this paper, we report the first study aimed at a better understanding of the security risks in this customization process. Our study is based on ADDICTED, a new tool we built for automatically detecting some types of flaws in customized driver protection. Specifically, on a customized phone, ADDICTED performs dynamic analysis to correlate the operations on a security-sensitive device to its related Linux files, and then determines whether those files are under-protected on the Linux layer by comparing them with their counterparts on an official Android OS. In this way, we can detect a set of likely security flaws on the phone. Using the tool, we analyzed three popular phones from Samsung, identified their likely flaws and built end-to-end attacks that allow an unprivileged app to take pictures and screenshots, and even log the keys the user enters through touch screen. Some of those flaws are found to exist on over a hundred phone models and affect millions of users. We reported the flaws and helped the manufacturers fix those problems. We further studied the security settings of device files on 2423 factory images from major phone manufacturers, discovered over 1,000 vulnerable images and also gained insights about how they are distributed across different Android versions, carriers and countries.},
author = {Zhou, Xiaoyong and Lee, Yeonjoon and Zhang, Nan and Naveed, Muhammad and Wang, XiaoFeng},
doi = {10.1109/SP.2014.33},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {ADDICTED tool,Android (operating system),Android Linux device driver customizations,Android OS,Android Security,Android phone manufacturers,Cameras,Hardware,Kernel,Linux,Linux files,Linux layer,Performance evaluation,Privacy,Samsung phones,Security,Smart phones,computer crime,customized driver protection,customized phone,device drivers,device files,dynamic analysis,end-to-end attacks,flaw detection,flaws detection,phone models,security flaws,security hazards,security implications,security risks,security settings,security-sensitive device,smart phones},
pages = {409--423},
title = {{The Peril of Fragmentation: Security Hazards in Android Device Driver Customizations}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956578},
year = {2014}
}
@article{Lee2014a,
abstract = {There have been many research efforts to secure Android applications and the high-level system mechanisms. The low-level operating system designs have been overlooked partially due to the belief that security issues at this level are similar to those on Linux, which are well-studied. However, we identify that certain Android modifications are at odds with security and result in serious vulnerabilities that need to be addressed immediately. In this paper, we analyze the Zygote process creation model, an Android operating system design for speeding up application launches. Zygote weakens Address Space Layout Randomization (ASLR) because all application processes are created with largely identical memory layouts. We design both remote and local attacks capable of bypassing the weakened ASLR and executing return-oriented programming on Android. We demonstrate the attacks using real applications, such as the Chrome Browser and VLC Media Player. Further, we design and implement Morula, a secure replacement for Zygote. Morula introduces a small amount of code to the Android operating system and can be easily adopted by device vendors. Our evaluation shows that, compared to Zygote, Morula incurs a 13 MB memory increase for each running application but allows each Android process to have an individually randomized memory layout and even a slightly shorter average launch time.},
author = {Lee, Byoungyoung and Lu, Long and Wang, Tielei and Kim, Taesoo and Lee, Wenke},
doi = {10.1109/SP.2014.34},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {ASLR,Android,Android (operating system),Android modifications,Android operating system design,Androids,Browsers,Chrome browser,Humanoid robots,Layout,Libraries,Media,Morula,Security,VLC media player,Zygote process creation model,address space layout randomization,individually randomized memory layout,low-level operating system designs,return-oriented programming,security issues,security of data,weakened ASLR fortification},
pages = {424--439},
title = {{From Zygote to Morula: Fortifying Weakened ASLR on Android}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956579},
year = {2014}
}
@article{Andrychowicz2014,
abstract = {Bit coin is a decentralized digital currency, introduced in 2008, that has recently gained noticeable popularity. Its main features are: (a) it lacks a central authority that controls the transactions, (b) the list of transactions is publicly available, and (c) its syntax allows more advanced transactions than simply transferring the money. The goal of this paper is to show how these properties of Bit coin can be used in the area of secure multiparty computation protocols (MPCs). Firstly, we show that the Bit coin system provides an attractive way to construct a version of "timed commitments", where the committer has to reveal his secret within a certain time frame, or to pay a fine. This, in turn, can be used to obtain fairness in some multiparty protocols. Secondly, we introduce a concept of multiparty protocols that work "directly on Bit coin". Recall that the standard definition of the MPCs guarantees only that the protocol "emulates the trusted third party". Hence ensuring that the inputs are correct, and the outcome is respected is beyond the scope of the definition. Our observation is that the Bit coin system can be used to go beyond the standard "emulation-based" definition, by constructing protocols that link their inputs and the outputs with the real Bit coin transactions. As an instantiation of this idea we construct protocols for secure multiparty lotteries using the Bit coin currency, without relying on a trusted authority (one of these protocols uses the Bit coin-based timed commitments mentioned above). Our protocols guarantee fairness for the honest parties no matter how the loser behaves. For example: if one party interrupts the protocol then her money is transferred to the honest participants. Our protocols are practical (to demonstrate it we performed their transactions in the actual Bit coin system), and can be used in real life as a replacement for the online gambling sites. We think that this paradigm can have also other applications. We discu- s some of them.},
author = {Andrychowicz, Marcin and Dziembowski, Stefan and Malinowski, Daniel and Mazurek, Lukasz and Mazurek, Łukasz},
doi = {10.1109/SP.2014.35},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Bitcoin,Cryptography,Games,Internet,MPC,Online banking,Protocols,Standards,and the function that,as an illustration of,bitcoin,bitocoin,called alice and bob,case when there are,cryptographic protocols,curity definition consider the,decentralized digital currency,electronic money,emulation-based definition,lottery,multiparty,multiparty computations,online gambling sites,only two,participants,secure multiparty computation protocols,secure multiparty lotteries,such se-,the practical meaning of,they,timed commitments},
number = {c},
pages = {443--458},
title = {{Secure Multiparty Computations on Bitcoin}},
url = {http://www.ieee-security.org/TC/SP2014/papers/toc.html$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956580},
year = {2014}
}
@article{Ben-sasson2014,
abstract = {Bitcoin is the first digital currency to see widespread adoption. While payments are conducted between pseudonyms, Bitcoin cannot offer strong privacy guarantees: payment transactions are recorded in a public decentralized ledger, from which much information can be deduced. Zerocoin (Miers et al., IEEE S{\{}{\&}{\}}P 2013) tackles some of these privacy issues by unlinking transactions from the payment's origin. Yet, it still reveals payments' destinations and amounts, and is limited in functionality.$\backslash$n$\backslash$nIn this paper, we construct a full-fledged ledger-based digital currency with strong privacy guarantees. Our results leverage recent advances in zero-knowledge Succinct Non-interactive ARguments of Knowledge (zk-SNARKs).$\backslash$n$\backslash$nFirst, we formulate and construct decentralized anonymous payment schemes (DAP schemes). A DAP scheme enables users to directly pay each other privately: the corresponding transaction hides the payment's origin, destination, and transferred amount. We provide formal definitions and proofs of the construction's security.$\backslash$n$\backslash$nSecond, we build Zerocash, a practical instantiation of our DAP scheme construction. In Zerocash, transactions are less than 1 kB and take under 6 ms to verify --- orders of magnitude more efficient than the less-anonymous Zerocoin and competitive with plain Bitcoin.},
author = {Ben-sasson, Eli and Chiesa, Alessandro and Garman, Christina and Green, Matthew and Miers, Ian and Tromer, Eran},
doi = {10.1109/SP.2014.36},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {bitcoin,decentralized electronic cash,zero-knowledge proofs},
pages = {1--56},
title = {{Zerocash : Decentralized Anonymous Payments from Bitcoin ( extended version )}},
year = {2014}
}
@article{Miller2014,
abstract = {Bit coin is widely regarded as the first broadly successful e-cash system. An oft-cited concern, though, is that mining Bit coins wastes computational resources. Indeed, Bit coin's underlying mining mechanism, which we call a scratch-off puzzle (SOP), involves continuously attempting to solve computational puzzles that have no intrinsic utility. We propose a modification to Bit coin that repurposes its mining resources to achieve a more broadly useful goal: distributed storage of archival data. We call our new scheme Perm coin. Unlike Bit coin and its proposed alternatives, Perm coin requires clients to invest not just computational resources, but also storage. Our scheme involves an alternative scratch-off puzzle for Bit coin based on Proofs-of-Retrievability (PORs). Successfully minting money with this SOP requires local, random access to a copy of a file. Given the competition among mining clients in Bit coin, this modified SOP gives rise to highly decentralized file storage, thus reducing the overall waste of Bit coin. Using a model of rational economic agents we show that our modified SOP preserves the essential properties of the original Bit coin puzzle. We also provide parameterizations and calculations based on realistic hardware constraints to demonstrate the practicality of Perm coin as a whole.},
author = {Miller, Andrew and Juels, Ari and Shi, Elaine and Parno, Bryan and Katz, Jonathan},
doi = {10.1109/SP.2014.37},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Bit coin mining,Bitcoin,Data mining,Investment,Online banking,Outsourcing,Peer-to-peer computing,Perm coin,Permacoin,Public key,archival data,computational puzzles,computational resources,data mining,data preservation,digital storage,distributed storage,e-cash system,electronic money,proofs-of-retrievability,rational economic agents,scratch-off puzzle},
pages = {475--490},
title = {{Permacoin: Repurposing Bitcoin Work for Data Preservation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956582},
year = {2014}
}
@article{Peddinti2014,
abstract = {Most of what we understand about data sensitivity is through user self-report (e.g., surveys); this paper is the first to use behavioral data to determine content sensitivity, via the clues that users give as to what information they consider private or sensitive through their use of privacy enhancing product features. We perform a large-scale analysis of user anonymity choices during their activity on Quora, a popular question-and-answer site.We identify categories of questions for which users are more likely to exercise anonymity and explore several machine learning approaches towards predicting whether a particular answer will be written anonymously. Our findings validate the viability of the proposed approach towards an automatic assessment of data sensitivity, show that data sensitivity is a nuanced measure that should be viewed on a continuum rather than as a binary concept, and advance the idea that machine learning over behavioral data can be effectively used in order to develop product features that can help keep users safe.},
author = {Peddinti, Sai Teja and Korolova, Aleksandra and Bursztein, Elie and Sampemane, Geetanjali},
doi = {10.1109/SP.2014.38},
isbn = {978-1-4799-4686-0},
issn = {10816011},
journal = {s{\&}p},
pages = {493--508},
title = {{Cloak and Swagger: Understanding Data Sensitivity Through the Lens of User Anonymity}},
year = {2014}
}
@article{Lopes2014,
abstract = {RaptorQ is the most advanced fountain code proposed so far. Its properties make it attractive for forward error correction (FEC), offering high reliability at low overheads (i.e., for a small amount of repair information) and efficient encoding and decoding operations. Since RaptorQ's emergence, it has already been standardized by the IETF, and there is the expectation that it will be adopted by several other standardization bodies, in areas related to digital media broadcast, cellular networks, and satellite communications. The paper describes a new attack on RaptorQ that breaks the near ideal FEC performance, by carefully choosing which packets are allowed to reach the receiver. Furthermore, the attack was extended to be performed over secure channels with IPsec/ESP. The paper also proposes a few solutions to protect the code from the attack, which could be easily integrated into the implementations.},
author = {Lopes, Jose and Neves, Nuno},
doi = {10.1109/SP.2014.39},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Decoding,Encoding,Equations,FEC,Generators,IETF,IPsec-ESP,Maintenance engineering,Mathematical model,RaptorQ,Receivers,advanced fountain code,cellular networks,code protection,cryptography,data protection,decoding,decoding operations,digital media broadcast,encoding,encoding operations,forward error correction,fountain codes,malicious faults,raptor codes,resilience,satellite communications,secure channels,telecommunication security},
pages = {509--523},
title = {{Stopping a Rapid Tornado with a Puff}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956584},
year = {2014}
}
@article{Rushanan2014,
author = {Rushanan, Michael and Rubin, Aviel D and Kune, Denis Foo and Swanson, Colleen M},
doi = {10.1109/SP.2014.40},
isbn = {978-1-4799-4686-0},
journal = {s{\&}p},
keywords = {body area networks,health,implantable medical devices,privacy,security},
pages = {524--539},
title = {{SoK: Security and Privacy in Implantable Medical Devices and Body Area Networks}},
url = {http://dl.acm.org/citation.cfm?id=2650286.2650767},
year = {2014}
}
@article{Mardziel2014,
abstract = {A metric is proposed for quantifying leakage of information about secrets and about how secrets change over time. The metric is used with a model of information flow for probabilistic, interactive systems with adaptive adversaries. The model and metric are implemented in a probabilistic programming language and used to analyze several examples. The analysis demonstrates that adaptivity increases information flow.},
author = {Mardziel, Piotr and Alvim, Mario S and Hicks, Michael and Clarkson, Michael R},
doi = {10.1109/SP.2014.41},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Adaptation models,Automata,Context,History,Measurement,Probabilistic logic,Security,cryptography,dynamic secret,dynamic secrets,gain function,high level languages,information flow,information leakage,interactive systems,probabilistic programming,probabilistic programming language,probabilistic systems,probability,quantitative information flow,vulnerability},
pages = {540--555},
title = {{Quantifying Information Flow for Dynamic Secrets}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956586},
year = {2014}
}
@article{Everspaugh2014,
abstract = {Virtualized environments are widely thought to cause problems for software-based random number generators (RNGs), due to use of virtual machine (VM) snapshots as well as fewer and believed-to-be lower quality entropy sources. Despite this, we are unaware of any published analysis of the security of critical RNGs when running in VMs. We fill this gap, using measurements of Linux's RNG systems (without the aid of hardware RNGs, the most common use case today) on Xen, VMware, and Amazon EC2. Despite CPU cycle counters providing a significant source of entropy, various deficiencies in the design of the Linux RNG makes its first output vulnerable during VM boots and, more critically, makes it suffer from catastrophic reset vulnerabilities. We show cases in which the RNG will output the exact same sequence of bits each time it is resumed from the same snapshot. This can compromise, for example, cryptographic secrets generated after resumption. We explore legacy-compatible countermeasures, as well as a clean-slate solution. The latter is a new RNG called Whirlwind that provides a simpler, more-secure solution for providing system randomness.},
author = {Everspaugh, Adam and Zhai, Yan and Jellinek, Robert and Ristenpart, Thomas and Swift, Michael},
doi = {10.1109/SP.2014.42},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {-random number generator,virtualization},
pages = {559--574},
title = {{Not-So-Random Numbers in Virtualized Linux and the Whirlwind RNG}},
url = {http://ieeexplore.ieee.org/ielx7/6954656/6956545/06956587.pdf?tp={\{}{\&}{\}}arnumber=6956587{\{}{\&}{\}}isnumber=6956545$\backslash$nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6956587},
year = {2014}
}
@article{Gokta2014,
author = {G{\"{o}}kta, Enes and Athanasopoulos, Elias},
journal = {s{\&}p},
keywords = {code-reuse attack,control-flow integrity evaluation},
title = {{Out Of Control : Overcoming Control-Flow Integrity}},
year = {2014}
}
@article{Yamaguchi2014,
abstract = {The vast majority of security breaches encountered today are a direct result of insecure code. Consequently, the protection of computer systems critically depends on the rigorous identification of vulnerabilities in software, a tedious and error-prone process requiring significant expertise. Unfortunately, a single flaw suffices to undermine the security of a system and thus the sheer amount of code to audit plays into the attacker's cards. In this paper, we present a method for effectively mining large amounts of source code for vulnerabilities. To this end, we introduce a novel representation of source code called a code property graph that merges concepts of classic program analysis, namely abstract syntax trees, control flow graphs and program dependence graphs, into a joint data structure. This comprehensive representation enables us to elegantly model templates for common vulnerabilities with graph traversals that, for instance, can identify buffer overflows, integer overflows, format string vulnerabilities, or memory disclosures. We implement our approach using a popular graph database and demonstrate its efficacy by identifying 18 previously unknown vulnerabilities in the source code of the Linux kernel.},
author = {Yamaguchi, Fabian and Golde, Nico and Arp, Daniel and Rieck, Konrad},
doi = {10.1109/SP.2014.44},
isbn = {978-1-4799-4686-0},
issn = {10816011},
journal = {s{\&}p},
keywords = {graph databases,static analysis,vulnerabilities},
pages = {590--604},
title = {{Modeling and Discovering Vulnerabilities with Code Property Graphs}},
url = {http://user.informatik.uni-goettingen.de/{\{}{~}{\}}krieck/docs/2014-ieeesp.pdf$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956589},
year = {2014}
}
@article{Jain2014,
abstract = {An essential goal of Virtual Machine Introspection (VMI) is assuring security policy enforcement and overall functionality in the presence of an untrustworthy OS. A fundamental obstacle to this goal is the difficulty in accurately extracting semantic meaning from the hypervisor's hardware level view of a guest OS, called the semantic gap. Over the twelve years since the semantic gap was identified, immense progress has been made in developing powerful VMI tools. Unfortunately, much of this progress has been made at the cost of reintroducing trust into the guest OS, often in direct contradiction to the underlying threat model motivating the introspection. Although this choice is reasonable in some contexts and has facilitated progress, the ultimate goal of reducing the trusted computing base of software systems is best served by a fresh look at the VMI design space. This paper organizes previous work based on the essential design considerations when building a VMI system, and then explains how these design choices dictate the trust model and security properties of the overall system. The paper then observes portions of the VMI design space which have been under-explored, as well as potential adaptations of existing techniques to bridge the semantic gap without trusting the guest OS. Overall, this paper aims to create an essential checkpoint in the broader quest for meaningful trust in virtualized environments through VM introspection.},
author = {Jain, Bhushan and Baig, Mirza Basim and Zhang, Dongli and Porter, Donald E and Sion, Radu},
doi = {10.1109/SP.2014.45},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {-vm introspection,Data structures,Kernel,Linux,Monitoring,Security,Semantics,VM Introspection,VMI system,Virtual machine monitors,information retrieval,security of data,semantic gap,semantic meaning extraction,system security,trust,trusted computing,trusted computing base,virtual machine introspection,virtual machines,virtualisation,virtualized environments},
pages = {605--620},
title = {{SoK: Introspections on Trust and the Semantic Gap}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956590},
year = {2014}
}
@article{Liu2014,
abstract = {RAM-model secure computation addresses the inherent limitations of circuit-model secure computation considered in almost all previous work. Here, we describe the first automated approach for RAM-model secure computation in the semi-honest model. We define an intermediate representation called SCVM and a corresponding type system suited for RAM-model secure computation. Leveraging compile-time optimizations, our approach achieves order-of-magnitude speedups compared to both circuit-model secure computation and the state-of-art RAM-model secure computation.},
author = {Liu, Chang and Huang, Yan and Shi, Elaine and Katz, Jonathan and Hicks, Michael},
doi = {10.1109/SP.2014.46},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Arrays,Computational modeling,Integrated circuit modeling,Program processors,Protocols,Random access memory,SCVM,Security,automated RAM-model secure computation approach,compile-time optimizations,intermediate representation,order-of-magnitude speedups,program compilers,random access machine,random-access storage,security of data},
pages = {623--638},
title = {{Automating Efficient RAM-Model Secure Computation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956591},
year = {2014}
}
@article{Naveed2014,
abstract = {Dynamic Searchable Symmetric Encryption allows a client to store a dynamic collection of encrypted documents with a server, and later quickly carry out keyword searches on these encrypted documents, while revealing minimal information to the server. In this paper we present a new dynamic SSE scheme that is simpler and more efficient than existing schemes while revealing less information to the server than prior schemes, achieving fully adaptive security against honest-but-curious servers. We implemented a prototype of our scheme and demonstrated its efficiency on datasets from prior work. Apart from its concrete efficiency, our scheme is also simpler: in particular, it does not require the server to support any operation other than upload and download of data. Thus the server in our scheme can be based solely on a cloud storage service, rather than a cloud computation service as well, as in prior work. In building our dynamic SSE scheme, we introduce a new primitive called Blind Storage, which allows a client to store a set of files on a remote server in such a way that the server does not learn how many files are stored, or the lengths of the individual files, as each file is retrieved, the server learns about its existence (and can notice the same file being downloaded subsequently), but the file's name and contents are not revealed. This is a primitive with several applications other than SSE, and is of independent interest.},
author = {Naveed, Muhammad and Prabhakaran, Manoj and Gunter, Carl A},
doi = {10.1109/SP.2014.47},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Adaptation models,Cloud computing,Encryption,Privacy,Servers,blind storage,cloud computation service,cloud computing,cloud security,cloud storage service,cryptography,dynamic SSE scheme,dynamic searchable encryption,dynamic searchable symmetric encryption,fully adaptive security,honest-but-curious server security,keyword search,secure cloud storage,storage management},
pages = {639--654},
title = {{Dynamic Searchable Encryption via Blind Storage}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956592},
year = {2014}
}
@article{Rastogi2014,
abstract = {In a Secure Multiparty Computation (SMC), mutually distrusting parties use cryptographic techniques to cooperatively compute over their private data, in the process each party learns only explicitly revealed outputs. In this paper, we present Wysteria, a high-level programming language for writing SMCs. As with past languages, like Fairplay, Wysteria compiles secure computations to circuits that are executed by an underlying engine. Unlike past work, Wysteria provides support for mixed-mode programs, which combine local, private computations with synchronous SMCs. Wysteria complements a standard feature set with built-in support for secret shares and with wire bundles, a new abstraction that supports generic n-party computations. We have formalized Wysteria, its refinement type system, and its operational semantics. We show that Wysteria programs have an easy-to-understand single-threaded interpretation and prove that this view corresponds to the actual multi-threaded semantics. We also prove type soundness, a property we show has security ramifications, namely that information about one party's data can only be revealed to another via (agreed upon) secure computations. We have implemented Wysteria, and used it to program a variety of interesting SMC protocols from the literature, as well as several new ones. We find that Wysteria's performance is competitive with prior approaches while making programming far easier, and more trustworthy.},
author = {Rastogi, Aseem and Hammer, Matthew A and Hicks, Michael},
doi = {10.1109/SP.2014.48},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Cryptography,Dependent type system,Educational institutions,Functional language,Protocols,SMC protocols,Secure multi-party computation,Semantics,Standards,Wires,Writing,Wysteria compiles,Wysteria performance,cryptographic protocols,cryptographic techniques,data privacy,formalized Wysteria programs,generic n-party computations,high level languages,high level programming language,mixed mode multiparty computations,mixed-mode programs,multi-threading,multithreaded semantics,operational semantics,private data,refinement type system,secure computations,secure multiparty computation,security ramifications,synchronous SMC},
pages = {655--670},
title = {{Wysteria: A Programming Language for Generic, Mixed-Mode Multiparty Computations}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956593},
year = {2014}
}
@article{Fett2014,
abstract = {The web constitutes a complex infrastructure and, as demonstrated by numerous attacks, rigorous analysis of standards and web applications is indispensable. Inspired by successful prior work, in particular the work by Akhawe et al. as well as Bansal et al., in this work we propose a formal model for the web infrastructure. While unlike prior works, which aim at automatic analysis, our model so far is not directly amenable to automation, it is much more comprehensive and accurate with respect to the standards and specifications. As such, it can serve as a solid basis for the analysis of a broad range of standards and applications. As a case study and another important contribution of our work, we use our model to carry out the first rigorous analysis of the Browser ID system (a.k.a. Mozilla Persona), a recently developed complex real-world single sign-on system that employs technologies such as AJAX, cross-document messaging, and HTML5 web storage. Our analysis revealed a number of very critical flaws that could not have been captured in prior models. We propose fixes for the flaws, formally state relevant security properties, and prove that the fixed system in a setting with a so-called secondary identity provider satisfies these security properties in our model. The fixes for the most critical flaws have already been adopted by Mozilla and our findings have been rewarded by the Mozilla Security Bug Bounty Program.},
archivePrefix = {arXiv},
arxivId = {arXiv:1403.1866v2},
author = {Fett, Daniel and Kusters, Ralf and Schmitz, Guido},
doi = {10.1109/SP.2014.49},
eprint = {arXiv:1403.1866v2},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {AJAX,Analytical models,Browsers,Formal Security Analysis,HTML5 web storage,Internet,Mathematical model,Mozilla security bug bounty program,Security,Single Sign-on,Standards,Web Model,Web Security,Web applications,Web infrastructure,Web security,Web servers,automatic analysis,browserID SSO system,cross-document messaging,formal model,formal security analysis,online front-ends,program debugging,rigorous analysis,secondary identity provider,security of data,single sign-on system,state relevant security properties},
pages = {673--688},
title = {{An Expressive Model for the Web Infrastructure: Definition and Application to the Browser ID SSO System}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956594},
year = {2014}
}
@article{Ma2014,
abstract = {A probabilistic password model assigns a probability value to each string. Such models are useful for research into understanding what makes users choose more (or less) secure passwords, and for constructing password strength meters and password cracking utilities. Guess number graphs generated from password models are a widely used method in password research. In this paper, we show that probability-threshold graphs have important advantages over guess-number graphs. They are much faster to compute, and at the same time provide information beyond what is feasible in guess-number graphs. We also observe that research in password modeling can benefit from the extensive literature in statistical language modeling. We conduct a systematic evaluation of a large number of probabilistic password models, including Markov models using different normalization and smoothing methods, and found that, among other things, Markov models, when done correctly, perform significantly better than the Probabilistic Context-Free Grammar model proposed in Weir et al., which has been used as the state-of-the-art password model in recent research.},
author = {Ma, Jerry and Yang, Weining and Luo, Min and Li, Ninghui},
doi = {10.1109/SP.2014.50},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Computational modeling,Dictionaries,Educational institutions,Markov models,Markov processes,Probabilistic logic,Testing,Training,graph theory,guess number graphs,password cracking utilities,password strength meters,probabilistic password models,probability,probability-threshold graphs,secure passwords,security of data,statistical language modeling},
pages = {689--704},
title = {{A Study of Probabilistic Password Models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956595},
year = {2014}
}
@article{Mare2014,
abstract = {Common authentication methods based on passwords, tokens, or fingerprints perform one-time authentication and rely on users to log out from the computer terminal when they leave. Users often do not log out, however, which is a security risk. The most common solution, inactivity timeouts, inevitably fail security (too long a timeout) or usability (too short a timeout) goals. One solution is to authenticate users continuously while they are using the terminal and automatically log them out when they leave. Several solutions are based on user proximity, but these are not sufficient: they only confirm whether the user is nearby but not whether the user is actually using the terminal. Proposed solutions based on behavioral biometric authentication (e.g., keystroke dynamics) may not be reliable, as a recent study suggests. To address this problem we propose Zero-Effort Bilateral Recurring Authentication (ZEBRA). In ZEBRA, a user wears a bracelet (with a built-in accelerometer, gyroscope, and radio) on her dominant wrist. When the user interacts with a computer terminal, the bracelet records the wrist movement, processes it, and sends it to the terminal. The terminal compares the wrist movement with the inputs it receives from the user (via keyboard and mouse), and confirms the continued presence of the user only if they correlate. Because the bracelet is on the same hand that provides inputs to the terminal, the accelerometer and gyroscope data and input events received by the terminal should correlate because their source is the same - the user's hand movement. In our experiments ZEBRA performed continuous authentication with 85{\{}{\%}{\}} accuracy in verifying the correct user and identified all adversaries within 11s. For a different threshold that trades security for usability, ZEBRA correctly verified 90{\{}{\%}{\}} of users and identified all adversaries within 50s.},
author = {Mare, Shrirang and Markham, Andres Molina and Cornelius, Cory and Peterson, Ronald and Kotz, David},
doi = {10.1109/SP.2014.51},
isbn = {978-1-4799-4686-0},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Accelerometers,Authentication,Gyroscopes,Keyboards,Mice,Wrist,ZEBRA,accelerometers,authentication methods,authorisation,behavioral biometric authentication,built-in accelerometer,computer terminal,continuous authentication,deauthentication,fingerprints,gyroscope,gyroscopes,human computer interaction,inactivity timeouts,interactive terminals,keystroke dynamics,message authentication,one-time authentication,passwords,radio,security,security risk,tokens,usability,user bracelet,user hand movement,user interaction,user proximity,users authentication,wearable,wrist movement,zero-effort bilateral recurring authentication},
pages = {705--720},
title = {{ZEBRA: Zero-Effort Bilateral Recurring Authentication}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6956596},
year = {2014}
}
@article{Zahur2013,
abstract = {Several techniques in computer security, including generic protocols for secure computation and symbolic execution, depend on implementing algorithms in static circuits. Despite substantial improvements in recent years, tools built using these techniques remain too slow for most practical uses. They require transforming arbitrary programs into either Boolean logic circuits, constraint sets on Boolean variables, or other equivalent representations, and the costs of using these tools scale directly with the size of the input circuit. Hence, techniques for more efficient circuit constructions have benefits across these tools. We show efficient circuit constructions for various simple but commonly used data structures including stacks, queues, and associative maps. While current practice requires effectively copying the entire structure for each operation, our techniques take advantage of locality and batching to provide amortized costs that scale polylogarithmically in the size of the structure. We demonstrate how many common array usage patterns can be significantly improved with the help of these circuit structures. We report on experiments using our circuit structures for both generic secure computation using garbled circuits and automated test input generation using symbolic execution, and demonstrate order of magnitude improvements for both applications.},
author = {Zahur, S and Evans, D},
doi = {10.1109/SP.2013.40},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Arrays,Boolean algebra,Boolean logic circuits,Boolean variables,Indexes,Logic gates,Multiplexing,Protocols,Radiation detectors,Wires,array usage patterns,automated test input generation,batching,circuit structures,circuits,computation security,computer security,data privacy,data structures,garbled circuits,generic protocols,locality,logic circuits,optimization,privacy tools,secure computation,security tools,static circuits,symbolic execution},
pages = {493--507},
title = {{Circuit Structures for Improving Efficiency of Security and Privacy Tools}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547129},
year = {2013}
}
@article{Harris2013,
abstract = {New operating systems, such as the Capsicum capability system, allow a programmer to write an application that satisfies strong security properties by invoking security-specific system calls at a few key points in the program. However, rewriting an application to invoke such system calls correctly is an error-prone process: even the Capsicum developers have reported difficulties in rewriting programs to correctly invoke system calls. This paper describes capweave, a tool that takes as input (i) an LLVM program, and (ii) a declarative policy of the possibly-changing capabilities that a program must hold during its execution, and rewrites the program to use Capsicum system calls to enforce the policy. Our experiments demonstrate that capweave can be applied to rewrite security-critical UNIX utilities to satisfy practical security policies. capweave itself works quickly, and the runtime overhead incurred in the programs that capweave produces is generally low for practical workloads. View full abstract},
author = {Harris, William R and Jha, Somesh and Reps, Thomas and Anderson, Jonathan and Watson, Robert N M},
doi = {10.1109/SP.2013.11},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
keywords = {capabilities,safety games},
number = {1},
pages = {18--32},
title = {{Declarative, temporal, and practical programming with capabilities}},
year = {2013}
}
@article{Jana2013,
author = {Jana, Suman},
doi = {10.1109/SP.2013.31},
journal = {s{\&}p},
title = {{A Scanner Darkly : Protecting User Privacy From Perceptual Applications}},
year = {2013}
}
@article{Szekeres2013,
abstract = {Memory corruption bugs in software written in low-level languages like C or C++ are one of the oldest problems in computer security. The lack of safety in these languages allows attackers to alter the program’s behavior or take full control over it by hijacking its control flow. This problem has existed for more than 30 years and a vast number of potential solutions have been proposed, yet memory corruption attacks continue to pose a serious threat. Real world exploits show that all currently deployed protections can be defeated. This paper sheds light on the primary reasons for this by describing attacks that succeed on today’s systems. We systematize the current knowledge about various protection techniques by setting up a general model for memory corrup- tion attacks. Using this model we show what policies can stop which attacks. The model identifies weaknesses of currently deployed techniques, as well as other proposed protections enforcing stricter policies. We analyze the reasons why protection mechanisms imple- menting stricter polices are not deployed. To achieve wide adoption, protection mechanisms must support a multitude of features and must satisfy a host of requirements. Especially important is performance, as experience shows that only solutions whose overhead is in reasonable bounds get deployed. A comparison of different enforceable policies helps de- signers of new protection mechanisms in finding the balance between effectiveness (security) and efficiency.We identify some open research problems, and provide suggestions on improving the adoption of newer techniques.},
author = {Szekeres, L{\'{a}}szl{\'{o}} and Payer, Mathias and Wei, Tao and Song, Dawn},
doi = {10.1109/SP.2013.13},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
pages = {48--62},
title = {{SoK: Eternal war in memory}},
year = {2013}
}
@article{Lee2013,
abstract = {We present the design, security proof, and implementation of an anonymous subscription service. Users register for the service by providing some form of identity, which might or might not be linked to a real-world identity such as a credit card, a web login, or a public key. A user logs on to the system by presenting a credential derived from information received at registration. Each credential allows only a single login in any authentication window, or epoch. Logins are anonymous in the sense that the service cannot distinguish which user is logging in any better than random guessing. This implies unlinkability of a user across different logins. We find that a central tension in an anonymous subscription service is the service provider's desire for a long epoch (to reduce server-side computation) versus users' desire for a short epoch (so they can repeatedly "re-anonymize" their sessions). We balance this tension by having short epochs, but adding an efficient operation for clients who do not need unlinkability to cheaply re-authenticate themselves for the next time period. We measure performance of a research prototype of our protocol that allows an independent service to offer anonymous access to existing services. We implement a music service, an Android-based subway-pass application, and a web proxy, and show that adding anonymity adds minimal client latency and only requires 33 KB of server memory per active user.},
author = {Lee, Michael Z and Dunn, Alan M and Waters, Brent and Witchel, Emmett and Katz, Jonathan},
doi = {10.1109/SP.2013.29},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
keywords = {Anonymous Subscriptions},
pages = {319--333},
pmid = {24504081},
title = {{Anon-Pass: Practical anonymous subscriptions}},
year = {2013}
}
@article{Vanegue2013,
author = {Vanegue, Julien},
doi = {10.1109/SP.2013.12},
journal = {s{\&}p},
keywords = {-security audit,extended static checkers,program verification,static analysis},
title = {{Towards practical reactive security audit using extended static checkers}},
year = {2013}
}
@article{Mowery2013,
abstract = {We present three techniques for extracting entropy during boot on embedded devices. Our first technique times the execution of code blocks early in the Linux kernel boot process. It is simple to implement and has a negligible runtime overhead, but, on many of the devices we test, gathers hundreds of bits of entropy. Our second and third techniques, which run in the bootloader, use hardware features - DRAM decay behavior and PLL locking latency, respectively -- and are therefore less portable and less generally applicable, but their behavior is easier to explain based on physically unpredictable processes. We implement and measure the effectiveness of our techniques on ARM-, MIPS-, and AVR32-based systems-on-a-chip from a variety of vendors.},
author = {Mowery, K and Wei, M and Kohlbrenner, D and Shacham, H and Swanson, S},
doi = {10.1109/SP.2013.46},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {ARM based systems-on-a-chip,AVR32-based systems-on-a-chip,DRAM chips,DRAM decay behavior,Entropy,Instruments,Kernel,Linux,Linux kernel boot process,MIPS-based systems-on-a-chip,PLL locking latency,Random access memory,System-on-chip,Timing,boot-time entropy,bootloader,code block execution,dram,embedded devices,embedded systems,entropics,entropy,entropy extraction,microprocessor chips,operating system kernels,phase locked loops,phase-locked loops,pll,randomness,system-on-chip,timing},
pages = {589--603},
title = {{Welcome to the Entropics: Boot-Time Entropy in Embedded Devices}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547135},
year = {2013}
}
@article{Tippenhauer2013,
abstract = {Abstract—Wireless communication provides unique security challenges, but also enables novel ways to defend against attacks. In the past few years, a number of works discussed the use of friendly jamming to protect the confidentiality of the communicated data as well as to enable message authentication and access control. In this work, we analytically and experimentally evaluate the confidentiality that can be achieved by the use of friendly jamming, given an attacker with multiple receiving antennas. We construct a MIMO-based attack that allows the attacker to recover data protected by friendly jamming and refine the conditions for which this attack is most effective. Our attack shows that friendly jamming cannot provide strong confidentiality guarantees in all settings. We further test our attack in a setting where friendly jamming is used to protect the communication to medical implants.},
author = {Tippenhauer, Nils Ole and Malisa, Luka and Ranganathan, Aanjhan and Capkun, Srdjan},
doi = {10.1109/SP.2013.21},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
keywords = {Anti-Jamming,Friendly Jamming,IMD},
pages = {160--173},
title = {{On limitations of friendly jamming for confidentiality}},
year = {2013}
}
@article{Shen2013,
abstract = {This paper presents a novel mechanism, called Ally Friendly Jamming, which aims at providing an intelligent jamming capability that can disable unauthorized (enemy) wireless communication but at the same time still allow authorized wireless devices to communicate, even if all these devices operate at the same frequency. The basic idea is to jam the wireless channel continuously but properly control the jamming signals with secret keys, so that the jamming signals are unpredictable interference to unauthorized devices, but are recoverable by authorized ones equipped with the secret keys. To achieve the ally friendly jamming capability, we develop new techniques to generate ally jamming signals, to identify and synchronize with multiple ally jammers. This paper also reports the analysis, implementation, and experimental evaluation of ally friendly jamming on a software defined radio platform. Both the analytical and experimental results indicate that the proposed techniques can effectively disable enemy wireless communication and at the same time maintain wireless communication between authorized devices.},
author = {Shen, Wenbo and Ning, Peng and {Wenbo Shen} and {Peng Ning} and {Xiaofan He} and {Huaiyu Dai}},
doi = {10.1109/SP.2013.22},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {-wireless,Communication system security,Correlation,Jamming,Noise,Receivers,Synchronization,Wireless,Wireless communication,ally friendly intelligent jamming,friendly jamming,interference,interference cancella-,interference cancellation,jamming,jamming signal control,software defined radio platform,software radio,wireless channel,wireless channels,wireless communication device,wireless connectivity},
pages = {174--188},
title = {{Ally Friendly Jamming: How to Jam Your Enemy and Maintain Your Own Wireless Connectivity at the Same Time}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547109},
year = {2013}
}
@article{Kune2013,
abstract = {Electromagnetic interference (EMI) affects circuits by inducing voltages on conductors. Analog sensing of signals on the order of a few millivolts is particularly sensitive to interference. This work (1) measures the susceptibility of analog sensor systems to signal injection attacks by intentional, low-power emission of chosen electromagnetic waveforms, and (2) proposes defense mechanisms to reduce the risks. Our experiments use specially crafted EMI at varying power and distance to measure susceptibility of sensors in implantable medical devices and consumer electronics. Results show that at distances of 1-2m, consumer electronic devices containing microphones are vulnerable to the injection of bogus audio signals. Our measurements show that in free air, intentional EMI under 10 W can inhibit pacing and induce defibrillation shocks at distances up to 1-2m on implantable cardiac electronic devices. However, with the sensing leads and medical devices immersed in a saline bath to better approximate the human body, the same experiment decreases to about 5 cm. Our defenses range from prevention with simple analog shielding to detection with a signal contamination metric based on the root mean square of waveform amplitudes. Our contribution to securing cardiac devices includes a novel defense mechanism that probes for forged pacing pulses inconsistent with the refractory period of cardiac tissue.},
author = {Kune, Denis Foo and Backes, John and Clark, Shane S and Kramer, Daniel and Reynolds, Matthew and Fu, Kevin and Kim, Yongdae and Xu, Wenyuan},
doi = {10.1109/SP.2013.20},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
keywords = {Attacks and defenses,analog sensors,embedded systems security,hardware security},
pages = {145--159},
title = {{Ghost talk: Mitigating EMI signal injection attacks against analog sensors}},
year = {2013}
}
@article{Brubaker2013,
author = {Brubaker, Chad},
doi = {10.1109/SP.2013.14},
journal = {s{\&}p},
keywords = {-censorship circumvention,41,for example,hides tor traffic by,mimicking skype video,morph,nications,skype-,systems,them as parrot circumvention,tor pluggable transports,unobservable commu-},
title = {{The Parrot is Dead : Observing Unobservable Network Communications}},
year = {2013}
}
@article{,
journal = {s{\&}p},
title = {{ Caveat Coercitor: Coercion-Evidence in Electronic Voting }},
year = {2013}
}
@article{Nikiforakis2013,
abstract = {The web has become an essential part of our society and is currently the main medium of information delivery. Billions of users browse the web on a daily basis, and there are single websites that have reached over one billion user accounts. In this environment, the ability to track users and their online habits can be very lucrative for advertising companies, yet very intrusive for the privacy of users. In this paper, we examine how web-based device fingerprinting currently works on the Internet. By analyzing the code of three popular browser-fingerprinting code providers, we reveal the techniques that allow websites to track users without the need of client-side identifiers. Among these techniques, we show how current commercial fingerprinting approaches use questionable practices, such as the circumvention of HTTP proxies to discover a user's real IP address and the installation of intrusive browser plugins. At the same time, we show how fragile the browser ecosystem is against fingerprinting through the use of novel browser-identifying techniques. With so many different vendors involved in browser development, we demonstrate how one can use diversions in the browsers' implementation to distinguish successfully not only the browser-family, but also specific major and minor versions. Browser extensions that help users spoof the user-agent of their browsers are also evaluated. We show that current commercial approaches can bypass the extensions, and, in addition, take advantage of their shortcomings by using them as additional fingerprinting features.},
author = {Nikiforakis, Nick and Kapravelos, Alexandros and Joosen, Wouter and Kruegel, Christopher and Piessens, Frank and Vigna, Giovanni},
doi = {10.1109/SP.2013.43},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
keywords = {browser extensions,device identification,fingerprinting},
pages = {541--555},
title = {{Cookieless Monster: Exploring the Ecosystem of Web-based Device Fingerprinting}},
year = {2013}
}
@article{Bhargavan2013,
abstract = {TLS is possibly the most used protocol for secure communications, with a 18-year history of flaws and fixes, ranging from its protocol logic to its cryptographic design, and from the Internet standard to its diverse implementations. We develop a verified reference implementation of TLS 1.2. Our code fully supports its wire formats, ciphersuites, sessions and connections, re-handshakes and resumptions, alerts and errors, and data fragmentation, as prescribed in the RFCs; it interoperates with mainstream web browsers and servers. At the same time, our code is carefully structured to enable its modular, automated verification, from its main API down to computational assumptions on its cryptographic algorithms. Our implementation is written in F{\{}{\#}{\}} and specified in F7. We present security specifications for its main components, such as authenticated stream encryption for the record layer and key establishment for the handshake. We describe their verification using the F7 typechecker. To this end, we equip each cryptographic primitive and construction of TLS with a new typed interface that captures its security properties, and we gradually replace concrete implementations with ideal functionalities. We finally typecheck the protocol state machine, and obtain precise security theorems for TLS, as it is implemented and deployed. We also revisit classic attacks and report a few new ones.},
author = {Bhargavan, K and Fournet, C and Kohlweiss, M and Pironti, A and Strub, P},
doi = {10.1109/SP.2013.37},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {API,Encryption,F7 typechecker,Formal Verification,F{\{}{\#}{\}},Internet,Internet standard,Libraries,Protocols,Provable Security,RFC,Security Protocol Implementation,Servers,Standards,TLS 1.2,Transport Layer Security,Web servers,alerts,application program interfaces,authenticated stream encryption,ciphersuites,classic attacks,computer network security,connections,cryptographic design,cryptographic protocols,data fragmentation,errors,file servers,formal specification,key establishment,mainstream Web browser,online front-ends,protocol logic,protocol state machine,re-handshakes,record layer,resumption,security specifications,sessions,time 18 year,transport layer security,typed interface,verified cryptographic security,wire formats},
pages = {445--459},
title = {{Implementing TLS with Verified Cryptographic Security}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547126},
year = {2013}
}
@article{Xu2013,
abstract = {This paper introduces a novel information hiding technique for Flash memory. The method hides data within an analog characteristic of Flash, the program time of individual bits. Because the technique uses analog behaviors, normal Flash memory operations are not affected and hidden information is invisible in the data stored in the memory. Even if an attacker checks a Flash chip's analog characteristics, experimental results indicate that the hidden information is difficult to distinguish from inherent manufacturing variation or normal wear on the device. Moreover, the hidden data can survive erasure of the Flash memory data, and the technique can be used on current Flash chips without hardware changes.},
author = {Xu, S Q and Kan, E and Suh, G E},
doi = {10.1109/SP.2013.26},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Error correction codes,Flash memories,Logic gates,Payloads,Standards,Stress,Transistors,analog characteristics,data encapsulation,flash chips,flash memories,flash memory,flash memory data,information hiding technique,manufacturing variation,security,steganography},
pages = {271--285},
title = {{Hiding Information in Flash Memory}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547115},
year = {2013}
}
@article{Hund2013,
abstract = {Due to the prevalence of control-flow hijacking attacks, a wide variety of defense methods to protect both user space and kernel space code have been developed in the past years. A few examples that have received widespread adoption include stack canaries, non-executable memory, and Address Space Layout Randomization (ASLR). When implemented correctly (i.e., a given system fully supports these protection methods and no information leak exists), the attack surface is significantly reduced and typical exploitation strategies are severely thwarted. All modern desktop and server operating systems support these techniques and ASLR has also been added to different mobile operating systems recently. In this paper, we study the limitations of kernel space ASLR against a local attacker with restricted privileges. We show that an adversary can implement a generic side channel attack against the memory management system to deduce information about the privileged address space layout. Our approach is based on the intrinsic property that the different caches are shared resources on computer systems. We introduce three implementations of our methodology and show that our attacks are feasible on four different x86-based CPUs (both 32- and 64-bit architectures) and also applicable to virtual machines. As a result, we can successfully circumvent kernel space ASLR on current operating systems. Furthermore, we also discuss mitigation strategies against our attacks, and propose and implement a defense solution with negligible performance overhead.},
author = {Hund, Ralf and Willems, Carsten and Holz, Thorsten},
doi = {10.1109/SP.2013.23},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
keywords = {Address Space Layout Randomization,Exploit Mitigation,Kernel Vulnerabilities,Timing Attacks},
pages = {191--205},
title = {{Practical timing side channel attacks against kernel space ASLR}},
year = {2013}
}
@article{Vu2013,
abstract = {We consider interactive, proof-based verifiable computation: how can a client machine specify a computation to a server, receive an answer, and then engage the server in an interactive protocol that convinces the client that the answer is correct, with less work for the client than executing the computation in the first place? Complexity theory and cryptography offer solutions in principle, but if implemented naively, they are ludicrously expensive. Recently, however, several strands of work have refined this theory and implemented the resulting protocols in actual systems. This work is promising but suffers from one of two problems: either it relies on expensive cryptography, or else it applies to a restricted class of computations. Worse, it is not always clear which protocol will perform better for a given problem.We describe a system that (a) extends optimized refinements of the non-cryptographic protocols to a much broader class of computations, (b) uses static analysis to fail over to the cryptographic ones when the non-cryptographic ones would be more expensive, and (c) incorporates this core into a built system that includes a compiler for a high-level language, a distributed server, and GPU acceleration. Experimental results indicate that our system performs better and applies more widely than the best in the literature.},
author = {Vu, V and Setty, S and Blumberg, A J and Walfish, M},
doi = {10.1109/SP.2013.48},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Computational modeling,Context,Cryptography,GPU acceleration,Logic gates,Mathematical model,Protocols,Servers,built system,client machine,complexity theory,cryptography,distributed server,high-level language,hybrid architecture,interactive protocol,interactive systems,interactive verifiable computation,noncryptographic protocols,proof-based verifiable computation,protocols,static analysis},
pages = {223--237},
title = {{A Hybrid Architecture for Interactive Verifiable Computation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547112},
year = {2013}
}
@article{Gligor2013,
abstract = {We present the Crossfire attack -- a powerful attack that degrades and often cuts off network connections to a variety of selected server targets (e.g., servers of an enterprise, a city, a state, or a small country) by flooding only a few network links. In Crossfire, a small set of bots directs low intensity flows to a large number of publicly accessible servers. The concentration of these flows on the small set of carefully chosen links floods these links and effectively disconnects selected target servers from the Internet. The sources of the Crossfire attack are undetectable by any targeted servers, since they no longer receive any messages, and by network routers, since they receive only low-intensity, individual flows that are indistinguishable from legitimate flows. The attack persistence can be extended virtually indefinitely by changing the set of bots, publicly accessible servers, and target links while maintaining the same disconnection targets. We demonstrate the attack feasibility using Internet experiments, show its effects on a variety of chosen targets (e.g., servers of universities, US states, East and West Coasts of the US), and explore several countermeasures.},
author = {Gligor, V D},
doi = {10.1109/SP.2013.19},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Bandwidth,Educational institutions,IP networks,Internet,Measurement,Protocols,Servers,bot connection,computer network security,crossfire attack persistence,disconnection target links,legitimate flows,network connections,network link floods,network routers,publicly accessible servers,target servers,telecommunication links,telecommunication network routing},
pages = {127--141},
title = {{The Crossfire Attack}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547106},
year = {2013}
}
@article{Alrwais2013,
abstract = {Malicious Web activities continue to be a major threat to the safety of online Web users. Despite the plethora forms of attacks and the diversity of their delivery channels, in the back end, they are all orchestrated through malicious Web infrastructures, which enable miscreants to do business with each other and utilize others' resources. Identifying the linchpins of the dark infrastructures and distinguishing those valuable to the adversaries from those disposable are critical for gaining an upper hand in the battle against them. In this paper, using nearly 4 million malicious URL paths crawled from different attack channels, we perform a large-scale study on the topological relations among hosts in the malicious Web infrastructure. Our study reveals the existence of a set of topologically dedicated malicious hosts that play orchestrating roles in malicious activities. They are well connected to other malicious hosts and do not receive traffic from legitimate sites. Motivated by their distinctive features in topology, we develop a graph-based approach that relies on a small set of known malicious hosts as seeds to detect dedicate malicious hosts in a large scale. Our method is general across the use of different types of seed data, and results in an expansion rate of over 12 times in detection with a low false detection rate of 2{\{}{\%}{\}}. Many of the detected hosts operate as redirectors, in particular Traffic Distribution Systems (TDSes) that are long-lived and receive traffic from new attack campaigns over time. These TDSes play critical roles in managing malicious traffic flows. Detecting and taking down these dedicated malicious hosts can therefore have more impact on the malicious Web infrastructures than aiming at short-lived doorways or exploit sites.},
author = {Alrwais, S},
doi = {10.1109/SP.2013.18},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Crawlers,Feeds,Internet,Labeling,Servers,TDS,Topology,Twitter,Uniform resource locators,attack channels,dark Web,graph-based approach,linchpins,low false detection rate,malicious URL paths,malicious Web infrastructures,malicious hosts,security of data,seed data,short-lived doorways,telecommunication traffic,topologically dedicated hosts,traffic distribution systems},
pages = {112--126},
title = {{Finding the Linchpins of the Dark Web: a Study on Topologically Dedicated Hosts on Malicious Web Infrastructures}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547105},
year = {2013}
}
@article{Stefanov2013,
abstract = {We design and build ObliviStore, a high performance, distributed ORAM-based cloud data store secure in the malicious model. To the best of our knowledge, ObliviStore is the fastest ORAM implementation known to date, and is faster by 10X or more in comparison with the best known ORAM implementation. ObliviStore achieves high throughput by making I/O operations asynchronous. Asynchrony introduces security challenges, i.e., we must prevent information leakage not only through access patterns, but also through timing of I/O events. We propose various practical optimizations which are key to achieving high performance, as well as techniques for a data center to dynamically scale up a distributed ORAM. We show that with 11 trusted machines (each with a modern CPU), and 20 Solid State Drives, ObliviStore achieves a throughput of 31.5MB/s with a block size of 4KB.},
author = {Stefanov, Emil and Shi, Elaine},
doi = {10.1109/SP.2013.25},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
keywords = {file system,oblivious ram,oblivious storage,oblivistore,oram},
pages = {253--267},
title = {{ObliviStore: High performance oblivious cloud storage}},
year = {2013}
}
@article{Rossow2013,
author = {Rossow, Christian and Andriesse, Dennis and Werner, Tillmann and Fkie, Fraunhofer},
doi = {10.1109/SP.2013.17},
journal = {s{\&}p},
title = {{SoK : P2PWNED — Modeling and Evaluating the Resilience of Peer-to-Peer Botnets}},
year = {2013}
}
@article{Zhang2013,
abstract = {Control Flow Integrity (CFI) provides a strong protection against modern control-flow hijacking attacks. How- ever, performance and compatibility issues limit its adoption. We propose a new practical and realistic protection method called CCFIR (Compact Control Flow Integrity and Random- ization), which addresses the main barriers to CFI adoption. CCFIR collects all legal targets of indirect control-transfer in- structions, puts them into a dedicated “Springboard section” in a random order, and then limits indirect transfers to flow only to them. Using the Springboard section for targets, CCFIR can validate a target more simply and faster than traditional CFI, and provide support for on-site target-randomization as well as better compatibility. Based on these approaches, CCFIR can stop control-flow hijacking attacks including ROP and return- into-libc. Results show that ROP gadgets are all eliminated.We observe that with the wide deployment of ASLR, Windows/x86 PE executables contain enough information in relocation tables which CCFIR can use to find all legal instructions and jump targets reliably, without source code or symbol information. We evaluate our prototype implementation on common web browsers and the SPEC CPU2000 suite: CCFIR protects large applications such as GCC and Firefox completely automati- cally, and has low performance overhead of about 3.6{\{}{\%}{\}}/8.6{\{}{\%}{\}} (average/max) using SPECint2000. Experiments on real-world exploits also show that CCFIR-hardened versions of IE6, Firefox 3.6 and other applications are protected effectively.},
author = {Zhang, Chao and Wei, Tao and Chen, Zhaofeng and Duan, Lei and Szekeres, L{\'{a}}szl{\'{o}} and McCamant, Stephen and Song, Dawn and Zou, Wei},
doi = {10.1109/SP.2013.44},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
pages = {559--573},
title = {{Practical control flow integrity and randomization for binary executables}},
year = {2013}
}
@article{Bellare2013,
abstract = {We advocate schemes based on fixed-key AES as the best route to highly efficient circuit-garbling. We provide such schemes making only one AES call per garbled-gate evaluation. On the theoretical side, we justify the security of these methods in the random-permutation model, where parties have access to a public random permutation. On the practical side, we provide the Just Garble system, which implements our schemes. Just Garble evaluates moderate-sized garbled-circuits at an amortized cost of 23.2 cycles per gate (7.25 nsec), far faster than any prior reported results.},
author = {Bellare, M and Keelveedhi, S and Rogaway, P},
doi = {10.1109/SP.2013.39},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Cryptography,Games,Garbled circuits,Just Garble system,Logic gates,Protocols,Semantics,Wires,Yao's protocol,circuit-garbling,cryptography,fixed-key AES,fixed-key blockcipher,garbled-gate evaluation,garbling schemes,moderate-sized garbled-circuits,multiparty computation,public random permutation,random-permutation model,timing study},
pages = {478--492},
title = {{Efficient Garbling from a Fixed-Key Blockcipher}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547128},
year = {2013}
}
@article{Clark2013,
abstract = {Internet users today depend daily on HTTPS for secure communication with sites they intend to visit. Over the years, many attacks on HTTPS and the certificate trust model it uses have been hypothesized, executed, and/or evolved. Meanwhile the number of browser-trusted (and thus, de facto, user-trusted) certificate authorities has proliferated, while the due diligence in baseline certificate issuance has declined. We survey and categorize prominent security issues with HTTPS and provide a systematic treatment of the history and on-going challenges, intending to provide context for future directions. We also provide a comparative evaluation of current proposals for enhancing the certificate infrastructure used in practice.},
author = {Clark, Jeremy and {Van Oorschot}, Paul C},
doi = {10.1109/SP.2013.41},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
keywords = {SSL,browser trust model,certificates,usability},
pages = {511--525},
title = {{SoK: SSL and HTTPS: Revisiting past challenges and evaluating certificate trust model enhancements}},
year = {2013}
}
@article{Reardon2013,
abstract = {Secure data deletion is the task of deleting data irrecoverably from a physical medium. In the digital world, data is not securely deleted by default; instead, many approaches add secure deletion to existing physical medium interfaces. Interfaces to the physical medium exist at different layers, such as user-level applications, the file system, the device driver, etc. Depending on which interface is used, the properties of an approach can differ significantly. In this paper, we survey the related work in detail and organize existing approaches in terms of their interfaces to physical media. We further present a taxonomy of adversaries differing in their capabilities as well as a systematization for the characteristics of secure deletion approaches. Characteristics include environmental assumptions, such as how the interface’s use affects the physical medium, as well as behavioural prop- erties of the approach such as the deletion latency and physical wear.We perform experiments to test a selection of approaches on a variety of file systems and analyze the assumptions made in practice.},
author = {Reardon, Joel and Basin, David and Capkun, Srdjan},
doi = {10.1109/SP.2013.28},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
keywords = {File systems,Flash memory,Magnetic memory,Secure deletion},
pages = {301--315},
title = {{SoK: Secure data deletion}},
year = {2013}
}
@article{Onarlioglu2013,
abstract = {Privacy has become an issue of paramount importance for many users. As a result, encryption tools such as True Crypt, OS-based full-disk encryption such as File Vault, and privacy modes in all modern browsers have become popular. However, although such tools are useful, they are not perfect. For example, prior work has shown that browsers still leave many traces of user information on disk even if they are started in private browsing mode. In addition, disk encryption alone is not sufficient, as key disclosure through coercion remains possible. Clearly, it would be useful and highly desirable to have OS-level support that provides strong privacy guarantees for any application -- not only browsers. In this paper, we present the design and implementation of PrivExec, the first operating system service for private execution. PrivExec provides strong, general guarantees of private execution, allowing any application to execute in a mode where storage writes, either to the filesystem or to swap, will not be recoverable by others during or after execution. PrivExec does not require explicit application support, recompilation, or any other preconditions. We have implemented a prototype of PrivExec by extending the Linux kernel that is performant, practical, and that secures sensitive data against disclosure.},
author = {Onarlioglu, K and Mulliner, C and Robertson, W and Kirda, E},
doi = {10.1109/SP.2013.24},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Browsers,Containers,Encryption,FileVault,Kernel,Linux,Linux kernel,OS-based full-disk encryption,PrivExec framework,Privacy,TrueCrypt,cryptography,data privacy,disk encryption,encryption tools,file organisation,filesystem,online front-ends,operating system service,operating systems,privacy,privacy modes,private browsing mode,private execution,sensitive data security,user information},
pages = {206--220},
title = {{PrivExec: Private Execution as an Operating System Service}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547111},
year = {2013}
}
@article{Alvisi2013,
abstract = {Sybil attacks in which an adversary forges a potentially unbounded number of identities are a danger to distributed systems and online social networks. The goal of sybil defense is to accurately identify sybil identities. This paper surveys the evolution of sybil defense protocols that leverage the structural properties of the social graph underlying a distributed system to identify sybil identities. We make two main contributions. First, we clarify the deep connection between sybil defense and the theory of random walks. This leads us to identify a community detection algorithm that, for the first time, offers provable guarantees in the context of sybil defense. Second, we advocate a new goal for sybil defense that addresses the more limited, but practically useful, goal of securely white-listing a local region of the graph.},
author = {Alvisi, L and Clement, A and Epasto, A and Lattanzi, S and Panconesi, A},
doi = {10.1109/SP.2013.33},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Communities,Detection algorithms,Facebook,Image edge detection,Protocols,Robustness,SoK framework,community detection algorithm,graph white-listing,random walks theory,security of data,social network,social networking (online),sybil defense protocol,sybil identity},
pages = {382--396},
title = {{SoK: The Evolution of Sybil Defense via Social Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547122},
year = {2013}
}
@article{Vasudevan2013,
abstract = {We present the design, implementation, and verification of XMHF- an eXtensible and Modular Hypervisor Framework. XMHF is designed to achieve three goals -- modular extensibility, automated verification, and high performance. XMHF includes a core that provides functionality common to many hypervisor-based security architectures and supports extensions that augment the core with additional security or functional properties while preserving the fundamental hypervisor security property of memory integrity (i.e., ensuring that the hypervisor's memory is not modified by software running at a lower privilege level). We verify the memory integrity of the XMHF core -- 6018 lines of code -- using a combination of automated and manual techniques. The model checker CBMC automatically verifies 5208 lines of C code in about 80 seconds using less than 2GB of RAM. We manually audit the remaining 422 lines of C code and 388 lines of assembly language code that are stable and unlikely to change as development proceeds. Our experiments indicate that XMHF's performance is comparable to popular high-performance general-purpose hypervisors for the single guest that it supports. View full abstract},
author = {Vasudevan, Amit and Chaki, Sagar and Jia, Limin and McCune, Jonathan and Newsome, James and Datta, Anupam},
doi = {10.1109/SP.2013.36},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
keywords = {Hypervisor Applications ("Hypapps"),Hypervisor Framework,Memory Integrity,Verification},
pages = {430--444},
title = {{Design, implementation and verification of an extensible and modular hypervisor framework}},
year = {2013}
}
@article{Popa2013,
author = {Popa, Raluca Ada and Li, Frank H and Zeldovich, Nickolai},
doi = {10.1109/SP.2013.38},
journal = {s{\&}p},
keywords = {-order-preserving encryption,encoding,order-preserving scheme guarantees},
title = {{An Ideal-Security Protocol for Order-Preserving Encoding}},
year = {2013}
}
@article{Murray2013,
abstract = {In contrast to testing, mathematical reasoning and formal verification can show the absence of whole classes of security vulnerabilities. We present the, to our knowledge, first complete, formal, machine-checked verification of information flow security for the implementation of a general-purpose microkernel; namely seL4. Unlike previous proofs of information flow security for operating system kernels, ours applies to the actual 8, 830 lines of C code that implement seL4, and so rules out the possibility of invalidation by implementation errors in this code. We assume correctness of compiler, assembly code, hardware, and boot code. We prove everything else. This proof is strong evidence of seL4's utility as a separation kernel, and describes precisely how the general purpose kernel should be configured to enforce isolation and mandatory information flow control. We describe the information flow security statement we proved (a variant of intransitive noninterference), including the assumptions on which it rests, as well as the modifications that had to be made to seL4 to ensure it was enforced. We discuss the practical limitations and implications of this result, including covert channels not covered by the formal proof.},
author = {Murray, T and Matichuk, D and Brassil, M and Gammie, P and Bourke, T and Seefried, S and Lewis, C and Klein, G},
doi = {10.1109/SP.2013.35},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Abstracts,Access control,Hardware,Kernel,Message systems,Silicon,assembly code,boot code,compiler,formal verification,hardware,inference mechanisms,information flow control,information flow enforcement,information flow security,machine-checked verification,mathematical reasoning,operating system kernels,program compilers,seL4 general-purpose microkernel,security of data,security vulnerability,separation kernel},
pages = {415--429},
title = {{seL4: From General Purpose to a Proof of Information Flow Enforcement}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547124},
year = {2013}
}
@article{AlFardan2013,
abstract = {The Transport Layer Security (TLS) protocol aims to provide confidentiality and integrity of data in transit across untrusted networks. TLS has become the de facto secure protocol of choice for Internet and mobile applications. DTLS is a variant of TLS that is growing in importance. In this paper, we present distinguishing and plaintext recovery attacks against TLS and DTLS. The attacks are based on a delicate timing analysis of decryption processing in the two protocols. We include experimental results demonstrating the feasibility of the attacks in realistic network environments for several differ- ent implementations of TLS and DTLS, including the leading OpenSSL implementations. We provide countermeasures for the attacks. Finally, we discuss the wider implications of our attacks for the cryptographic design used by TLS and DTLS},
author = {AlFardan, Nadhem J and Paterson, Kenneth G},
doi = {10.1109/SP.2013.42},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
keywords = {CBC-mode encryption,DTLS,TLS,plaintext recovery,timing attack},
pages = {526--540},
title = {{Lucky thirteen: Breaking the TLS and DTLS record protocols}},
year = {2013}
}
@article{Nikolaenko2013,
abstract = {Ridge regression is an algorithm that takes as input a large number of data points and finds the best-fit linear curve through these points. The algorithm is a building block for many machine-learning operations. We present a system for privacy-preserving ridge regression. The system outputs the best-fit curve in the clear, but exposes no other information about the input data. Our approach combines both homomorphic encryption and Yao garbled circuits, where each is used in a different part of the algorithm to obtain the best performance. We implement the complete system and experiment with it on real data-sets, and show that it significantly outperforms pure implementations based only on homomorphic encryption or Yao circuits.},
author = {Nikolaenko, Valeria and Weinsberg, Udi and Ioannidis, Stratis and Joye, Marc and Boneh, Dan and Taft, Nina},
doi = {10.1109/SP.2013.30},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
pages = {334--348},
title = {{Privacy-preserving ridge regression on hundreds of millions of records}},
year = {2013}
}
@article{Hritcu2013,
abstract = {Existing designs for fine-grained, dynamic information-flow control assume that it is acceptable to terminate the entire system when an incorrect flow is detected-i.e, they give up availability for the sake of confidentiality and integrity. This is an unrealistic limitation for systems such as long-running servers. We identify public labels and delayed exceptions as crucial ingredients for making information-flow errors recoverable in a sound and usable language, and we propose two new error-handling mechanisms that make all errors recoverable. The first mechanism builds directly on these basic ingredients, using not-a-values (NaVs) and data flow to propagate errors. The second mechanism adapts the standard exception model to satisfy the extra constraints arising from information flow control, converting thrown exceptions to delayed ones at certain points. We prove that both mechanisms enjoy the fundamental soundness property of non-interference. Finally, we describe a prototype implementation of a full-scale language with NaVs and report on our experience building robust software components in this setting.},
author = {Hritcu, C and Greenberg, M and Karel, B and Pierce, B C and Morrisett, G},
doi = {10.1109/SP.2013.10},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Availability,Calculus,Context,Data structures,IFCException,NaV,NaVs,Security,Servers,Standards,availability,data flow,data flow analysis,delayed exceptions,dynamic information flow control,error handling,error recovery,error-handling mechanisms,exception handling,fine-grained dynamic information flow control,fine-grained labeling,full-scale language,fundamental soundness property,information flow error recovery,noninterference property,not-a-values,programming-language design,public labels,reliability,robust software components,software reliability,standard exception model,system recovery},
pages = {3--17},
title = {{All Your IFCException Are Belong to Us}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547098},
year = {2013}
}
@article{Snow2013,
abstract = {Fine-grained address space layout randomization (ASLR) has recently been proposed as a method of efficiently mitigating runtime attacks. In this paper, we introduce the design and implementation of a framework based on a novel attack strategy, dubbed just-in-time code reuse, that undermines the benefits of fine-grained ASLR. Specifically, we derail the assumptions embodied in fine-grained ASLR by exploiting the ability to repeatedly abuse a memory disclosure to map an application's memory layout on-the-fly, dynamically discover API functions and gadgets, and JIT-compile a target program using those gadgets -- all within a script environment at the time an exploit is launched. We demonstrate the power of our framework by using it in conjunction with a real-world exploit against Internet Explorer, and also provide extensive evaluations that demonstrate the practicality of just-in-time code reuse attacks. Our findings suggest that fine-grained ASLR may not be as promising as first thought.},
author = {Snow, K Z and Monrose, F and Davi, L and Dmitrienko, A and Liebchen, C and Sadeghi, A},
doi = {10.1109/SP.2013.45},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {API functions,API gadgets,ASLR,Internet Explorer,JIT-compile,Layout,Libraries,Payloads,Programming,Registers,Runtime,Security,application program interfaces,attack strategy,fine-grained address space layout randomization,just-in-time code reuse attacks,memory disclosure,program compilers,runtime attacks,script environment,search engines,security of data},
pages = {574--588},
title = {{Just-In-Time Code Reuse: On the Effectiveness of Fine-Grained Address Space Layout Randomization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547134},
year = {2013}
}
@article{Biryukov2013,
abstract = {Tor is the most popular volunteer-based anonymity network consisting of over 3000 volunteer-operated relays. Apart from making connections to servers hard to trace to their origin it can also provide receiver privacy for Internet services through a feature called “hidden services”. In this paper we expose flaws both in the design and implementation of Tor’s hidden services that allow an attacker to measure the popularity of arbitrary hidden services, take down hidden services and deanonymize hidden services. We give a practical evaluation of our techniques by studying: (1) a recent case of a botnet using Tor hidden services for command and control channels; (2) Silk Road, a hidden service used to sell drugs and other contraband; (3) the hidden service of the DuckDuckGo search engine.},
author = {Biryukov, Alex and Pustogarov, Ivan and Weinmann, Ralf Philipp},
doi = {10.1109/SP.2013.15},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
keywords = {Tor,anonymity network,hidden services,privacy},
pages = {80--94},
title = {{Trawling for tor hidden services: Detection, measurement, deanonymization}},
year = {2013}
}
@article{Parno2013,
author = {Parno, Bryan and Gentry, Craig and Raykova, Mariana},
doi = {10.1109/SP.2013.47},
journal = {s{\&}p},
pages = {238--252},
title = {{Pinocchio : Nearly Practical Verifiable Computation}},
year = {2013}
}
@article{Ruhrmair2013,
abstract = {In recent years, PUF-based schemes have not only been suggested for the basic security tasks of tamper sensitive key storage or system identification, but also for more complex cryptographic protocols like oblivious transfer (OT), bit commitment (BC), or key exchange (KE). In these works, so-called "Strong PUFs" are regarded as a new, fundamental cryptographic primitive of their own, comparable to the bounded storage model, quantum cryptography, or noisebased cryptography. This paper continues this line of research, investigating the correct adversarial attack model and the actual security of such protocols. In its first part, we define and compare different attack models. They reach from a clean, first setting termed the "stand-alone, good PUF model" to stronger scenarios like the "bad PUF model" and the "PUF re-use model". We argue why these attack models are realistic, and that existing protocols would be faced with them if used in practice. In the second part, we execute exemplary security analyses of existing schemes in the new attack models. The evaluated protocols include recent schemes from Brzuska et al. published at Crypto 2011 [1] and from Ostrovsky et al. [18]. While a number of protocols are certainly secure in their own, original attack models, the security of none of the considered protocols for OT, BC, or KE is maintained in all of the new, realistic scenarios. One consequence of our work is that the design of advanced cryptographic PUF protocols needs to be strongly reconsidered. Furthermore, it suggests that Strong PUFs require additional hardware properties in order to be broadly usable in such protocols: Firstly, they should ideally be "erasable", meaning that single PUF-responses can be erased without affecting other responses. If the area efficient implementation of this feature turns out to be difficult, new forms of Controlled PUFs [8] (such as Logically Erasable and Logically Reconfigurable PUFs [13]) may suffice in certain applications. Se- ondly, PUFs should be "certifiable", meaning that one can verify that the PUF has been produced faithfully and has not been manipulated in any way afterwards. The combined implementation of these features represents a pressing and challenging problem, which we pose to the PUF hardware community in this work.},
author = {Ruhrmair, U and van Dijk, M},
doi = {10.1109/SP.2013.27},
isbn = {978-0-7695-4977-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {(Strong) PUFs,(Strong) Physical Unclonable Functions,Adaptation models,Attack Models,BC protocol,Biological system modeling,Bit Commitment,Certifiable PUFs,Computational modeling,Cryptography,Erasable PUFs,Hardware,KE protocol,Key Exchange,OT protocol,Oblivious Transfer,PUF reuse model,PUF-based scheme,Protocols,adversarial attack model,bad PUF model,bit commitment protocol,bounded storage model,controlled PUF,cryptographic primitive,cryptographic protocol,cryptographic protocols,good PUF model,key exchange protocol,noise-based cryptography,oblivious transfer protocol,physical unclonable function,quantum cryptography,security evaluation,security protocol},
pages = {286--300},
title = {{PUFs in Security Protocols: Attack Models and Security Evaluations}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547116},
year = {2013}
}
@article{Miers2013,
abstract = {Bitcoin is the first e-cash system to see widespread adoption. While Bitcoin offers the potential for new types of financial interaction, it has significant limitations regarding privacy. Specifically, because the Bitcoin transaction log is completely public, users’ privacy is protected only through the use of pseudonyms. In this paper we propose Zerocoin, a crypto- graphic extension to Bitcoin that augments the protocol to allow for fully anonymous currency transactions. Our system uses standard cryptographic assumptions and does not introduce new trusted parties or otherwise change the security model of Bitcoin. We detail Zerocoin’s cryptographic construction, its integration into Bitcoin, and examine its performance both in terms of computation and impact on the Bitcoin protocol.},
author = {Miers, Ian and Garman, Christina and Green, Matthew and Rubin, Aviel D},
doi = {10.1109/SP.2013.34},
isbn = {9780769549774},
issn = {10816011},
journal = {s{\&}p},
keywords = {cryptography,data privacy,electronic money},
pages = {397--411},
title = {{Zerocoin: Anonymous distributed e-cash from bitcoin}},
url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\{}{\&}{\}}arnumber=6547123},
year = {2013}
}
@article{Roesner2012,
abstract = {Modern client platforms, such as iOS, Android, Windows Phone, Windows 8, and web browsers, run each application in an isolated environment with limited privileges. A pressing open problem in such systems is how to allow users to grant applications access to user-owned resources, e.g., to privacy- and cost-sensitive devices like the camera or to user data residing in other applications. A key challenge is to enable such access in a way that is non-disruptive to users while still maintaining least-privilege restrictions on applications. In this paper, we take the approach of user-driven access control, whereby permission granting is built into existing user actions in the context of an application, rather than added as an afterthought via manifests or system prompts. To allow the system to precisely capture permission-granting intent in an application’s context, we introduce access control gadgets (ACGs). Each user-owned resource exposes ACGs for applications to embed. The user’s authentic UI interactions with an ACG grant the application permission to access the corresponding resource. Our prototyping and evaluation experience indicates that user- driven access control enables in-context, non-disruptive, and least-privilege permission granting on modern client platforms.},
author = {Roesner, Franziska and Kohno, Tadayoshi and Moshchuk, Alexander and Parno, Bryan and Wang, Helen J and Cowan, Crispin},
doi = {10.1109/SP.2012.24},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {ACGs,access control,access control gadgets,least-privilege,operating systems,permission granting,permissions,user intent,user-driven access control,user-owned resources},
number = {Oakland},
pages = {224--238},
title = {{User-driven access control: Rethinking permission granting in modern operating systems}},
year = {2012}
}
@article{Kovah2012,
abstract = {In this paper, we present a comprehensive timingbased attestation system suitable for typical enterprise use and evidence of that systems performance. This system, similar to Pioneer [19] but built with relaxed assumptions suitable for an enterprise setting, successfully detects attacks on code integrity over 6 hops of an enterprise network, even with an average of 1.7{\{}{\%}{\}} time overhead for the attacker. We also present the first implementation and evaluation of a Trusted Platform Module (TPM) hardware timing-based attestation protocol. We describe the set-up and results of a set of experiments showing the effectiveness of our timing-based system; the data address previous work questioning the efficacy of timing-based attestation in practical settings. While it is our firm belief that system measurement itself is an worthwhile goal, and timing-based attestation systems can provide equally-trustworthy measurements a hardware-based attestation systems, we feel that Time Of Check, Time Of Use (TOCTOU) attacks have not gotten appropriate attention in the literature. To address this topic, we present the three conditions required to execute such an attack, and how past attacks and defenses relate to these conditions.},
author = {Kovah, Xeno and Kallenberg, Corey and Weathers, Chris and Herzog, Amy and Albin, Matthew and Butterworth, John},
doi = {10.1109/SP.2012.45},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {TOCTOU attack,remote attestation,software-based attestation,timing-based attestation,trusted platform module},
pages = {239--253},
title = {{New results for timing-based attestation}},
year = {2012}
}
@article{Kusters2012,
abstract = {Verifiability is a central property of modern e-voting systems. Intuitively, verifiability means that voters can check that their votes were actually counted and that the published result of the election is correct, even if the voting machines/authorities are (partially) untrusted. In this paper, we raise awareness of a simple attack, which we call a clash attack, on the verifiability of e-voting systems. The main idea behind this attack is that voting machines manage to provide different voters with the same receipt. As a result, the voting authorities can safely replace ballots by new ballots, and by this, manipulate the election without being detected. This attack does not seem to have attracted much attention in the literature. Even though the attack is quite simple, we show that, under reasonable trust assumptions, it applies to several e-voting systems that have been designed to provide verifiability. In particular, we show that it applies to the prominent ThreeBallot and VAV voting systems as well as to two e-voting systems that have been deployed in real elections: the Wombat Voting system and a variant of the Helios voting system. We discuss countermeasures for each of these systems and for (various variants of) Helios provide a formal analysis based on a rigorous definition of verifiability. More precisely, our analysis of Helios is with respect to the more general and in the area of e-voting often overlooked notion of accountability.},
author = {K{\"{u}}sters, Ralf and Truderung, Tomasz and Vogt, Andreas},
doi = {10.1109/SP.2012.32},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {accountability,protocol analysis,verifiability,voting},
pages = {395--409},
title = {{Clash attacks on the verifiability of e-voting systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6234426},
year = {2012}
}
@article{Afroz2012,
abstract = {In digital forensics, questions often arise about the authors of documents: their identity, demographic background, and whether they can be linked to other documents. The field of stylometry uses linguistic features and machine learning techniques to answer these questions. While stylometry techniques can identify authors with high accuracy in non-adversarial scenarios, their accuracy is reduced to random guessing when faced with authors who intentionally obfuscate their writing style or attempt to imitate that of another author. While these results are good for privacy, they raise concerns about fraud. We argue that some linguistic features change when people hide their writing style and by identifying those features, stylistic deception can be recognized. The major contribution of this work is a method for detecting stylistic deception in written documents. We show that using a large feature set, it is possible to distinguish regular documents from deceptive documents with 96.6{\{}{\%}{\}} accuracy (F-measure). We also present an analysis of linguistic features that can be modified to hide writing style.},
author = {Afroz, Sadia and Brennan, Michael and Greenstadt, Rachel},
doi = {10.1109/SP.2012.34},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {deception,machine learning,privacy,stylometry},
pages = {461--475},
title = {{Detecting hoaxes, frauds, and deception in writing style online}},
year = {2012}
}
@article{Wang2012,
abstract = {We demonstrate that unmodified commercial Flash memory can provide two important security functions: true random number generation and digital fingerprinting. Taking advantage of random telegraph noise (a type of quantum noise source in highly scaled Flash memory cells) enables high quality true random number generation at a rate up to 10Kbits / second. A scheme based on partial programming exploits process variation in threshold voltages to allow quick generation of many unique fingerprints that can be used for identification and authentication. Both schemes require no change to Flash chips or interfaces, and do not require additional hardware.},
author = {Wang, Yinglei and Yu, Wing Kei and Wu, Shuo and Malysa, Greg and Suh, G Edward and Kan, Edwin C},
doi = {10.1109/SP.2012.12},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {device authentication,flash memory,hardware fingerprints,security,true random number generation},
pages = {33--47},
title = {{Flash memory for ubiquitous hardware security functions: True random number generation and device fingerprints}},
year = {2012}
}
@article{Bonneau2012,
abstract = {We report on the largest corpus of user-chosen passwords ever studied, consisting of anonymized password histograms representing almost 70 million Yahoo! users, mitigating privacy concerns while enabling analysis of dozens of subpopulations based on demographic factors and site usage characteristics. This large data set motivates a thorough statistical treatment of estimating guessing difficulty by sampling from a secret distribution. In place of previously used metrics such as Shannon entropy and guessing entropy, which cannot be estimated with any realistically sized sample, we develop partial guessing metrics including a new variant of guesswork parameterized by an attacker's desired success rate. Our new metric is comparatively easy to approximate and directly relevant for security engineering. By comparing password distributions with a uniform distribution which would provide equivalent security against different forms of guessing attack, we estimate that passwords provide fewer than 10 bits of security against an online, trawling attack, and only about 20 bits of security against an optimal offline dictionary attack. We find surprisingly little variation in guessing difficulty; every identifiable group of users generated a comparably weak password distribution. Security motivations such as the registration of a payment card have no greater impact than demographic factors such as age and nationality. Even proactive efforts to nudge users towards better password choices with graphical feedback make little difference. More surprisingly, even seemingly distant language communities choose the same weak passwords and an attacker never gains more than a factor of 2 efficiency gain by switching from the globally optimal dictionary to a population-specific lists.},
author = {Bonneau, Joseph},
doi = {10.1109/SP.2012.49},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {authentication,computer security,data mining,information theory,statistics},
number = {Section VII},
pages = {538--552},
title = {{The science of guessing: Analyzing an anonymized corpus of 70 million passwords}},
year = {2012}
}
@article{Hiser2012,
abstract = {Through randomization of the memory space and the confinement of code to non-data pages, computer security researchers have made a wide range of attacks against program binaries more difficult. However, attacks have evolved to exploit weaknesses in these defenses. To thwart these attacks, we introduce a novel technique called Instruction Location Randomization (ILR). Conceptually, ILR randomizes the location of every instruction in a program, thwarting an attacker's ability to re-use program functionality (e.g., arc-injection attacks and return-oriented programming attacks). ILR operates on arbitrary executable programs, requires no compiler support, and requires no user interaction. Thus, it can be automatically applied post-deployment, allowing easy and frequent re-randomization. Our preliminary prototype, working on 32-bit x86 Linux ELF binaries, provides a high degree of entropy. Individual instructions are randomly placed within a 31-bit address space. Thus, attacks that rely on a priori knowledge of the location of code or derandomization are not feasible. We demonstrated ILR's defensive capabilities by defeating attacks against programs with vulnerabilities, including Adobe's PDF viewer, acroread, which had an in-the-wild vulnerability. Additionally, using an industry-standard CPU performance benchmark suite, we compared the run time of prototype ILR-protected executables to that of native executables. The average run-time overhead of ILR was 13{\{}{\%}{\}} with more than half the programs having effectively no overhead (15 out of 29), indicating that ILR is a realistic and cost-effective mitigation technique.},
author = {Hiser, Jason and Nguyen-Tuong, Anh and Co, Michele and Hall, Matthew and Davidson, Jack W},
doi = {10.1109/SP.2012.39},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {ASLR,Diversity,Exploit prevention,Randomization,Return-oriented-programming,arc-injection},
pages = {571--585},
title = {{ILR: Where'd my gadgets go?}},
year = {2012}
}
@article{Kolbitsch2012,
abstract = {JavaScript-based malware attacks have in- creased in recent years and currently represent a significant threat to the use of desktop computers, smartphones, and tablets. While static and runtime methods for malware detection have been proposed in the literature, both on the client side, for just-in- time in-browser detection, as well as offline, crawler- based malware discovery, these approaches encounter the same fundamental limitation. Web-based malware tends to be environment-specific, targeting a particular browser, often attacking specific versions of installed plugins. This targeting occurs because the malware exploits vulnerabilities in specific plugins and fails otherwise. As a result, a fundamental limitation for detecting a piece of malware is thatmalware is triggered infrequently, only showing itself when the right environ- ment is present. We observe that, using fingerprinting techniques that capture and exploit unique properties of browser configurations, almost all existing malware can be made virtually impossible for malware scanners to detect. This paper proposes Rozzle, a JavaScript multi- execution virtual machine, as a way to explore multi- ple execution paths within a single execution so that environment-specific malware will reveal itself. Using large-scale experiments, we show that Rozzle increases the detection rate for offline runtime detection by almost seven times. In addition, Rozzle triples the effectiveness of online runtime detection. We show that Rozzle incurs virtually no runtime overhead and allows us to replace multiple VMs running different browser configurations with a single Rozzle-enabled browser, reducing the hardware requirements, network bandwidth, and power consumption},
author = {Kolbitsch, Clemens and Livshits, Benjamin and Zorn, Benjamin and Seifert, Christian},
doi = {10.1109/SP.2012.48},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {JavaScript,cloaking,malware},
pages = {443--457},
title = {{Rozzle: De-cloaking Internet malware}},
year = {2012}
}
@article{Arden2012,
abstract = {Mobile code is now a nearly inescapable component of modern computing, thanks to client-side code that runs within web browsers. The usual tension between security and functionality is particularly acute in a mobile-code setting, and current platforms disappoint on both dimensions. We introduce a new architecture for secure mobile code, with which developers can use, publish, and share mobile code securely across trust domains. This architecture enables new kinds of distributed applications, and makes it easier to reuse and evolve code from untrusted providers. The architecture gives mobile code considerable expressive power: it can securely access distributed, persistent, shared information from multiple trust domains, unlike web applications bound by the same-origin policy. The core of our approach is analyzing how flows of information within mobile code affect confidentiality and integrity. Because mobile code is untrusted, this analysis requires novel constraints on information flow and authority. We show that these constraints offer principled enforcement of strong security while avoiding the limitations of current mobile-code security mechanisms. We evaluate our approach by demonstrating a variety of mobile-code applications, showing that new functionality can be offered along with strong security.},
author = {Arden, Owen and George, Michael D and Liu, Jed and Vikram, K and Askarov, Aslan and Myers, Andrew C},
doi = {10.1109/SP.2012.22},
isbn = {978-1-4673-1244-8},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Authorization,Computer architecture,Fabrics,Internet,Libraries,Mobile communication,Social network services,Web applications,Web browsers,codes,distributed systems,evolution,information flow,information flow control,mobile code,mobile code sharing,mobile computing,mobile-code security mechanisms,mobile-code setting,modern computing,programming languages,same-origin policy,secure mobile code,security,security of data,trust domains,trusted computing},
pages = {191--205},
title = {{Sharing Mobile Code Securely with Information Flow Control}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6234413},
year = {2012}
}
@article{Hsiao2012,
abstract = {Popular anonymous communication systems often require sending packets through a sequence of relays on dilated paths for strong anonymity protection. As a result, increased end-to-end latency renders such systems inadequate for the majority of Internet users who seek an intermediate level of anonymity protection while using latency-sensitive applications, such as Web applications. This paper serves to bridge the gap between communication systems that provide strong anonymity protection but with intolerable latency and non-anonymous communication systems by considering a new design space for the setting. More specifically, we explore how to achieve near-optimal latency while achieving an intermediate level of anonymity with a weaker yet practical adversary model (i.e., protecting an end-hosts identity and location from servers) such that users can choose between the level of anonymity and usability. We propose Lightweight Anonymity and Privacy (LAP), an efficient network-based solution featuring lightweight path establishment and stateless communication, by concealing an end-hosts topological location to enhance anonymity against remote tracking. To show practicality, we demonstrate that LAP can work on top of the current Internet and proposed future Internet architectures.},
author = {Hsiao, Hsu Chun and Kim, Tiffany Hyun Jin and Perrig, Adrian and Yamada, Akira and Nelson, Samuel C and Gruteser, Marco and Meng, Wei},
doi = {10.1109/SP.2012.37},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {location privacy,low-latency anonymous routing,topological anonymity},
pages = {506--520},
title = {{LAP: Lightweight anonymity and privacy}},
year = {2012}
}
@article{,
journal = {s{\&}p},
title = {{ [SoK]: OB-PWS: Obfuscation-Based Private Web Search }},
year = {2012}
}
@article{Wang2012a,
abstract = {With the boom of software-as-a-service and social networking, web-based single sign-on (SSO) schemes are being deployed by more and more commercial websites to safeguard many web resources. Despite prior research in formal verification, little has been done to analyze the security quality of SSO schemes that are commercially deployed in the real world. Such an analysis faces unique technical challenges, including lack of access to well-documented protocols and code, and the complexity brought in by the rich browser elements (script, Flash, etc.). In this paper, we report the first “field study” on popular web SSO systems. In every studied case, we focused on the actual web traffic going through the browser, and used an algorithm to recover important semantic information and identify potential exploit opportunities. Such opportunities guided us to the discoveries of real flaws. In this study, we discovered 8 serious logic flaws in high-profile ID providers and relying party websites, such as OpenID (including Google ID and PayPal Access), Facebook, JanRain, Freelancer, FarmVille, Sears.com, etc. Every flaw allows an attacker to sign in as the victim user. We reported our findings to affected companies, and received their acknowledgements in various ways. All the reported flaws, except those discovered very recently, have been fixed. This study shows that the overall security quality of SSO deployments seems worrisome. We hope that the SSO community conducts a study similar to ours, but in a larger scale, to better understand to what extent SSO is insecurely deployed and how to respond to the situation.},
author = {Wang, Rui and Chen, Shuo and Wang, XiaoFeng},
doi = {10.1109/SP.2012.30},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {Authentication,Logic Flaw,Secure Protocol,Single-Sign-On,Web Service},
pages = {365--379},
title = {{Signing me onto your accounts through Facebook and Google: A traffic-guided security study of commercially deployed single-sign-on web services}},
year = {2012}
}
@article{Jana2012,
abstract = {We describe a new side-channel attack. By tracking changes in the application's memory footprint, a concurrent process belonging to a different user can learn its secrets. Using Web browsers as the target, we show how an unprivileged, local attack process-for example, a malicious Android app-can infer which page the user is browsing, as well as finer-grained information: whether she is a paid customer, her interests, etc. This attack is an instance of a broader problem. Many isolation mechanisms in modern systems reveal accounting information about program execution, such as memory usage and CPU scheduling statistics. If temporal changes in this public information are correlated with the program's secrets, they can lead to a privacy breach. To illustrate the pervasiveness of this problem, we show how to exploit scheduling statistics for keystroke sniffing in Linux and Android, and how to combine scheduling statistics with the dynamics of memory usage for more accurate adversarial inference of browsing behavior.},
author = {Jana, Suman and Shmatikov, Vitaly},
doi = {10.1109/SP.2012.19},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
pages = {143--157},
title = {{Memento: Learning secrets from process footprints}},
year = {2012}
}
@article{Huang2012,
abstract = {Known protocols for secure two-party computation that are designed to provide full security against malicious behavior are significantly less efficient than protocols intended only to thwart semi-honest adversaries. We present a concrete design and implementation of protocols achieving security guarantees that are much stronger than are possible with semi-honest protocols, at minimal extra cost. Specifically, we consider protocols in which a malicious adversary may learn a single (arbitrary) bit of additional information about the honest party's input. Correctness of the honest party's output is still guaranteed. Adapting prior work of Mohassel and Franklin, the basic idea in our protocols is to conduct two separate runs of a (specific) semi-honest, garbled-circuit protocol, with the parties swapping roles, followed by an inexpensive secure equality test. We provide a rigorous definition and prove that this protocol leaks no more than one additional bit against a malicious adversary. In addition, we propose some heuristic enhancements to reduce the overall information a cheating adversary learns. Our experiments show that protocols meeting this security level can be implemented at cost very close to that of protocols that only achieve semi-honest security. Our results indicate that this model enables the large-scale, practical applications possible within the semi-honest security model, while providing dramatically stronger security guarantees.},
author = {Huang, Yan and Katz, Jonathan and Evans, David},
doi = {10.1109/SP.2012.43},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {privacy-preserving protocols,secure two-party computation},
pages = {272--284},
title = {{Quid-pro-quo-tocols: Strengthening semi-honest protocols with dual execution}},
year = {2012}
}
@article{Becker2012,
abstract = {Over the last 15 years, many policy languages have been developed for specifying policies and credentials under the trust management paradigm. What has been missing is a formal semantics - in particular, one that would capture the inherently dynamic nature of trust management, where access decisions are based on the local policy in conjunction with varying sets of dynamically submitted credentials. The goal of this paper is to rest trust management on a solid formal foundation. To this end, we present a model theory that is based on Kripke structures for counterfactual logic. The semantics enjoys compositionality and full abstraction with respect to a natural notion of observational equivalence between trust management policies. Furthermore, we present a corresponding Hilbert-style axiomatization that is expressive enough for reasoning about a system's observables on the object level. We describe an implementation of a mechanization of the proof theory, which can be used to prove non-trivial meta-theorems about trust management systems, as well as analyze probing attacks on such systems. Our benchmark results show that this logic-based approach performs significantly better than the only previously available, ad-hoc analysis method for probing attacks.},
author = {Becker, Moritz Y and Russo, Alessandra and Sultana, Nik},
doi = {10.1109/SP.2012.20},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {Datalog,access control,counterfactual logic,credential,policy language,probing attack,semantics,trust management},
number = {c},
pages = {161--175},
title = {{Foundations of logic-based trust management}},
year = {2012}
}
@article{Jang2012,
abstract = {Programmers should never fix the same bug twice. Unfortunately this often happens when patches to buggy code are not propagated to all code clones. Unpatched code clones represent latent bugs, and for security-critical problems, latent vulnerabilities, thus are important to detect quickly. In this paper we present ReDeBug, a system for quickly finding unpatched code clones in OS-distribution scale code bases. While there has been previous work on code clone detection, ReDeBug represents a unique design point that uses a quick, syntax-based approach that scales to OS distribution-sized code bases that include code written in many different languages. Compared to previous approaches, ReDeBug may find fewer code clones, but gains scale, speed, reduces the false detection rate, and is language agnostic. We evaluated ReDeBug by checking all code from all packages in the Debian Lenny/Squeeze, Ubuntu Maverick/Oneiric, all Source Forge C and C++ projects, and the Linux kernel for unpatched code clones. ReDeBug processed over 2.1 billion lines of code at 700,000 LoC/min to build a source code database, then found 15,546 unpatched copies of known vulnerable code in currently deployed code by checking 376 Debian/Ubuntu security-related patches in 8 minutes on a commodity desktop machine. We show the real world impact of ReDeBug by confirming 145 real bugs in the latest version of Debian Squeeze packages.},
author = {Jang, Jiyong and Agrawal, Abeer and Brumley, David},
doi = {10.1109/SP.2012.13},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {debug,scalability,unpatched code clone},
pages = {48--62},
title = {{ReDeBug: Finding unpatched code clones in entire OS distributions}},
year = {2012}
}
@article{Invernizzi2012,
abstract = {Malicious web pages that use drive-by download attacks or social engineering techniques to install unwanted software on a user's computer have become the main avenue for the propagation of malicious code. To search for malicious web pages, the first step is typically to use a crawler to collect URLs that are live on the Internet. Then, fast prefiltering techniques are employed to reduce the amount of pages that need to be examined by more precise, but slower, analysis tools (such as honey clients). While effective, these techniques require a substantial amount of resources. A key reason is that the crawler encounters many pages on the web that are benign, that is, the "toxicity" of the stream of URLs being analyzed is low. In this paper, we present EVILSEED, an approach to search the web more efficiently for pages that are likely malicious. EVILSEED starts from an initial seed of known, malicious web pages. Using this seed, our system automatically generates search engines queries to identify other malicious pages that are similar or related to the ones in the initial seed. By doing so, EVILSEED leverages the crawling infrastructure of search engines to retrieve URLs that are much more likely to be malicious than a random page on the web. In other words EVILSEED increases the "toxicity" of the input URL stream. Also, we envision that the features that EVILSEED presents could be directly applied by search engines in their prefilters. We have implemented our approach, and we evaluated it on a large-scale dataset. The results show that EVILSEED is able to identify malicious web pages more efficiently when compared to crawler-based approaches.},
author = {Invernizzi, Luca and Comparetti, Paolo Milani and Benvenuti, Stefano and Kruegel, Christopher and Cova, Marco and Vigna, Giovanni},
doi = {10.1109/SP.2012.33},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {Drive-By Downloads,Guided Crawl-Web Security,Guided Crawling,Web Security},
pages = {428--442},
title = {{EvilSeed: A guided approach to finding malicious web pages}},
year = {2012}
}
@article{DeCristofaro2012,
abstract = {In the last several years, micro-blogging Online Social Networks (OSNs), such as Twitter, have taken the world by storm, now boasting over 100 million subscribers. As an unparalleled stage for an enormous audience, they offer fast and reliable centralized diffusion of pithy tweets to great multitudes of information-hungry and always-connected followers. At the same time, this information gathering and dissemination paradigm prompts some important privacy concerns about relationships between tweeters, followers and interests of the latter. In this paper, we assess privacy in today's Twitter-like OSNs and describe an architecture and a trial implementation of a privacy-preserving service called Hummingbird. It is essentially a variant of Twitter that protects tweet contents, hashtags and follower interests from the (potentially) prying eyes of the centralized server. We argue that, although inherently limited by Twitter's mission of scalable information-sharing, this degree of privacy is valuable. We demonstrate, via a working prototype, that Hummingbird's additional costs are tolerably low. We also sketch out some viable enhancements that might offer better privacy in the long term.},
author = {{De Cristofaro}, Emiliano and Soriente, Claudio and Tsudik, Gene and Williams, Andrew},
doi = {10.1109/SP.2012.26},
isbn = {978-1-4673-1244-8},
issn = {10816011},
journal = {s{\&}p},
keywords = {privacy twitter multi-party computation},
pages = {285--299},
title = {{Hummingbird: Privacy at the time of Twitter}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6234419},
year = {2012}
}
@article{Kelley2012,
author = {Kelley, Patrick},
journal = {s{\&}p},
title = {{Guess again ( and again and again ): Measuring password strength by simulating password- cracking algorithms ( CMU-CyLab-11-008 )}},
year = {2012}
}
@article{Bonneau2012a,
abstract = {We evaluate two decades of proposals to replace text passwords for general-purpose user authentication on the web using a broad set of twenty-five usability, deployability and security benefits that an ideal scheme might provide. The scope of proposals we survey is also extensive, including password management software, federated login protocols, graphical password schemes, cognitive authentication schemes, one-time passwords, hardware tokens, phone-aided schemes and biometrics. Our comprehensive approach leads to key insights about the difficulty of replacing passwords. Not only does no known scheme come close to providing all desired benefits: none even retains the full set of benefits that legacy passwords already provide. In particular, there is a wide range from schemes offering minor security benefits beyond legacy passwords, to those offering significant security benefits in return for being more costly to deploy or more difficult to use. We conclude that many academic proposals have failed to gain traction because researchers rarely consider a sufficiently wide range of real-world constraints. Beyond our analysis of current schemes, our framework provides an evaluation methodology and benchmark for future web authentication proposals.},
author = {Bonneau, Joseph and Herley, Cormac and {Van Oorschot}, Paul C and Stajano, Frank},
doi = {10.1109/SP.2012.44},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {authentication,computer security,deployability,economics,human computer interaction,security and usability,software engineering},
pages = {553--567},
title = {{The quest to replace passwords: A framework for comparative evaluation of web authentication schemes}},
year = {2012}
}
@article{Dai2012,
abstract = {Abstract—Response-computable authentication (RCA) is a two-party authentication model widely adopted by authentication systems, where an authentication system independently computes the expected user response and authenticates a user if the actual user response matches the expected value. Such authentication systems have long been threatened by malicious developers who can plant backdoors to bypass normal authentication, which is often seen in insider-related incidents. A malicious developer can plant backdoors by hiding logic in source code, by planting delicate vulnerabilities, or even by using weak cryptographic algorithms. Because of the common usage of cryptographic techniques and code protection in authentication modules, it is very difficult to detect and eliminate backdoors from login systems. In this paper, we propose a framework for RCA systems to ensure that the authentication process is not affected by backdoors. Our approach decomposes the authentication module into components. Components with simple logic are verified by code analysis for correctness; components with cryp- tographic/obfuscated logic are sandboxed and verified through testing. The key component of our approach is NaPu, a native sandbox to ensure pure functions, which protects the complex and backdoor-prone part of a login module.We also use a testing-based process to either detect backdoors in the sandboxed component or verify that the component has no backdoors that can be used practically. We demonstrated the effectiveness of our approach in real-world applications by porting and verifying several popular login modules into this framework},
author = {Dai, Shuaifu and Wei, Tao and Zhang, Chao and Wang, Tielei and Ding, Yu and Liang, Zhenkai and Zou, Wei},
doi = {10.1109/SP.2012.10},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
pages = {3--17},
title = {{A framework to eliminate backdoors from response-computable authentication}},
year = {2012}
}
@article{Cha2012,
abstract = {In this paper we present MAYHEM, a new sys- tem for automatically finding exploitable bugs in binary (i.e., executable) programs. Every bug reported by MAYHEM is accompanied by a working shell-spawning exploit. The working exploits ensure soundness and that each bug report is security- critical and actionable. M AYHEM works on raw binary code without debugging information. To make exploit generation possible at the binary-level, MAYHEM addresses two major technical challenges: actively managing execution paths without exhausting memory, and reasoning about symbolic memory indices, where a load or a store address depends on user input. To this end, we propose two novel techniques: 1) hybrid symbolic execution for combining online and offline (concolic) execution to maximize the benefits of both techniques, and 2) index-based memory modeling, a technique that allows MAYHEM to efficiently reason about symbolic memory at the binary level. We used M AYHEM to find and demonstrate 29 exploitable vulnerabilities in both Linux and Windows programs, 2 of which were previously undocumented.},
author = {Cha, Sang Kil and Avgerinos, Thanassis and Rebert, Alexandre and Brumley, David},
doi = {10.1109/SP.2012.31},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {exploit generation,hybrid execution,index-based memory modeling,symbolic memory},
pages = {380--394},
title = {{Unleashing Mayhem on binary code}},
year = {2012}
}
@article{Howe2012,
abstract = {The home computer user is often said to be the weakest link in computer security. They do not always follow security advice, and they take actions, as in phishing, that compromise themselves. In general, we do not understand why users do not always behave safely, which would seem to be in their best interest. This paper reviews the literature of surveys and studies of factors that influence security decisions for home computer users. We organize the review in four sections: un- derstanding of threats, perceptions of risky behavior, efforts to avoid security breaches and attitudes to security interventions. We find that these studies reveal a lot of reasons why current security measures may not match the needs or abilities of home computer users and suggest future work needed to inform how security is delivered to this user group},
author = {Howe, Adele E and Ray, Indrajit and Roberts, Mark and Urbanska, Malgorzata and Byrne, Zinta},
doi = {10.1109/SP.2012.23},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {home users,usability and security},
pages = {209--223},
title = {{The psychology of security for the home computer user}},
year = {2012}
}
@article{Dyer2012,
abstract = {We consider the setting of HTTP traffic over encrypted tunnels, as used to conceal the identity of websites visited by a user. It is well known that traffic analysis (TA) attacks can accurately identify the website a user visits despite the use of encryption, and previous work has looked at specific attack/countermeasure pairings. We provide the first comprehensive analysis of general-purpose TA countermeasures. We show that nine known countermeasures are vulnerable to simple attacks that exploit coarse features of traffic (e.g., total time and bandwidth). The considered countermeasures include ones like those standardized by TLS, SSH, and IPsec, and even more complex ones like the traffic morphing scheme of Wright et al. As just one of our results, we show that despite the use of traffic morphing, one can use only total upstream and downstream bandwidth to identify -- with 98{\{}{\%}{\}} accuracy - which of two websites was visited. One implication of what we find is that, in the context of website identification, it is unlikely that bandwidth-efficient, general-purpose TA countermeasures can ever provide the type of security targeted in prior work.},
author = {Dyer, Kevin P and Coull, Scott E and Ristenpart, Thomas and Shrimpton, Thomas},
doi = {10.1109/SP.2012.28},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {encrypted traffic,machine learning,padding,privacy,traffic analysis countermeasures},
pages = {332--346},
title = {{Peek-a-boo, I still see you: Why efficient traffic analysis countermeasures fail}},
year = {2012}
}
@article{Payer2012,
abstract = {The standard loader (ld.so) is a common target of attacks. The loader is a trusted component of the application, and faults in the loader are problematic, e.g., they may lead to local privilege escalation for SUID binaries. Software-based fault isolation (SFI) provides a framework to execute arbitrary code while protecting the host system. A problem of current approaches to SFI is that fault isolation is decoupled from the dynamic loader, which is treated as a black box. The sandbox has no information about the (expected) execution behavior of the application and the connections between different shared objects. As a consequence, SFI is limited in its ability to identify devious application behavior. This paper presents a new approach to run untrusted code in a user-space sandbox. The approach replaces the standard loader with a security-aware trusted loader. The secure loader and the sandbox together cooperate to allow controlled execution of untrusted programs. A secure loader makes security a first class concept and ensures that the SFI system does not allow any unchecked code to be executed. The user-space sandbox builds on the secure loader and subsequently dynamically checks for malicious code and ensures that all control flow instructions of the application adhere to an execution model. The combination of the secure loader and the user-space sandbox enables the safe execution of untrusted code in user-space. Code injection attacks are stopped before any unintended code is executed. Furthermore, additional information provided by the loader can be used to support additional security properties, e.g., in lining of Procedure Linkage Table calls reduces the number of indirect control flow transfers and therefore limits jump-oriented attacks. This approach implements a secure platform for privileged applications and applications reachable over the network that anticipates and confines security threats from the beginning.},
author = {Payer, Mathias and Hartmann, Tobias and Gross, Thomas R},
doi = {10.1109/SP.2012.11},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {process creation,sandboxing,secure loading,software-based fault isolation},
pages = {18--32},
title = {{Safe loading - A foundation for secure execution of untrusted programs}},
year = {2012}
}
@article{Zhou2012,
abstract = {A trusted path is a protected channel that assures the secrecy and authenticity of data transfers between a user's input/output (I/O) device and a program trusted by that user. We argue that, despite its incontestable necessity, current commodity systems do not support trusted path with any significant assurance. This paper presents a hyper visor-based design that enables a trusted path to bypass an untrusted operating-system, applications, and I/O devices, with a minimal Trusted Computing Base (TCB). We also suggest concrete I/O architectural changes that will simplify future trusted-path system design. Our system enables users to verify the states and configurations of one or more trusted-paths using a simple, secret less, hand-held device. We implement a simple user-oriented trusted path as a case study.},
author = {Zhou, Zongwei and Gligor, Virgil D and Newsome, James and McCune, Jonathan M},
doi = {10.1109/SP.2012.42},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {Device Input/Output,Hypervisor,Isolation,Trusted Path,Trustworthy Computing},
pages = {616--630},
title = {{Building verifiable trusted path on commodity x86 computers}},
year = {2012}
}
@article{Cremers2012,
abstract = {After several years of theoretical research on distance bounding protocols, the first implementations of such protocols have recently started to appear. These protocols are typically analyzed with respect to three types of attacks, which are historically known as Distance Fraud, Mafia Fraud, and Terrorist Fraud. We define and analyze a fourth main type of attack on distance bounding protocols, called Distance Hijacking. This type of attack poses a serious threat in many practical scenarios. We show that many proposed distance bounding protocols are vulnerable to Distance Hijacking, and we propose solutions to make these protocols resilient to this type of attack. We show that verifying distance bounding protocols using existing informal and formal frameworks does not guarantee the absence of Distance Hijacking attacks. We extend a formal framework for reasoning about distance bounding protocols to include overshadowing attacks. We use the resulting framework to prove the absence of all of the found attacks for protocols to which our countermeasures have been applied.},
author = {Cremers, Cas and Rasmussen, Kasper B and Schmidt, Benedikt and Capkun, Srdjan},
doi = {10.1109/SP.2012.17},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {Distance bounding,attacks,formal model,formal verification,hijacking,location verification,multi-prover environment,position verification},
pages = {113--127},
title = {{Distance hijacking attacks on distance bounding protocols}},
year = {2012}
}
@article{Zhou2012a,
abstract = {The popularity and adoption of smartphones has greatly stimulated the spread of mobile malware, especially on the popular platforms such as Android. In light of their rapid growth, there is a pressing need to develop effective solutions. However, our defense capability is largely constrained by the limited understanding of these emerging mobile malware and the lack of timely access to related samples. In this paper, we focus on the Android platform and aim to systematize or characterize existing Android malware. Particularly, with more than one year effort, we have managed to collect more than 1,200 malware samples that cover the majority of existing Android malware families, ranging from their debut in August 2010 to recent ones in October 2011. In addition, we systematically characterize them from various aspects, including their installation methods, activation mech- anisms as well as the nature of carried malicious payloads. The characterization and a subsequent evolution-based study of representative families reveal that they are evolving rapidly to circumvent the detection from existing mobile anti-virus software. Based on the evaluation with four representative mobile security software, our experiments show that the best case detects 79.6{\{}{\%}{\}} of them while the worst case detects only 20.2{\{}{\%}{\}} in our dataset. These results clearly call for the need to better develop next-generation anti-mobile-malware solutions.},
author = {Zhou, Yajin and Jiang, Xuxian},
doi = {10.1109/SP.2012.16},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {Android malware,smartphone security},
number = {4},
pages = {95--109},
title = {{Dissecting Android malware: Characterization and evolution}},
year = {2012}
}
@article{Narayanan2012,
abstract = {We study techniques for identifying an anonymous author via linguistic stylometry, i.e., comparing the writing style against a corpus of texts of known authorship. We exper- imentally demonstrate the effectiveness of our techniques with as many as 100,000 candidate authors. Given the increasing availability of writing samples online, our result has serious implications for anonymity and free speech — an anonymous blogger or whistleblower may be unmasked unless they take steps to obfuscate their writing style. While there is a huge body of literature on authorship recognition based on writing style, almost none of it has studied corpora of more than a few hundred authors. The problem becomes qualitatively different at a large scale, as we show, and techniques from prior work fail to scale, both in terms of accuracy and performance. We study a variety of classifiers, both “lazy” and “eager,” and show how to handle the huge number of classes. We also develop novel techniques for confi- dence estimation of classifier outputs. Finally, we demonstrate stylometric authorship recognition on texts written in different contexts. In over 20{\{}{\%}{\}} of cases, our classifiers can correctly identify an anonymous author given a corpus of texts from 100,000 authors; in about 35{\{}{\%}{\}} of cases the correct author is one of the top 20 guesses. If we allow the classifier the option of not making a guess, via confidence estimation we are able to increase the precision of the top guess from 20{\{}{\%}{\}} to over 80{\{}{\%}{\}} with only a halving of recall.},
author = {Narayanan, Arvind and Paskov, Hristo and Gong, Neil Zhenqiang and Bethencourt, John and Stefanov, Emil and Shin, Eui Chul Richard and Song, Dawn},
doi = {10.1109/SP.2012.46},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
pages = {300--314},
title = {{On the feasibility of internet-scale author identification}},
year = {2012}
}
@article{Qian2012,
abstract = {In this paper, we report a newly discovered "off- path TCP sequence number inference" attack enabled by firewall middleboxes. It allows an off-path (i.e., not man-in-the-middle) attacker to hijack a TCP connection and inject malicious content, effectively granting the attacker write-only permission on the connection. For instance, with the help of unprivileged malware, we demonstrate that a successful attack can hijack an HTTP session and return a phishing Facebook login page issued by a browser. With the same mechanisms, it is also possible to inject malicious Javascript to post tweets or follow other people on behalf of the victim. The TCP sequence number inference attack is mainly enabled by the sequence-number-checking firewall middleboxes. Through carefully-designed and well-timed probing, the TCP sequence number state kept on the firewall middlebox can be leaked to an off-path attacker. We found such firewall middleboxes to be very popular in cellular networks — at least 31.5{\{}{\%}{\}} of the 149 measured networks deploy such firewalls. Finally, since the sequence-number-checking feature is enabled by design, it is unclear how to mitigate the problem easily.},
author = {Qian, Zhiyun and Mao, Z Morley},
doi = {10.1109/SP.2012.29},
isbn = {978-1-4673-1244-8},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Firewall middleboxes,HTTP session,IP networks,Java,Javascript,Malware,Middleboxes,Radiation detectors,Servers,TCP connection,TCP connection hijack,TCP sequence number inference,Web browser,authorisation,carefully-designed probing,cellular networks,cellular radio,computer crime,computer network security,firewall middleboxes,hypermedia,invasive software,malicious content,off-path TCP sequence number inference attack,off-path attacker,online front-ends,phishing Face book login page,security reduction,sequence-number-checking feature,sequence-number-checking firewall middle boxes,social networking (online),transport protocols,unprivileged malware,well-timed probing,write-only permission},
pages = {347--361},
title = {{Off-path TCP sequence number inference attack: How firewall middleboxes reduce security}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6234423},
year = {2012}
}
@article{Backes2012,
abstract = {Online behavioral advertising (OBA) involves the tracking of web users' online activities in order to deliver tailored advertisements. OBA has become a rapidly increasing source of revenue for a number of web services, and it is typically conducted by third-party data analytics firms such as brokers, which track user behaviors across web-sessions using mechanisms such as persistent cookies. This practice raises significant privacy concerns among users and privacy advocates alike. Therefore, the task of designing OBA systems that do not reveal user profiles to third parties has been receiving growing interest from the research community. Nevertheless, existing solutions are not ideal for privacy preserving OBA: some of them do not provide adequate privacy to users or adequate targeting information to brokers, while others require trusted third parties that are difficult to realize. In this paper, we propose ObliviAd a provably secure architecture for privacy preserving OBA. The distinguishing features of our approach are the usage of secure hardware-based private information retrieval for distributing advertisements and high-latency mixing of electronic tokens for billing advertisers without disclosing any information about client profiles to brokers. ObliviAd does not assume any trusted party and provides brokers an economical alternative that preserves the privacy of users without hampering the precision of ads selection. We present the first formal security definitions for OBA systems (namely, profile privacy, profile unlink ability, and billing correctness) and conduct a formal security analysis of ObliviAd using ProVerif, an automated cryptographic protocol verifier, establishing the aforementioned security properties against a strong adversarial model. Finally, we demonstrated the practicality of our approach with an experimental evaluation.},
author = {Backes, Michael and Kate, Aniket and Maffei, Matteo and Pecina, Kim},
doi = {10.1109/SP.2012.25},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {behavorial advertising,formal verification,oblivious ram,privacy,private information retrieval,trusted hardware,unlinkability},
pages = {257--271},
title = {{ObliviAd: Provably secure and practical online behavioral advertising}},
year = {2012}
}
@article{Tschantz2012,
abstract = {Privacy policies often place restrictions on the purposes for which a governed entity may use personal information. For example, regulations, such as the Health Insurance Portability and Accountability Act (HIPAA), require that hospital employees use medical information for only certain purposes, such as treatment, but not for others, such as gossip. Thus, using formal or automated methods for enforcing privacy policies requires a semantics of purpose restrictions to determine whether an action is for a purpose or not. We provide such a semantics using a formalism based on planning. We model planning using a modified version of Markov Decision Processes (MDPs), which exclude redundant actions for a formal definition of redundant. We argue that an action is for a purpose if and only if the action is part of a plan for optimizing the satisfaction of that purpose under the MDP model. We use this formalization to define when a sequence of actions is only for or not for a purpose. This semantics enables us to create and implement an algorithm for automating auditing, and to describe formally and compare rigorously previous enforcement methods. To validate our semantics, we conduct a survey to compare our semantics to how people commonly understand the word "purpose".},
author = {Tschantz, Michael Carl and Datta, Anupam and Wing, Jeannette M},
doi = {10.1109/SP.2012.21},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {Formal Methods,Privacy},
pages = {176--190},
title = {{Formalizing and enforcing purpose restrictions in privacy policies}},
year = {2012}
}
@article{Rossow2012,
abstract = {Malware researchers rely on the observation of malicious code in execution to collect datasets for a wide array of experiments, including generation of detection models, study of longitudinal behavior, and validation of prior research. For such research},
author = {Rossow, Christian and Dietrich, Christian J and Grier, Chris and Kreibich, Christian and Paxson, Vern and Pohlmann, Norbert and Bos, Herbert and {Van Steen}, Maarten},
doi = {10.1109/SP.2012.14},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {datasets,dynamic analysis,experiments,malware},
pages = {65--79},
title = {{Prudent practices for designing malware experiments: Status quo and outlook}},
year = {2012}
}
@article{Akhoondi2012,
abstract = {The widely used Tor anonymity network is designed to enable low-latency anonymous communication. However, in practice, interactive communication on Tor-which accounts for over 90{\{}{\%}{\}} of connections in the Tor network [1]-incurs latencies over 5x greater than on the direct Internet path. In addition, since path selection to establish a circuit in Tor is oblivious to Internet routing, anonymity guarantees can breakdown in cases where an autonomous system (AS) can correlate traffic across the entry and exit segments of a circuit. In this paper, we show that both of these shortcomings in Tor can be addressed with only client-side modifications, i.e., without requiring a revamp of the entire Tor architecture. To this end, we design and implement a new Tor client, LASTor. First, we show that LASTor can deliver significant latency gains over the default Tor client by simply accounting for the inferred locations of Tor relays while choosing paths. Second, since the preference for low latency paths reduces the entropy of path selection, we design LASTor's path selection algorithm to be tunable. A user can choose an appropriate tradeoff between latency and anonymity by specifying a value between 0 (lowest latency) and 1 (highest anonymity) for a single parameter. Lastly, we develop an efficient and accurate algorithm to identify paths on which an AS can correlate traffic between the entry and exit segments. This algorithm enables LASTor to avoid such paths and improve a user's anonymity, while the low runtime of the algorithm ensures that the impact on end-to-end latency of communication is low. By applying our techniques to measurements of real Internet paths and by using LASTor to visit the top 200 websites from several geographically-distributed end-hosts, we show that, in comparison to the default Tor client, LASTor reduces median latencies by 25{\{}{\%}{\}} while also reducing the false negative rate of not detecting a potential snooping AS from 57{\{}{\%}{\}} to 11{\{}{\%}{\}}. Index Terms},
author = {Akhoondi, Masoud and Yu, Curtis and Madhyastha, Harsha V},
doi = {10.1109/SP.2012.35},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
pages = {476--490},
title = {{LASTor: A low-latency AS-aware Tor client}},
year = {2012}
}
@article{Jana2012a,
abstract = {We systematically describe two classes of evasion exploits against automated malware detectors. Chameleon at- tacks confuse the detectors’ file-type inference heuristics, while werewolf attacks exploit discrepancies in format-specific file parsing between the detectors and actual operating systems and applications. These attacks do not rely on obfuscation, metamorphism, binary packing, or any other changes to malicious code. Because they enable even the simplest, easily detectable viruses to evade detection, we argue that file pro- cessing has become the weakest link of malware defense. Using a combination of manual analysis and black-box differential fuzzing, we discovered 45 new evasion exploits and tested them against 36 popular antivirus scanners, all of which proved vulnerable to various chameleon and werewolf attacks. I.},
author = {Jana, Suman and Shmatikov, Vitaly},
doi = {10.1109/SP.2012.15},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
pages = {80--94},
title = {{Abusing file processing in malware detectors for fun and profit}},
year = {2012}
}
@article{,
journal = {s{\&}p},
title = {{ [SoK]: Third-Party Web Tracking Policy and Technology }},
year = {2012}
}
@article{Fu2012,
abstract = {It is generally believed to be a tedious, time consuming, and error-prone process to develop a virtual machine introspection (VMI) tool manually because of the semantic gap. Recent advances in Virtuoso show that we can largely narrow the semantic gap. But it still cannot completely automate the VMI tool generation. In this paper, we present VMST, an entirely new technique that can automatically bridge the semantic gap and generate the VMI tools. The key idea is that, through system wide instruction monitoring, we can automatically identify the introspection related data and redirect these data accesses to the in-guest kernel memory. VMST offers a number of new features and capabilities. Particularly, it automatically enables an in-guest inspection program to become an introspection program. We have tested VMST over 15 commonly used utilities on top of 20 different Linux kernels. The experimental results show that our technique is general (largely OS-agnostic), and it introduces 9.3X overhead on average for the introspected program compared to the native non-redirected one.},
author = {Fu, Yangchun and Lin, Zhiqiang},
doi = {10.1109/SP.2012.40},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
pages = {586--600},
title = {{Space traveling across VM: Automatically bridging the semantic gap in virtual machine introspection via online kernel data redirection}},
year = {2012}
}
@article{,
journal = {s{\&}p},
title = {{ Scalable Fault Localization under Dynamic Traffic Patterns }},
year = {2012}
}
@article{Pappas2012,
abstract = {The wide adoption of non-executable page protec- tions in recent versions of popular operating systems has given rise to attacks that employ return-oriented programming (ROP) to achieve arbitrary code execution without the injection of any code. Existing defenses against ROP exploits either require source code or symbolic debugging information, or impose a significant runtime overhead, which limits their applicability for the protection of third-party applications. In this paper we present in-place code randomization, a practical mitigation technique against ROP attacks that can be applied directly on third-party software. Our method uses various narrow-scope code transformations that can be applied statically, without changing the location of basic blocks, allowing the safe randomization of stripped binaries even with partial disassembly coverage. These transformations effectively eliminate about 10{\{}{\%}{\}}, and probabilistically break about 80{\{}{\%}{\}} of the useful instruction sequences found in a large set of PE files. Since no additional code is inserted, in-place code randomization does not incur any measurable runtime overhead, enabling it to be easily used in tandem with existing exploit mitigations such as address space layout randomization. Our evaluation using publicly available ROP exploits and two ROP code generation toolkits demonstrates that our technique prevents the exploitation of the tested vulnerableWindows 7 applications, including Adobe Reader, as well as the automated construction of alternative ROP payloads that aim to circumvent in-place code randomization using solely any remaining unaffected instruction sequences.},
author = {Pappas, Vasilis and Polychronakis, Michalis and Keromytis, Angelos D},
doi = {10.1109/SP.2012.41},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
pages = {601--615},
title = {{Smashing the gadgets: Hindering return-oriented programming using in-place code randomization}},
year = {2012}
}
@article{Driessen2012,
abstract = {There is a rich body of work related to the security aspects of cellular mobile phones, in particular with respect to the GSM and UMTS systems. To the best of our knowledge, however, there has been no investigation of the security of satellite phones (abbr. sat phones). Even though a niche market compared to the G2 and G3 mobile systems, there are several 100,000 sat phone subscribers worldwide. Given the sensitive nature of some of their application domains (e.g., natural disaster areas or military campaigns), security plays a particularly important role for sat phones. In this paper, we analyze the encryption systems used in the two existing (and competing) sat phone standards, GMR-1 and GMR-2. The first main contribution is that we were able to completely reverse engineer the encryption algorithms employed. Both ciphers had not been publicly known previously. We describe the details of the recovery of the two algorithms from freely available DSP-firmware updates for sat phones, which included the development of a custom disassembler and tools to analyze the code, and extending prior work on binary analysis to efficiently identify cryptographic code. We note that these steps had to be repeated for both systems, because the available binaries were from two entirely different DSP processors. Perhaps somewhat surprisingly, we found that the GMR-1 cipher can be considered a proprietary variant of the GSM A5/2 algorithm, whereas the GMR-2 cipher is an entirely new design. The second main contribution lies in the cryptanalysis of the two proprietary stream ciphers. We were able to adopt known A5/2 cipher text-only attacks to the GMR-1 algorithm with an average case complexity of 232 steps. With respect to the GMR-2 cipher, we developed a new attack which is powerful in a known-plaintext setting. In this situation, the encryption key for one session, i.e., one phone call, can be recovered with approximately 50-65 bytes of key stream and a moderate computation- l complexity. A major finding of our work is that the stream ciphers of the two existing satellite phone systems are considerably weaker than what is state-of-the-art in symmetric cryptography.},
author = {Driessen, Benedikt and Hund, Ralf and Willems, Carsten and Paar, Christof and Holz, Thorsten},
doi = {10.1109/SP.2012.18},
isbn = {9780769546810},
issn = {10816011},
journal = {s{\&}p},
keywords = {Binary Analysis,Cryptanalysis,Mobile Security,Satellite Phone Systems},
pages = {128--142},
title = {{Don't trust satellite phones: A security analysis of two satphone standards}},
year = {2012}
}
@article{White2011,
abstract = {In this work, we unveil new privacy threats against Voice-over-IP (VoIP) communications. Although prior work has shown that the interaction of variable bit-rate codecs and length-preserving stream ciphers leaks information, we show that the threat is more serious than previously thought. In particular, we derive approximate transcripts of encrypted VoIP conversations by segmenting an observed packet stream into subsequences representing individual phonemes and classifying those subsequences by the phonemes they encode. Drawing on insights from the computational linguistics and speech recognition communities, we apply novel techniques for unmasking parts of the conversation. We believe our ability to do so underscores the importance of designing secure (yet efficient) ways to protect the confidentiality of VoIP conversations.},
author = {White, Andrew M and Matthews, Austin R and Snow, Kevin Z and Monrose, Fabian},
doi = {10.1109/SP.2011.34},
isbn = {978-1-4577-0147-4},
issn = {1081-6011},
journal = {s{\&}p},
pages = {3--18},
title = {{Phonotactic Reconstruction of Encrypted VoIP Conversations: Hookt on Fon-iks}},
year = {2011}
}
@article{Fredrikson2011,
abstract = {We present REPRIV, a system that combines the goals of privacy and content personalization in the browser. REPRIV discovers user interests and shares them with third- parties, but only with an explicit permission of the user. We demonstrate how always-on user interest mining can effectively infer user interests in a real browser. We go on to discuss an extension framework that allows third-party code to extract and disseminate more detailed information, as well as language-based techniques for verifying the absence of privacy leaks in this untrusted code. To demonstrate the effectiveness of our model, we present REPRIV extensions that perform personalization for Netflix, Twitter, Bing, and GetGlue. This paper evaluates important aspects of REPRIV in realistic scenarios. We show that REPRIV’s default in-browser mining can be done with no noticeable overhead to normal browsing, and that the results it produces converge quickly. We demonstrate that REPRIV personalization yields higher quality results than those that may be obtained about the user from public sources. We then go on to show similar results for each of our case studies: that REPRIV enables high-quality personalization, as shown by cases studies in news and search result personalization we evaluated on thousands of instances, and that the performance impact each case has on the browser is minimal. We conclude that personalized content and individual privacy on the web are not mutually exclusive},
author = {Fredrikson, Matthew and Livshits, Benjamin},
doi = {10.1109/SP.2011.37},
isbn = {978-1-4577-0147-4},
issn = {10816011},
journal = {s{\&}p},
pages = {131--146},
title = {{RePriv: Re-imagining Content Personalization and In-browser Privacy}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5958026},
year = {2011}
}
@article{Goldberg2011,
author = {Goldberg, Ian and Henry, Ryan},
journal = {s{\&}p},
pages = {1--17},
title = {{Formalizing Anonymous Blacklisting Systems}},
url = {http://www.google.co.kr/search?sourceid=chrome{\{}{\&}{\}}ie=UTF-8{\{}{\&}{\}}q="Formalizing+Anonymous+Blacklisting+Systems" papers3://publication/uuid/2C763625-01AA-4448-A90C-AF94E2385B23},
year = {2011}
}
@article{Guha2011,
abstract = {Popup blocking, form filling, and many other features of modern web browsers were first introduced as third-party extensions. New extensions continue to enrich browsers in unanticipated ways. However, powerful extensions require capabilities, such as cross-domain network access and local storage, which, if used improperly, pose a security risk. Several browsers try to limit extension capabilities, but an empirical survey we conducted shows that many extensions are over-privileged under existing mechanisms. This paper presents $\backslash$ibex, a new framework for authoring, analyzing, verifying, and deploying secure browser extensions. Our approach is based on using type-safe, high-level languages to program extensions against an API providing access to a variety of browser features. We propose using Data log to specify fine-grained access control and dataflow policies to limit the ways in which an extension can use this API, thus restricting its privilege over security-sensitive web content and browser resources. We formalize the semantics of policies in terms of a safety property on the execution of extensions and develop a verification methodology that allows us to statically check extensions for policy compliance. Additionally, we provide visualization tools to assist with policy analysis, and compilers to translate extension source code to either. NET byte code or JavaScript, facilitating cross-browser deployment of extensions. We evaluate our work by implementing and verifying{\{}{\~{}}{\}}$\backslash$NumExt extensions with a diverse set of features and security policies. We deploy our extensions in Internet Explorer, Chrome, Fire fox, and a new experimental HTML5 platform called C3. In so doing, we demonstrate the versatility and effectiveness of our approach.},
author = {Guha, Arjun and Fredrikson, Matthew and Livshits, Benjamin and Swamy, Nikhil},
doi = {10.1109/SP.2011.36},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
pages = {115--130},
title = {{Verified security for browser extensions}},
year = {2011}
}
@article{Taly2011,
abstract = {JavaScript is widely used to provide client-side functionality in Web applications. To provide services ranging from maps to advertisements, Web applications may incorporate untrusted JavaScript code from third parties. The trusted portion of each application may then expose an API to untrusted code, interposing a reference monitor that mediates access to security-critical resources. However, a JavaScript reference monitor can only be effective if it cannot be circumvented through programming tricks or programming language idiosyncrasies. In order to verify complete mediation of critical resources for applications of interest, we define the semantics of a restricted version of JavaScript devised by the ECMA Standards committee for isolation purposes, and develop and test an automated tool that can soundly establish that a given API cannot be circumvented or subverted. Our tool reveals a previously-undiscovered vulnerability in the widely-examined Yahoo! AD Safe filter and verifies confinement of the repaired filter and other examples from the Object-Capability literature.},
author = {Taly, Ankur and Erlingsson, {\'{U}}lfar and Mitchell, John C and Miller, Mark S and Nagra, Jasvir},
doi = {10.1109/SP.2011.39},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
keywords = {APIs,Javascript,Language-based security,Points-to analysis},
pages = {363--378},
title = {{Automated analysis of security-critical JavaScript APIs}},
year = {2011}
}
@article{Bangerter2011,
author = {Bangerter, Endre and Gullasch, David and Krenn, Stephan},
journal = {s{\&}p},
pages = {1--16},
title = {{Cache Games – Bringing Access-Based Cache Attacks on AES to Practice}},
url = {http://eprint.iacr.org/2010/594 file:///Users/sangho/Dropbox/Library.papers3/2011/Cache Games – Bringing Access-Based Cache Attacks on AES to Practice-Bangerter.pdf papers3://publication/uuid/200C8B2E-6CD1-4FF0-91CF-F836B0DD4B1C},
year = {2011}
}
@article{Parno2011,
abstract = {To protect computation, a security architecture must safeguard not only the software that performs it but also the state on which the software operates. This requires more than just preserving state confidentiality and integrity, since, e.g., software may err if its state is rolled back to a correct but stale version. For this reason, we present Memoir, the first system that fully ensures the continuity of a protected software module's state. In other words, it ensures that a module's state remains persistently and completely inviolate. A key contribution of Memoir is a technique to ensure rollback resistance without making the system vulnerable to system crashes. It does this by using a deterministic module, storing a concise summary of the module's request history in protected NVRAM, and allowing only safe request replays after crashes. Since frequent NVRAM writes are impractical on modern hardware, we present a novel way to leverage limited trusted hardware to minimize such writes. To ensure the correctness of our design, we develop formal, machine-verified proofs of safety. To demonstrate Memoir's practicality, we have built it and conducted evaluations demonstrating that it achieves reasonable performance on real hardware. Furthermore, by building three useful Memoir-protected modules that rely critically on state continuity, we demonstrate Memoir's versatility.},
author = {Parno, Bryan and Lorch, Jacob R and Douceur, John R and Mickens, James and McCune, Jonathan M},
doi = {10.1109/SP.2011.38},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
pages = {379--394},
title = {{Memoir: Practical state continuity for protected modules}},
year = {2011}
}
@article{Wimberly2011,
abstract = {Choosing the security architecture and policies for a system is a demanding task that must be informed by an understanding of user behavior. We investigate the hypothesis that adding visible security features to a system increases user confidence in the security of a system and thereby causes users to reduce how much effort they spend in other security areas. In our study, 96 volunteers each created a pair of accounts, one secured only by a password and one secured by both a password and a fingerprint reader. Our results strongly support our hypothesis -- on average. When using the fingerprint reader, users created passwords that would take one three-thousandth as long to break, thereby potentially negating the advantage two-factor authentication could have offered.},
author = {Wimberly, Hugh and Liebrock, Lorie M},
doi = {10.1109/SP.2011.35},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
keywords = {Risk compensation,Security policy,Two-factor authentication,User study},
pages = {32--46},
title = {{Using fingerprint authentication to reduce system security: An empirical study}},
year = {2011}
}
@article{Levchenko2011,
author = {Levchenko, K and Pitsillidis, A},
doi = {10.1109/SP.2011.24},
isbn = {978-0-7695-4402-1},
issn = {1081-6011},
journal = {s{\&}p},
title = {{Click trajectories: End-to-end analysis of the spam value chain}},
url = {http://ieeexplore.ieee.org/xpls/abs{\{}{\_}{\}}all.jsp?arnumber=5958044},
year = {2011}
}
@article{Owen2011,
abstract = {We describe how to combine a minimal Trusted Computing Base (TCB) with polyinstantiated and slightly augmented Commercial Off The Shelf (COTS) software programs in separate Single Level Secure (SLS) partitions to create MultiLevel Secure (MLS) applications. These MLS applications can coordinate fine grained (intra-document) Bell LaPadula (BLP) [6] separation between information at multiple security levels. The untrusted COTS programs in the SLS partitions send at-level file edits as diff transactions to the TCB. The TCB verifies that BLP semantics will be observed and then patches these transactions into its canonical representation of the file. Finally, it releases appropriately filtered versions back to each SLS partition for re-assembly into the COTS program's standard file format. Furthermore, by judiciously restricting how the user can interact with the system the multiple SLS instantiations of the COTS program can be made to appear as if they are a single MLS instantiation. We demonstrate the utility of this approach using Microsoft Word and DokuWiki.},
author = {Owen, Chris and Grove, Duncan and Newby, Tristan and Murray, Alex and North, Chris and Pope, Michael},
doi = {10.1109/SP.2011.15},
isbn = {978-1-4577-0147-4},
issn = {10816011},
journal = {s{\&}p},
keywords = {Application virtualization,Computer security,Data security,Data storage systems,File systems,Information entropy,Information security,Military computing,Multilevel systems,Software architecture},
pages = {281--296},
title = {{PRISM: Program Replication and Integration for Seamless MILS}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5958035},
year = {2011}
}
@article{Kilzer2011,
author = {Kilzer, Ann and Calandrino, Joseph A and Narayanan, Arvind and Felten, Edward W and Shmatikov, Vitaly},
journal = {s{\&}p},
pages = {1--16},
title = {{“You Might Also Like:” Privacy Risks of Collaborative Filtering}},
url = {http://www.google.co.kr/url?sa=t{\{}{\&}{\}}source=web{\{}{\&}{\}}cd=1{\{}{\&}{\}}ved=0CCkQFjAA{\{}{\&}{\}}url=http://www.cs.utexas.edu/{\{}{~}{\}}shmat/shmat{\{}{\_}{\}}oak11ymal.pdf{\{}{\&}{\}}rct=j{\{}{\&}{\}}q="You Might Also Like:" Privacy Risks of Collaborative Filtering"{\{}{\&}{\}}ei=YsV-TYHsEZGkvgPSt5DUBw{\{}{\&}{\}}usg=AFQjCNHQvp8EW},
year = {2011}
}
@article{Fong2011,
abstract = {In Face book-style Social Network Systems (FSNSs), which are a generalization of the access control model of Face book, an access control policy specifies a graph-theoretic relationship between the resource owner and resource access or that must hold in the social graph in order for access to be granted. Pseudonymous identities may collude to alter the topology of the social graph and gain access that would otherwise be forbidden. We formalize Denning's Principle of Privilege Attenuation (POPA) as a run-time property, and demonstrate that it is a necessary and sufficient condition for preventing the above form of Sybil attacks. A static policy analysis is then devised for verifying that an FSNS is POPA compliant (and thus Sybil free). The static analysis is proven to be both sound and complete. We also extend our analysis to cover a peculiar feature of FSNS, namely, what Fong et al. dubbed as Stage-I Authorization. We discuss the anomalies resulted from this extension, and point out the need to redesign Stage-I Authorization to support a rational POPA-compliance analysis.},
author = {Fong, Philip W L},
doi = {10.1109/SP.2011.16},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
keywords = {Access control,Policy analysis,Principle of Privilege Attenuation,Social Network Systems,Soundness and completeness of static analysis,Sybil attacks},
pages = {263--278},
title = {{Preventing Sybil attacks by Privilege Attenuation: A design principle for Social Network Systems}},
year = {2011}
}
@article{Vaughan2011,
abstract = {We explore the inference of expressive human-readable declassification policies as a step towards providing practical tools and techniques for strong language-based information security. Security-type systems can enforce expressive information-security policies, but can require enormous programmer effort before any security benefit is realized. To reduce the burden on the programmer, we focus on inference of expressive yet intuitive information-security policies from programs with few programmer annotations. We define a novel security policy language that can express what information a program may release, under what conditions (or, when) such release may occur, and which procedures are involved with the release (or, where in the code the release occur). We describe a dataflow analysis for precisely inferring these policies, and build a tool that instantiates this analysis for the Java programming language. We validate the policies, analysis, and our implementation by applying the tool to a collection of simple Java programs.},
author = {Vaughan, Jeffrey a. and Chong, Stephen},
doi = {10.1109/SP.2011.20},
isbn = {978-1-4577-0147-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {declassification policies,inference of security policies,information flow,language-based security},
pages = {180--195},
title = {{Inference of Expressive Declassification Policies}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5958029},
year = {2011}
}
@article{Weinberg2011,
abstract = {History sniffing attacks allow web sites to learn about users' visits to other sites. The major browsers have recently adopted a defense against the current strategies for history sniffing. In a user study with 307 participants, we demonstrate that history sniffing remains feasible via interactive techniques which are not covered by the defense. While these techniques are slower and cannot hope to learn as much about users' browsing history, we see no practical way to defend against them.},
author = {Weinberg, Zachary and Chen, Eric Y and Jayaraman, Pavithra Ramesh and Jackson, Collin},
doi = {10.1109/SP.2011.23},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
pages = {147--161},
title = {{I still know what you visited last summer: Leaking browsing history via user interaction and side channel attacks}},
year = {2011}
}
@article{Sturton2011,
abstract = {In previous work Hicks et al. proposed a method called Unused Circuit Identification (UCI) for detecting malicious backdoors hidden in circuits at design time. The UCI algorithm essentially looks for portions of the circuit that go unused during design-time testing and flags them as potentially malicious. In this paper we construct circuits that have malicious behavior, but that would evade detection by the UCI algorithm and still pass design-time test cases. To enable our search for such circuits, we define one class of malicious circuits and perform a bounded exhaustive enumeration of all circuits in that class. Our approach is simple and straight forward, yet it proves to be effective at finding circuits that can thwart UCI. We use the results of our search to construct a practical attack on an open-source processor. Our malicious backdoor allows any user-level program running on the processor to enter supervisor mode through the use of a secret {\{}{\&}{\}}{\{}{\#}{\}}x0E2; knock. We close with a discussion on what we see as a major challenge facing any future design-time malicious hardware detection scheme: identifying a sufficient class of malicious circuits to defend against.},
author = {Sturton, Cynthia and Hicks, Matthew and Wagner, David and King, Samuel T},
doi = {10.1109/SP.2011.32},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
keywords = {Attack,Hardware,Security},
pages = {64--77},
title = {{Defeating UCI: Building stealthy and malicious hardware}},
year = {2011}
}
@article{Becher2011,
abstract = {We are currently moving from the Internet society to a mobile society where more and more access to information is done by previously dumb phones. For example, the number of mobile phones using a full blown OS has risen to nearly200{\{}{\&}{\}}{\{}{\#}{\}}x025; from Q3/2009 to Q3/2010. As a result, mobile security is no longer immanent, but imperative. This survey paper provides a concise overview of mobile network security, attack vectors using the back end system and the web browser, but also the hardware layer and the user as attack enabler. We show differences and similarities between "normal" security and mobile security, and draw conclusions for further research opportunities in this area.},
author = {Becher, Michael and Freiling, Felix C and Hoffmann, Johannes and Holz, Thorsten and Uellenbeck, Sebastian and Wolf, Christopher},
doi = {10.1109/SP.2011.29},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
keywords = {Mobile security,Smartphones,Survey},
number = {March 2010},
pages = {96--111},
pmid = {17932744},
title = {{Mobile security catching up? Revealing the nuts and bolts of the security of mobile devices}},
year = {2011}
}
@article{Zhang2011,
abstract = {We present the first Internet architecture designed to provide route control, failure isolation, and explicit trust information for end-to-end communications. SCION separates ASes into groups of independent routing sub-planes, called trust domains, which then interconnect to form complete routes. Trust domains provide natural isolation of routing failures and human misconfiguration, give endpoints strong control for both inbound and outbound traffic, provide meaningful and enforceable trust, and enable scalable routing updates with high path freshness. As a result, our architecture provides strong resilience and security properties as an intrinsic consequence of good design principles, avoiding piecemeal add-on protocols as security patches. Meanwhile, SCION only assumes that a few top-tier ISPs in the trust domain are trusted for providing reliable end-to-end communications, thus achieving a small Trusted Computing Base. Both our security analysis and evaluation results show that SCION naturally prevents numerous attacks and provides a high level of resilience, scalability, control, and isolation.},
author = {Zhang, Xin and Hsiao, Hsu Chun and Hasker, Geoffrey and Chan, Haowen and Perrig, Adrian and Andersen, David G},
doi = {10.1109/SP.2011.45},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
pages = {212--227},
title = {{SCION: Scalability, control, and isolation on next-generation networks}},
year = {2011}
}
@article{Zhang2011a,
abstract = {Security is a major barrier to enterprise adoption of cloud computing. Physical co-residency with other tenants poses a particular risk, due to pervasive virtualization in the cloud. Recent research has shown how side channels in shared hardware may enable attackers to exfiltrate sensitive data across virtual machines (VMs). In view of such risks, cloud providers may promise physically isolated resources to select tenants, but a challenge remains: Tenants still need to be able to verify physical isolation of their VMs. We introduce Home Alone, a system that lets a tenant verify its VMs' exclusive use of a physical machine. The key idea in Home Alone is to invert the usual application of side channels. Rather than exploiting a side channel as a vector of attack, Home Alone uses a side-channel (in the L2 memory cache) as a novel, defensive detection tool. By analyzing cache usage during periods in which "friendly" VMs coordinate to avoid portions of the cache, a tenant using Home Alone can detect the activity of a co-resident "foe" VM. Key technical contributions of Home Alone include classification techniques to analyze cache usage and guest operating system kernel modifications that minimize the performance impact of friendly VMs sidestepping monitored cache portions. Home Alone requires no modification of existing hyper visors and no special action or cooperation by the cloud provider.},
author = {Zhang, Yinqian and Juels, Ari and Oprea, Alina and Reiter, Michael K},
doi = {10.1109/SP.2011.31},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
keywords = {Co-residency detection,Side channel},
pages = {313--328},
title = {{HomeAlone: Co-residency detection in the cloud via side-channel analysis}},
url = {http://ieeexplore.ieee.org/ielx5/5955408/5958008/05958037.pdf?tp={\{}{\&}{\}}arnumber=5958037{\{}{\&}{\}}isnumber=5958008$\backslash$nhttp://ieeexplore.ieee.org/xpls/abs{\{}{\_}{\}}all.jsp?arnumber=5958037{\{}{\&}{\}}tag=1},
year = {2011}
}
@article{Wang2011,
abstract = {Web applications increasingly integrate third-party services. The integration introduces new security challenges due to the complexity for an application to coordinate its internal states with those of the component services and the web client across the Internet. In this paper, we study the security implications of this problem to merchant websites that accept payments through third-party cashiers (e.g., PayPal, Amazon Payments and Google Checkout), which we refer to as Cashier-as-a-Service or CaaS. We found that leading merchant applications (e.g., NopCommerce and Interspire), popular online stores (e.g., Buy.com and JR.com) and a prestigious CaaS provider (Amazon Payments) all contain serious logic flaws that can be exploited to cause inconsistencies between the states of the CaaS and the merchant. As a result, a malicious shopper can purchase an item at an arbitrarily low price, shop for free after paying for one item, or even avoid payment. We reported our findings to the affected parties. They either updated their vulnerable software or continued to work on the fixes with high priorities. We further studied the complexity in finding this type of logic flaws in typical CaaS-based checkout systems, and gained a preliminary understanding of the effort that needs to be made to improve the security assurance of such systems during their development and testing processes.},
author = {Wang, Rui and Chen, Shuo and Wang, XiaoFeng and Qadeer, Shaz},
doi = {10.1109/SP.2011.26},
isbn = {978-1-4577-0147-4},
issn = {10816011},
journal = {s{\&}p},
keywords = {Cashier-as-a-Service,e-Commerce security,logic bug,program verification,web API},
pages = {465--480},
title = {{How to Shop for Free Online -- Security Analysis of Cashier-as-a-Service Based Web Stores}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5958046},
year = {2011}
}
@article{Goldberg2011a,
author = {Goldberg, Ian and Henry, Ryan},
journal = {s{\&}p},
pages = {1--15},
title = {{Extending Nymble-like Systems}},
url = {http://www.google.co.kr/search?sourceid=chrome{\{}{\&}{\}}ie=UTF-8{\{}{\&}{\}}q="Extending+Nymble-like+Systems" papers3://publication/uuid/BD57DB6C-A876-429A-B960-A67538455524},
year = {2011}
}
@article{Shokri2011,
abstract = {It is a well-known fact that the progress of personal communication devices leads to serious concerns about privacy in general, and location privacy in particular. As a response to these issues, a number of Location-Privacy Protection Mechanisms (LPPMs) have been proposed during the last decade. However, their assessment and comparison remains problematic because of the absence of a systematic method to quantify them. In particular, the assumptions about the attacker's model tend to be incomplete, with the risk of a possibly wrong estimation of the users' location privacy. In this paper, we address these issues by providing a formal framework for the analysis of LPPMs, it captures, in particular, the prior information that might be available to the attacker, and various attacks that he can perform. The privacy of users and the success of the adversary in his location-inference attacks are two sides of the same coin. We revise location privacy by giving a simple, yet comprehensive, model to formulate all types of location-information disclosure attacks. Thus, by formalizing the adversary's performance, we propose and justify the right metric to quantify location privacy. We clarify the difference between three aspects of the adversary's inference attacks, namely their accuracy, certainty, and correctness. We show that correctness determines the privacy of users. In other words, the expected estimation error of the adversary is the metric of users' location privacy. We rely on well-established statistical methods to formalize and implement the attacks in a tool: the Location-Privacy Meter that measures the location privacy of mobile users, given various LPPMs. In addition to evaluating some example LPPMs, by using our tool, we assess the appropriateness of some popular metrics for location privacy: entropy and k-anonymity. The results show a lack of satisfactory correlation between these two metrics and the success of the adversary in inferring the users' actual locations.},
author = {Shokri, Reza and Theodorakopoulos, George and {Le Boudec}, Jean Yves and Hubaux, Jean Pierre},
doi = {10.1109/SP.2011.18},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
keywords = {Evaluation framework,Location privacy,Location traces,Location-Privacy Meter,Quantifying metric},
pages = {247--262},
title = {{Quantifying location privacy}},
year = {2011}
}
@article{Nanevski2011,
abstract = {We present Relational Hoare Type Theory (RHTT), a novel language and verification system capable of expressing and verifying rich information flow and access control policies via dependent types. We show that a number of security policies which have been formalized separately in the literature can all be expressed in RHTT using only standard type-theoretic constructions such as monads, higher-order functions, abstract types, abstract predicates, and modules. Example security policies include conditional declassification, information erasure, and state-dependent information flow and access control. RHTT can reason about such policies in the presence of dynamic memory allocation, deallocation, pointer aliasing and arithmetic. The system, theorems and examples have all been formalized in Coq.},
author = {Nanevski, Aleksandar and Banerjee, Anindya and Garg, Deepak},
doi = {10.1109/SP.2011.12},
isbn = {978-1-4577-0147-4},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Access Control,Information Flow,Type Theory},
number = {c},
pages = {165--179},
title = {{Verification of Information Flow and Access Control Policies with Dependent Types}},
year = {2011}
}
@article{Eggert2011,
abstract = {The paper considers several definitions of information flow security for intransitive policies from the point of view of the complexity of verifying whether a finite-state system is secure. The results are as follows. Checking (i) P-security (Goguen and Meseguer), (ii) IP-security (Haigh and Young), and (iii) TA-security (van der Meyden) are all in PTIME, while checking TO-security (van der Meyden) is undecidable. The most important ingredients in the proofs of the PTIME upper bounds are new characterizations of the respective security notions, which also enable the algorithms to return simple counterexamples demonstrating insecurity. Our results for IP-security improve a previous doubly exponential bound of Hadj-Alouane et al.},
author = {Eggert, Sebastian and {Van Der Meyden}, Ron and Schnoor, Henning and Wilke, Thomas},
doi = {10.1109/SP.2011.30},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
keywords = {Information flow,Noninterference,Verification},
pages = {196--211},
title = {{The complexity of intransitive noninterference}},
year = {2011}
}
@article{Armknecht2011,
abstract = {Physical attacks against cryptographic devices typically take advantage of information leakage (e.g., side-channels attacks) or erroneous computations (e.g., fault injection attacks). Preventing or detecting these attacks has become a challenging task in modern cryptographic research. In this context intrinsic physical properties of integrated circuits, such as Physical(ly) Unclonable Functions{\{}{\~{}}{\}}(PUFs), can be used to complement classical cryptographic constructions, and to enhance the security of cryptographic devices. PUFs have recently been proposed for various applications, including anti-counterfeiting schemes, key generation algorithms, and in the design of block ciphers. However, currently only rudimentary security models for PUFs exist, limiting the confidence in the security claims of PUF-based security primitives. A useful model should at the same time (i) define the security properties of PUFs abstractly and naturally, allowing to design and formally analyze PUF-based security solutions, and (ii) provide practical quantification tools allowing engineers to evaluate PUF instantiations. In this paper, we present a formal foundation for security primitives based on PUFs. Our approach requires as little as possible from the physics and focuses more on the main properties at the heart of most published works on PUFs: robustness (generation of stable answers), unclonability (not provided by algorithmic solutions), and unpredictability. We first formally define these properties and then show that they can be achieved by previously introduced PUF instantiations. We stress that such a consolidating work allows for a meaningful security analysis of security primitives taking advantage of physical properties, becoming increasingly important in the development of the next generation secure information systems.},
author = {Armknecht, Frederik and Maes, Roel and Sadeghi, Ahmad Reza and Standaert, Fran{\c{c}}ois Xavier and Wachsmann, Christian},
doi = {10.1109/SP.2011.10},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
keywords = {Formal security model,Physically Unclonable Function (PUF),Robustness,Unclonability,Unpredictability},
pages = {397--412},
title = {{A formal foundation for the security features of physical functions}},
year = {2011}
}
@article{Thomas2011,
abstract = {On the heels of the widespread adoption of web services such as social networks and URL shorteners, scams, phishing, and malware have become regular threats. Despite extensive research, email-based spam filtering techniques generally fall short for protecting other web services. To better address this need, we present Monarch, a real-time system that crawls URLs as they are submitted to web services and determines whether the URLs direct to spam. We evaluate the viability of Monarch and the fundamental challenges that arise due to the diversity of web service spam. We show that Monarch can provide accurate, real-time protection, but that the underlying characteristics of spam do not generalize across web services. In particular, we find that spam targeting email qualitatively differs in significant ways from spam campaigns targeting Twitter. We explore the distinctions between email and Twitter spam, including the abuse of public web hosting and redirector services. Finally, we demonstrate Monarch's scalability, showing our system could protect a service such as Twitter -- which needs to process 15 million URLs/day -- for a bit under {\{}{\$}{\}}800/day.},
author = {Thomas, Kurt and Grier, Chris and Ma, Justin and Paxson, Vern and Song, Dawn},
doi = {10.1109/SP.2011.25},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
pages = {447--462},
title = {{Design and Evaluation of a Real-Time URL spam Filtering Service}},
year = {2011}
}
@article{Bursztein2011,
abstract = {CAPTCHAs, which are automated tests intended to distinguish humans from programs, are used on many web sites to prevent bot-based account creation and spam. To avoid imposing undue user friction, CAPTCHAs must be easy for humans and difficult for machines. However, the scientific basis for successful CAPTCHA design is still emerging. This paper examines the widely used class of audio CAPTCHAs based on distorting non-continuous speech with certain classes of noise and demonstrates that virtually all current schemes, including ones from Microsoft, Yahoo, and eBay, are easily broken. More generally, we describe a set of fundamental techniques, packaged together in our Decaptcha system, that effectively defeat a wide class of audio CAPTCHAs based on non-continuous speech. Decaptcha's performance on actual observed and synthetic CAPTCHAs indicates that such speech CAPTCHAs are inherently weak and, because of the importance of audio for various classes of users, alternative audio CAPTCHAs must be developed.},
author = {Bursztein, Elie and Beauxis, Romain and Paskov, Hristo and Perito, Daniele and Fabry, Celine and Mitchell, John},
doi = {10.1109/SP.2011.14},
isbn = {978-1-4577-0147-4},
issn = {10816011},
journal = {s{\&}p},
pages = {19--31},
title = {{The Failure of Noise-Based Non-continuous Audio Captchas}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5958019},
year = {2011}
}
@article{Jana2011,
abstract = {TxBox is a new system for sand boxing untrusted applications. It speculatively executes the application in a system transaction, allowing security checks to be parallelized and yielding significant performance gains for techniques such as on-access anti-virus scanning. TxBox is not vulnerable to TOCTTOU attacks and incorrect mirroring of kernel state. Furthermore, TxBox supports automatic recovery: if a violation is detected, the sand boxed program is terminated and all of its effects on the host are rolled back. This enables effective enforcement of security policies that span multiple system calls.},
author = {Jana, Suman and Porter, Donald E and Shmatikov, Vitaly},
doi = {10.1109/SP.2011.33},
isbn = {978-1-4577-0147-4},
issn = {10816011},
journal = {s{\&}p},
keywords = {sandbox,speculative execution,transaction},
pages = {329--344},
title = {{TxBox: Building Secure, Efficient Sandboxes with System Transactions}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5958038},
year = {2011}
}
@article{Dolan-Gavitt2011,
abstract = {Introspection has featured prominently in many recent security solutions, such as virtual machine-based intrusion detection, forensic memory analysis, and low-artifact malware analysis. Widespread adoption of these approaches, however, has been hampered by the semantic gap: in order to extract meaningful information about the current state of a virtual machine, detailed knowledge of the guest operating system's inner workings is required. In this paper, we present a novel approach for automatically creating introspection tools for security applications with minimal human effort. By analyzing dynamic traces of small, in-guest programs that compute the desired introspection information, we can produce new programs that retrieve the same information from outside the guest virtual machine. We demonstrate the efficacy of our techniques by automatically generating 17 programs that retrieve security information across 3 different operating systems, and show that their functionality is unaffected by the compromise of the guest system. Our technique allows introspection tools to be effortlessly generated for multiple platforms, and enables the development of rich introspection-based security applications.},
author = {Dolan-Gavitt, Brendan and Leek, Tim and Zhivich, Michael and Giffin, Jonathon and Lee, Wenke},
doi = {10.1109/SP.2011.11},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
pages = {297--312},
title = {{Virtuoso: Narrowing the semantic gap in virtual machine introspection}},
year = {2011}
}
@article{Kashyap2011,
abstract = {Secure information flow guarantees the secrecy and integrity of data, preventing an attacker from learning secret information (secrecy) or injecting untrusted information (integrity). Covert channels can be used to subvert these security guarantees, for example, timing and termination channels can, either intentionally or inadvertently, violate these guarantees by modifying the timing or termination behavior of a program based on secret or untrusted data. Attacks using these covert channels have been published and are known to work in practice{\{}{\&}{\}}{\{}{\#}{\}}x0E2; as techniques to prevent non-covert channels are becoming increasingly practical, covert channels are likely to become even more attractive for attackers to exploit. The goal of this paper is to understand the subtleties of timing and termination-sensitive noninterference, explore the space of possible strategies for enforcing noninterference guarantees, and formalize the exact guarantees that these strategies can enforce. As a result of this effort we create a novel strategy that provides stronger security guarantees than existing work, and we clarify claims in existing work about what guarantees can be made.},
author = {Kashyap, Vineeth and Wiedermann, Ben and Hardekopf, Ben},
doi = {10.1109/SP.2011.19},
isbn = {978-1-4577-0147-4},
issn = {1081-6011},
journal = {s{\&}p},
pages = {413--428},
title = {{Timing- and Termination-Sensitive Secure Information Flow: Exploring a New Approach}},
year = {2011}
}
@article{Johnson2011,
abstract = {A security analyst often needs to understand two runs of the same program that exhibit a difference in program state or output. This is important, for example, for vulnerability analysis, as well as for analyzing a malware program that features different behaviors when run in different environments. In this paper we propose a differential slicing approach that automates the analysis of such execution differences. Differential slicing outputs a causal difference graph that captures the input differences that triggered the observed difference and the causal path of differences that led from those input differences to the observed difference. The analyst uses the graph to quickly understand the observed difference. We implement differential slicing and evaluate it on the analysis of 11 real-world vulnerabilities and 2 malware samples with environment-dependent behaviors. We also evaluate it in an informal user study with two vulnerability analysts. Our results show that differential slicing successfully identifies the input differences that caused the observed difference and that the causal difference graph significantly reduces the amount of time and effort required for an analyst to understand the observed difference.},
author = {Johnson, Noah M and Juan, Caballero and Chen, Kevin Zhijie and McCamant, Stephen and Poosankam, Pongsin and Reynaud, Daniel and Song, Dawn},
doi = {10.1109/SP.2011.41},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
pages = {347--362},
title = {{Differential slicing: Identifying causal execution differences for security applications}},
year = {2011}
}
@article{Bursztein2011a,
abstract = {We present a generic tool, Kartograph, that lifts the fog of war in online real-time strategy games by snooping on the memory used by the game. Kartograph is passive and cannot be detected remotely. Motivated by these passive attacks, we present secure protocols for distributing game state among players so that each client only has data it is allowed to see. Our system, Open Conflict, runs real-time games with distributed state. To support our claim that Open Conflict is sufficiently fast for real-time strategy games, we show the results of an extensive study of 1000 replays of Star craft II games between expert players. At the peak of a typical game, Open Conflict needs only 22 milliseconds on one CPU core each time state is synchronized.},
author = {Bursztein, Elie and Hamburg, Mike and Lagarenne, Jocelyn and Boneh, Dan},
doi = {10.1109/SP.2011.28},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
keywords = {Map hacks,Multi-player games},
pages = {506--520},
title = {{OpenConflict: Preventing real time map hacks in online games}},
year = {2011}
}
@article{Waksman2011,
abstract = {Hardware components can contain hidden backdoors, which can be enabled with catastrophic effects or for ill-gotten profit. These backdoors can be inserted by a malicious insider on the design team or a third-party IP provider. In this paper, we propose techniques that allow us to build trustworthy hardware systems from components designed by untrusted designers or procured from untrusted third-party IP providers. We present the first solution for disabling digital, design-level hardware backdoors. The principle is that rather than try to discover the malicious logic in the design -- an extremely hard problem -- we make the backdoor design problem itself intractable to the attacker. The key idea is to scramble inputs that are supplied to the hardware units at runtime, making it infeasible for malicious components to acquire the information they need to perform malicious actions. We show that the proposed techniques cover the attack space of deterministic, digital HDL backdoors, provide probabilistic security guarantees, and can be applied to a wide variety of hardware components. Our evaluation with the SPEC 2006 benchmarks shows negligible performance loss (less than 1{\{}{\&}{\}}{\{}{\#}{\}}x025; on average) and that our techniques can be integrated into contemporary microprocessor designs.},
author = {Waksman, Adam and Sethumadhavan, Simha},
doi = {10.1109/SP.2011.27},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
keywords = {Backdoors,Hardware,Performance,Security,Triggers},
pages = {49--63},
title = {{Silencing Hardware Backdoors}},
year = {2011}
}
@article{Duong2011,
abstract = {This paper discusses how cryptography is misused in the security design of a large part of the Web. Our focus is on ASP.NET, the web application framework developed by Microsoft that powers 25{\{}{\&}{\}}{\{}{\#}{\}}x025; of all Internet web sites. We show that attackers can abuse multiple cryptographic design flaws to compromise ASP.NET web applications. We describe practical and highly efficient attacks that allow attackers to steal cryptographic secret keys and forge authentication tokens to access sensitive information. The attacks combine decryption oracles, unauthenticated encryptions, and the reuse of keys for different encryption purposes. Finally, we give some reasons why cryptography is often misused in web technologies, and recommend steps to avoid these mistakes.},
author = {Duong, Thai and Rizzo, Juliano},
doi = {10.1109/SP.2011.42},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
keywords = {Application security,Cryptography,Decryption oracle attack,Unauthenticated encryption,Web security},
pages = {481--489},
title = {{Cryptography in the web: The case of cryptographic design flaws in ASP.NET}},
year = {2011}
}
@article{Kusters2011,
abstract = {In this paper, we present new insights into central properties of voting systems, namely verifiability, privacy, and coercion-resistance. We demonstrate that the combination of the two forms of verifiability considered in the literature -- individual and universal verifiability -- are, unlike commonly believed, insufficient to guarantee overall verifiability. We also demonstrate that the relationship between coercion-resistance and privacy is more subtle than suggested in the literature. Our findings are partly based on a case study of prominent voting systems, Three Ballot and VAV, for which, among others, we show that, unlike commonly believed, they do not provide any reasonable level of verifiability, even though they satisfy individual and universal verifiability. Also, we show that the original variants of Three Ballot and VAV provide a better level of coercion-resistance than of privacy.},
author = {K{\"{u}}sters, Ralf and Truderung, Tomasz and Vogt, Andreas},
doi = {10.1109/SP.2011.21},
isbn = {9780769544021},
issn = {10816011},
journal = {s{\&}p},
keywords = {Coercion-resistance,Privacy,Protocol analysis,Verifiability,Voting},
pages = {538--553},
title = {{Verifiability, privacy, and coercion-resistance: New insights from a case study}},
year = {2011}
}
@article{Lewko2010,
abstract = {In this work, we design a method for creating public key broadcast encryption systems. Our main technical innovation is based on a new "two equation" technique for revoking users. This technique results in two key contributions: First, our new scheme has ciphertext size overhead {\{}{\$}{\}}O(r){\{}{\$}{\}}, where {\{}{\$}{\}}r{\{}{\$}{\}} is the number of revoked users, and the size of public and private keys is only a emph{\{}{\{}{\}}constant{\{}{\}}{\}} number of group elements from an elliptic-curve group of prime order. In addition, the public key allows us to encrypt to an unbounded number of users. Our system is the first to achieve such parameters. We give two versions of our scheme: a simpler version which we prove to be selectively secure in the standard model under a new, but non-interactive assumption, and another version that employs the new dual system encryption technique of Waters to obtain adaptive security under the d-BDH and decisional Linear assumptions. Second, we show that our techniques can be used to realize Attribute-Based Encryption (ABE) systems with non-monotonic access formulas, where our key storage is significantly more efficient than previous solutions. This result is also proven selectively secure in the standard model under our new non-interactive assumption.},
author = {Lewko, Allison and Sanais, Amit and Waters, Brent},
doi = {10.1109/SP.2010.23},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
pages = {273--285},
title = {{Revocation systems with very small private keys}},
year = {2010}
}
@article{Franklin2010,
abstract = {The security of systems such as operating systems, hypervisors, and web browsers depend critically on reference monitors to correctly enforce their desired security policy in the presence of adversaries. Recent progress in developing reference monitors with small code size and narrow interfaces has made automated formal verification of reference monitors a more tractable goal. However, a significant remaining factor for the complexity of automated verification is the size of the data structures (e.g., access control matrices) over which the programs operate. This paper develops a parametric verification technique that scales even when reference monitors and adversaries operate over unbounded, but finite data structures. Specifically, we develop a parametric guarded command language for modeling reference monitors and adversaries. We also present a parametric temporal specification logic for expressing security policies that the monitor is expected to enforce. The central technical results of the paper are a set of small model theorems. These theorems state that in order to verify that a policy is enforced by a reference monitor with an arbitrarily large data structure, it is sufficient to model check the monitor with just one entry in its data structure. We apply our methodology to verify the designs of two hypervisors, SecVisor and the sHype mandatory-access-control extension to Xen. Our approach is able to prove that sHype and a variant of the original SecVisor design correctly enforces the expected security properties in the presence of powerful adversaries.},
author = {Franklin, Jason and Chaki, Sagar and Datta, Anupam and Seshadri, Arvind},
doi = {10.1109/SP.2010.29},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
pages = {365--379},
title = {{Scalable parametric verification of secure systems: How to verify reference monitors without worrying about data structure size}},
year = {2010}
}
@article{Devriese2010,
abstract = {A program is defined to be noninterferent if its outputs cannot be influenced by inputs at a higher security level than their own. Various researchers have demonstrated how this property (or closely related properties) can be achieved through information flow analysis, using either a static analysis (with a type system or otherwise), or using a dynamic monitoring system. We propose an alternative approach, based on a technique we call secure multi-execution. The main idea is to execute a program multiple times, once for each security level, using special rules for I/O operations. Outputs are only produced in the execution linked to their security level. Inputs are replaced by default inputs except in executions linked to their security level or higher. Input side effects are supported by making higher-security-level executions reuse inputs obtained in lower-security-level threads. We show that this approach is interesting from both a theoretical and practical viewpoint. Theoretically, we prove for a simple deterministic language with I/O operations, that this approach guarantees complete soundness (even for the timing and termination covert channels), as well as good precision (identical I/O for terminating runs of termination-sensitively noninterferent programs). On the practical side, we present an experiment implementing secure multi-execution in the mainstream Spidermonkey Javascript engine, exploiting parallelism on a current multi-core computer. Benchmark results of execution time and memory for the Google Chrome v8 Benchmark suite show that the approach is practical for a mainstream browser setting. Certain programs are even executed faster under secure multi-execution than under the standard execution. We discuss challenges and propose possible solutions for implementing the technique in a real browser, in particular handling the DOM tree and browser callback functions. Finally, we discuss how secure multi-execution can be extended to handle language feature-$\backslash$n-$\backslash$ns like exceptions, concurrency or nondeterminism.},
author = {Devriese, Dominique and Piessens, Frank},
doi = {10.1109/SP.2010.15},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
pages = {109--124},
title = {{Noninterference through secure multi-execution}},
year = {2010}
}
@article{Waksman2010,
abstract = {Most security mechanisms proposed to date unquestioningly place trust in microprocessor hardware. This trust, however, is misplaced and dangerous because microprocessors are vulnerable to insider attacks that can catastrophically compromise security, integrity and privacy of computer systems. In this paper, we describe several methods to strengthen the fundamental assumption about trust in microprocessors. By employing practical, lightweight attack detectors within a microprocessor, we show that it is possible to protect against malicious logic embedded in microprocessor hardware. We propose and evaluate two area-efficient hardware methods - TrustNet and DataWatch - that detect attacks on microprocessor hardware by knowledgeable, malicious insiders. Our mechanisms leverage the fact that multiple components within a microprocessor (e.g., fetch, decode pipeline stage etc.) must necessarily coordinate and communicate to execute even simple instructions, and that any attack on a microprocessor must cause erroneous communications between micro architectural subcomponents used to build a processor. A key aspect of our solution is that TrustNet and DataWatch are themselves highly resilient to corruption. We demonstrate that under realistic assumptions, our solutions can protect pipelines and on-chip cache hierarchies at negligible area cost and with no performance impact. Combining TrustNet and DataWatch with prior work on fault detection has the potential to provide complete coverage against a large class of microprocessor attacks.},
author = {Waksman, Adam and Sethumadhavan, Simha},
doi = {10.1109/SP.2010.19},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
keywords = {Backdoors,Hardware security,Microprocessors,Security based on causal structure and division of},
pages = {173--188},
title = {{Tamper evident microprocessors}},
year = {2010}
}
@article{Parno2010,
abstract = {Trusting a computer for a security-sensitive task (such as checking email or banking online) requires the user to know something about the computer's state. We examine research on securely capturing a computer's state, and consider the utility of this information both for improving security on the local computer (e.g., to convince the user that her computer is not infected with malware) and for communicating a remote computer's state (e.g., to enable the user to check that a web server will adequately protect her data). Although the recent "Trusted Computing" initiative has drawn both positive and negative attention to this area, we consider the older and broader topic of bootstrapping trust in a computer. We cover issues ranging from the wide collection of secure hardware that can serve as a foundation for trust, to the usability issues that arise when trying to convey computer state information to humans. This approach unifies disparate research efforts and highlights opportunities for additional work that can guide real-world improvements in computer security.},
author = {Parno, Bryan and McCune, Jonathan M and Perrig, Adrian},
doi = {10.1109/SP.2010.32},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
pages = {414--429},
title = {{Bootstrapping trust in commodity computers}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5504802},
year = {2010}
}
@article{Yao2010,
abstract = {To address the increasing demand for wireless bandwidth, cognitive radio networks (CRNs) have been proposed to increase the efficiency of channel utilization; they enable the sharing of channels among secondary (unlicensed) and primary (licensed) users on a non-interference basis. A secondary user in a CRN should constantly monitor for the presence of a primary user's signal to avoid interfering with the primary user. However, to gain unfair share of radio channels, an attacker (e.g., a selfish secondary user) may mimic a primary user's signal to evict other secondary users. Therefore, a secure primary user detection method that can distinguish a primary user's signal from an attacker's signal is needed. A unique challenge in addressing this problem is that Federal Communications Commission (FCC) prohibits any modification to primary users. Consequently, existing cryptographic techniques cannot be used directly. In this paper, we develop a novel approach for authenticating primary users' signals in CRNs, which conforms to FCC's requirement. Our approach integrates cryptographic signatures and wireless link signatures (derived from physical radio channel characteristics) to enable primary user detection in the presence of attackers. Essential to our approach is a {\{}{\{}{\}}em helper node{\{}{\}}{\}} placed physically close to a primary user. The helper node serves as a "bridge" to enable a secondary user to verify cryptographic signatures carried by the helper node's signals and then obtain the helper node's authentic link signatures to verify the primary user's signals. A key contribution in our paper is a novel physical layer authentication technique that enables the helper node to authenticate signals from its associated primary user. Unlike previous techniques for link signatures, our approach explores the geographical proximity of the helper node to the primary user, and thus does not require any training process.},
author = {Yao, Liu and Peng, Ning and Huaiyu, Dai},
doi = {10.1109/SP.2010.24},
isbn = {1081-6011},
journal = {s{\&}p},
keywords = {Bandwidth,Cognitive radio,Computer security,Computer vision,Cryptography,FCC,Monitoring,Signal processing,Software radio,TV,cognitive radio networks,link signatures,primary user detection},
pages = {286--301},
title = {{Authenticating Primary Users' Signals in Cognitive Radio Networks via Integrated Cryptographic and Wireless Link Signatures}},
url = {http://ieeexplore.ieee.org/ielx5/5504620/5504699/05504794.pdf?tp={\{}{\&}{\}}arnumber=5504794{\{}{\&}{\}}isnumber=5504699},
year = {2010}
}
@article{Sommer2010,
author = {Sommer, Robin and Paxson, Vern},
journal = {s{\&}p},
title = {{Outside the Closed World: On Using Machine Learning for Network Intrusion}},
year = {2010}
}
@article{Wang2010,
abstract = {Fuzz testing has proven successful in finding security vulnerabilities in large programs. However, traditional fuzz testing tools have a well-known common drawback: they are ineffective if most generated malformed inputs are rejected in the early stage of program running, especially when target programs employ checksum mechanisms to verify the integrity of inputs. In this paper, we present TaintScope, an automatic fuzzing system using dynamic taint analysis and symbolic execution techniques, to tackle the above problem. TaintScope has several novel contributions: 1) TaintScope is the first checksum-aware fuzzing tool to the best of our knowledge. It can identify checksum fields in input instances, accurately locate checksum-based integrity checks by using branch profiling techniques, and bypass such checks via control flow alteration. 2) TaintScope is a directed fuzzing tool working at X86 binary level (on both Linux and Window). Based on fine-grained dynamic taint tracing, TaintScope identifies which bytes in a well-formed input are used in security-sensitive operations (e.g., invoking system/library calls) and then focuses on modifying such bytes. Thus, generated inputs are more likely to trigger potential vulnerabilities. 3) TaintScope is fully automatic, from detecting checksum, directed fuzzing, to repairing crashed samples. It can fix checksum values in generated inputs using combined concrete and symbolic execution techniques. We evaluate TaintScope on a number of large real-world applications. Experimental results show that TaintScope can accurately locate the checksum checks in programs and dramatically improve the effectiveness of fuzz testing. TaintScope has already found 27 previously unknown vulnerabilities in several widely used applications, including Adobe Acrobat, Google Picasa, Microsoft Paint, and ImageMagick. Most of these severe vulnerabilities have been confirmed by Secunia and oCERT, and assigned CVE identifiers (such as CVE-2009-1882, CVE-200-$\backslash$n-$\backslash$n9-2688). Corresponding patches from vendors are released or in progress based on our reports.},
author = {Wang, Tielei and Wei, Tao and Gu, Guofei and Zou, Wei},
doi = {10.1109/SP.2010.37},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
keywords = {Dynamic taint analysis,Fuzzirig,Symbolic execution},
pages = {497--512},
title = {{TaintScope: A checksum-aware directed fuzzing tool for automatic software vulnerability detection}},
year = {2010}
}
@article{Maffeis2010,
abstract = {A growing number of current web sites combine active content (applications) from untrusted sources, as in so-called mashups. The object-capability model provides an appealing approach for isolating untrusted content: if separate applications are provided disjoint capabilities, a sound object-capability framework should prevent untrusted applications from interfering with each other, without preventing interaction with the user or the hosting page. In developing language-based foundations for isolation proofs based on object-capability concepts, we identify a more general notion of authority safety that also implies resource isolation. After proving that capability safety implies authority safety, we show the applicability of our framework for a specific class of mashups. In addition to proving that a JavaScript subset based on Google Caja is capability safe, we prove that a more expressive subset of JavaScript is authority safe, even though it is not based on the object-capability model.},
author = {Maffeis, Sergio and Mitchell, John C and Taly, Ankur},
doi = {10.1109/SP.2010.16},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
keywords = {Capabilities,JavaScript,Language-based security,Operational semantics},
pages = {125--140},
title = {{Object capabilities and isolation of untrusted web applications}},
year = {2010}
}
@article{Hamadou2010,
abstract = {Belief and vulnerability have been proposed recently to quantify information flow in security systems. Both concepts stand as alternatives to the traditional approaches founded on Shannon entropy and mutual information, which were shown to provide inadequate security guarantees. In this paper we unify the two concepts in one model so as to cope with (potentially inaccurate) attackers' extra knowledge. To this end we propose a new metric based on vulnerability that takes into account the adversary's beliefs.},
author = {Hamadou, Sardaouna and Sassone, Vladimiro and Palamidessi, Catuscia},
doi = {10.1109/SP.2010.13},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
keywords = {Accuracy,Information flow,Information hiding,Quantitative and probabilistic models,Security,Uncertainty},
pages = {79--92},
title = {{Reconciling belief and vulnerability in information flow}},
year = {2010}
}
@article{Wondracek2010,
abstract = {Social networking sites such as Facebook, LinkedIn, and Xing have been reporting exponential growth rates and have millions of registered users. In this paper, we introduce a novel de-anonymization attack that exploits group membership information that is available on social networking sites. More precisely, we show that information about the group memberships of a user (i.e., the groups of a social network to which a user belongs) is sufficient to uniquely identify this person, or, at least, to significantly reduce the set of possible candidates. That is, rather than tracking a user's browser as with cookies, it is possible to track a person. To determine the group membership of a user, we leverage well-known web browser history stealing attacks. Thus, whenever a social network user visits a malicious website, this website can launch our de-anonymization attack and learn the identity of its visitors. The implications of our attack are manifold, since it requires a low effort and has the potential to affect millions of social networking users. We perform both a theoretical analysis and empirical measurements to demonstrate the feasibility of our attack against Xing, a medium-sized social network with more than eight million members that is mainly used for business relationships. Furthermore, we explored other, larger social networks and performed experiments that suggest that users of Facebook and LinkedIn are equally vulnerable.},
author = {Wondracek, Gilbert and Holz, Thorsten and Kirda, Engin and Kruegel, Christopher},
doi = {10.1109/SP.2010.21},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
pages = {223--238},
title = {{A practical attack to de-anonymize social network users}},
year = {2010}
}
@article{Qian2010,
abstract = {Spam is increasingly accepted as a problem associated with compromised hosts or email accounts. This problem not only makes the tracking of spam sources difficult but also enables a massive amount of illegitimate or unwanted emails to be disseminated quickly. Various attempts have been made to analyze, backtrack, detect, and prevent spam using both network as well as content characteristics. However, relatively less attention has been given to understanding how spammers actually carry out their spamming activities from a network angle. Spammers{\{}{\&}{\}}amp;{\{}{\#}{\}}x02019; network behavior has significant impact on spammers{\{}{\&}{\}}amp;{\{}{\#}{\}}x02019; common goal, sending spam in a stealthy and efficient manner. Our work thoroughly investigates a fairly unknown spamming technique we name as triangular spamming that exploits routing irregularities of spoofed IP packets. It is highly stealthy and efficient in that triangular spamming enables 1) exploiting bandwidth diversity of botnet hosts to carry out spam campaigns effectively without divulging precious high-bandwidth hosts and 2) bypassing the current SMTP traffic blocking policies. Despite its relative obscurity, its use has been confirmed by the network operator community. Through carefully devised probing techniques and actual deployment of triangular spamming on Planetlab (a wide-area distributed testbed), we investigate the feasibility, impact of triangular spamming and propose practical detection and prevention methods. From our probing experiments, we found that 97{\{}{\&}{\}}amp;{\{}{\#}{\}}x025; of the networks which block outbound SMTP traffic are vulnerable to triangular spamming and only 44{\{}{\&}{\}}amp;{\{}{\#}{\}}x025; of them are listed on Spamhaus Policy Blocking List (PBL).},
author = {Qian, Zhiyun and Mao, Z Morley and Xie, Yinglian and Yu, Fang},
doi = {10.1109/SP.2010.42},
isbn = {978-1-4244-6894-2},
issn = {10816011},
journal = {s{\&}p},
pages = {207--222},
title = {{Investigation of triangular spamming: A stealthy and efficient spamming technique}},
year = {2010}
}
@article{Schwartz2010,
abstract = {Dynamic taint analysis and forward symbolic execution are quickly becoming staple techniques in security analyses. Example applications of dynamic taint analysis and forward symbolic execution include malware analysis, input filter generation, test case generation, and vulnerability discovery. Despite the widespread usage of these two techniques, there has been little effort to formally define the algorithms and summarize the critical issues that arise when these techniques are used in typical security contexts. The contributions of this paper are two-fold. First, we precisely describe the algorithms for dynamic taint analysis and forward symbolic execution as extensions to the run-time semantics of a general language. Second, we highlight important implementation choices, common pitfalls, and considerations when using these techniques in a security context.},
author = {Schwartz, Edward J and Avgerinos, Thanassis and Brumley, David},
doi = {10.1109/SP.2010.26},
isbn = {978-1-4244-6894-2},
issn = {10816011},
journal = {s{\&}p},
keywords = {Dynamic analysis,Symbolic execution,Taint analysis},
pages = {317--331},
pmid = {1511570},
title = {{All You Ever Wanted to Know about Dynamic Taint Analysis and Forward Symbolic Execution (but Might Have Been Afraid to Ask)}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5504796},
year = {2010}
}
@article{Murdoch2010,
abstract = {EMV is the dominant protocol used for smart card payments worldwide, with over 730 million cards in circulation. Known to bank customers as {\{}{\&}{\}}amp;{\{}{\#}{\}}x0201C;Chip and PIN{\{}{\&}{\}}amp;{\{}{\#}{\}}x0201D;, it is used in Europe; it is being introduced in Canada; and there is pressure from banks to introduce it in the USA too. EMV secures credit and debit card transactions by authenticating both the card and the customer presenting it through a combination of cryptographic authentication codes, digital signatures, and the entry of a PIN. In this paper we describe and demonstrate a protocol flaw which allows criminals to use a genuine card to make a payment without knowing the card{\{}{\&}{\}}amp;{\{}{\#}{\}}x02019;s PIN, and to remain undetected even when the merchant has an online connection to the banking network. The fraudster performs a man-in-the-middle attack to trick the terminal into believing the PIN verified correctly, while telling the card that no PIN was entered at all. The paper considers how the flaws arose, why they remained unknown despite EMV{\{}{\&}{\}}amp;{\{}{\#}{\}}x02019;s wide deployment for the best part of a decade, and how they might be fixed. Because we have found and validated a practical attack against the core functionality of EMV, we conclude that the protocol is broken. This failure is significant in the field of protocol design, and also has important public policy implications, in light of growing reports of fraud on stolen EMV cards. Frequently, banks deny such fraud victims a refund, asserting that a card cannot be used without the correct PIN, and concluding that the customer must be grossly negligent or lying. Our attack can explain a number of these cases, and exposes the need for further research to bridge the gap between the theoretical and practical security of bank payment systems. It also demonstrates the need for the next version of EMV to be engineered properly.},
author = {Murdoch, Steven J and Drimer, Saar and Anderson, Ross and Bond, Mike},
doi = {10.1109/SP.2010.33},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
keywords = {Authentication,Bank security,Card fraud,Chip and PIN,EMV,Protocol failure,Security economics},
pages = {433--446},
title = {{Chip and PIN is broken}},
year = {2010}
}
@article{Bau2010,
abstract = {Black-box web application vulnerability scanners are automated tools that probe web applications for security vulnerabilities. In order to assess the current state of the art, we obtained access to eight leading tools and carried out a study of: (i) the class of vulnerabilities tested by these scanners, (ii) their effectiveness against target vulnerabilities, and (iii) the relevance of the target vulnerabilities to vulnerabilities found in the wild. To conduct our study we used a custom web application vulnerable to known and projected vulnerabilities, and previous versions of widely used web applications containing known vulnerabilities. Our results show the promise and effectiveness of automated tools, as a group, and also some limitations. In particular, "stored" forms of Cross Site Scripting (XSS) and SQL Injection (SQLI) vulnerabilities are not currently found by many tools. Because our goal is to assess the potential of future research, not to evaluate specific vendors, we do not report comparative data or make any recommendations about purchase of specific tools.},
author = {Bau, Jason and Bursztein, Elie and Gupta, Divij and Mitchell, John},
doi = {10.1109/SP.2010.27},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
keywords = {Black box testing,Security standards compliance,Vulnerability detection,Web application security},
pages = {332--345},
title = {{State of the art: Automated black-box web application vulnerability testing}},
year = {2010}
}
@article{Chan2010,
abstract = {We consider resource-constrained broadcast authentication for {\{}{\$}{\}}n{\{}{\$}{\}} receivers in a static, known network topology. There are only two known broadcast authentication protocols that do not use asymmetric cryptography, one-time signatures, multi-receiver MACs, or time synchronization [1], [2]. Both these protocols require three passes of a message front traversing the network. We investigate whether this amount of interaction can be improved efficiently for specific common topology classes, namely, linear topologies, tree topologies and fully connected topologies. We show modifications to the protocols allowing them to complete in just two passes in the linear and fully connected cases with a small constant factor increase in per-node communication overhead, and a further optimization that achieves the equivalent of just a single pass in the linear case with {\{}{\$}{\}}O(log n){\{}{\$}{\}} increase in per-node communication overhead. We also prove new lower bounds for round complexity, or the maximum number of consecutive interactions in a protocol. We show that protocols with efficient per-node communication overhead (polylogarithmic in {\{}{\$}{\}}n{\{}{\$}{\}}) must require at least {\{}{\$}{\}}2log n{\{}{\$}{\}} rounds in any topology; this implies that our two-pass protocol in the fully-connected topology requires the fewest possible passes, and this bound is asymptotically tight for the full-duplex communication model. Furthermore, we show that communication-efficient protocols must take asymptotically more than {\{}{\$}{\}}2log n{\{}{\$}{\}} rounds on trees; this implies that that there are some tree topologies for which two passes do not suffice and the existing three-pass algorithms may be optimal.},
author = {Chan, Haowen and Perrig, Adrian},
doi = {10.1109/SP.2010.22},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
keywords = {Broadcast authentication,Fully connected topology,Linear topology,Multicast authentication,Path topology},
pages = {257--272},
title = {{Round-efficient broadcast authentication protocols for fixed topology classes}},
year = {2010}
}
@article{Comparetti2010,
abstract = {To handle the growing flood of malware, security vendors and analysts rely on tools that automatically identify and analyze malicious code. Current systems for automated malware analysis typically follow a dynamic approach, executing an unknown program in a controlled environment (sandbox) and recording its runtime behavior. Since dynamic analysis platforms directly run malicious code, they are resilient to popular malware defense techniques such as packing and code obfuscation. Unfortunately, in many cases, only a small subset of all possible malicious behaviors is observed within the short time frame that a malware sample is executed. To mitigate this issue, previous work introduced techniques such as multi-path or forced execution to increase the coverage of dynamic malware analysis. Unfortunately, using these techniques is potentially expensive, as the number of paths that require analysis can grow exponentially. In this paper, we propose Reanimator, a novel solution to determine the capabilities (malicious functionality) of malware programs. Our solution is based on the insight that we can leverage behavior observed while dynamically executing a specific malware sample to identify similar functionality in other programs. More precisely, when we observe malicious actions during dynamic analysis, we automatically extract and model the parts of the malware binary that are responsible for this behavior. We then leverage these models to check whether similar code is present in other samples. This allows us to statically identify dormant functionality (functionality that is not observed during dynamic analysis) in malicious programs. We evaluate our approach on thousands of real-world malware samples, and we show that our system is successful in identifying additional, malicious functionality. As a result, our approach can significantly improve the coverage of malware analysis results.},
author = {Comparetti, Paolo Milani and Salvaneschi, Guido and Kirda, Engin and Kolbitsch, Clemens and Kruegel, Christopher and Zanero, Stefano},
doi = {10.1109/SP.2010.12},
isbn = {978-1-4244-6894-2},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Assembly,Computational modeling,Computer architecture,Digital signal processing,Digital signal processing chips,Educational institutions,Large scale integration,Logic,Registers,Telecommunication control,binary analysis,dormant functionality,malware analysis},
pages = {61--76},
title = {{Identifying Dormant Functionality in Malware Programs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5504706},
year = {2010}
}
@article{Kolbitsch2010,
abstract = {Unfortunately, malicious software is still an unsolved problem and a major threat on the Internet. An important component in the fight against malicious software is the analysis of malware samples: Only if an analyst understands the behavior of a given sample, she can design appropriate countermeasures. Manual approaches are frequently used to analyze certain key algorithms, such as downloading of encoded updates, or generating new DNS domains for command and control purposes. In this paper, we present a novel approach to automatically extract, from a given binary executable, the algorithm related to a certain activity of the sample. We isolate and extract these instructions and generate a so-called gadget, i.e., a stand-alone component that encapsulates a specific behavior. We make sure that a gadget can autonomously perform a specific task by including all relevant code and data into the gadget such that it can be executed in a self-contained fashion. Gadgets are useful entities in analyzing malicious software: In particular, they are valuable for practitioners, as understanding a certain activity that is embedded in a binary sample (e.g., the update function) is still largely a manual and complex task. Our evaluation with several real-world samples demonstrates that our approach is versatile and useful in practice.},
author = {Kolbitsch, Clemens and Holz, Thorsten and Kruegel, Christopher and Kirda, Engin},
doi = {10.1109/SP.2010.10},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
pages = {29--44},
title = {{Inspector gadget: Automated extraction of proprietary gadgets from malware binaries}},
year = {2010}
}
@article{Mccune2010,
abstract = {TrustVisor},
author = {Mccune, Jonathan M and Qu, Ning and Li, Yanlin and Datta, Anupam and Gligor, Virgil D and Perrig, Adrian and Zhou, Zongwei},
doi = {10.1109/SP.2010.17},
isbn = {978-1-4244-6894-2},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {DRTM-like TRTM,Hypervisor protect app (PAL),app-level hypercall},
title = {{TrustVisor : Efficient TCB Reduction and Attestation TrustVisor : Efficient TCB Reduction and Attestation ∗}},
year = {2010}
}
@article{Wang2010a,
abstract = {Virtualization is being widely adopted in today{\{}{\&}{\}}amp;{\{}{\#}{\}}x02019;s computing systems. Its unique security advantages in isolating and introspecting commodity OSes as virtual machines (VMs) have enabled a wide spectrum of applications. However, a common, fundamental assumption is the presence of a trustworthy hypervisor. Unfortunately, the large code base of commodity hypervisors and recent successful hypervisor attacks (e.g., VM escape) seriously question the validity of this assumption. In this paper, we present HyperSafe, a lightweight approach that endows existing Type-I bare-metal hypervisors with a unique self-protection capability to provide lifetime control flow integrity. Specifically, we propose two key techniques. The first one, non-bypassable memory lockdown, reliably protects the hypervisor{\{}{\&}{\}}amp;{\{}{\#}{\}}x02019;s code and static data from being compromised even in the presence of exploitable memory corruption bugs (e.g., buffer overflows), therefore successfully providing hypervisor code integrity. The second one, restricted pointer indexing, introduces one layer of indirection to convert the control data into pointer indexes. These pointer indexes are restricted such that the corresponding call/return targets strictly follow the hypervisor control flow graph, hence expanding protection to control-flow integrity. We have built a prototype and used it to protect two open-source Type-I hypervisors: BitVisor and Xen. The experimental results with synthetic hypervisor exploits and benchmarking programs show HyperSafe can reliably enable the hypervisor self-protection and provide the integrity guarantee with a small performance overhead.},
author = {Wang, Zhi and Jiang, Xuxian},
doi = {10.1109/SP.2010.30},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
pages = {380--395},
title = {{HyperSafe: A lightweight approach to provide lifetime hypervisor control-flow integrity}},
year = {2010}
}
@article{Hicks2010,
abstract = {The computer systems security arms race between attackers and defenders has largely taken place in the domain of software systems, but as hardware complexity and design processes have evolved, novel and potent hardware-based security threats are now possible. This paper presents a hybrid hardware/software approach to defending against malicious hardware. We propose BlueChip, a defensive strategy that has both a design-time component and a runtime component. During the design verification phase, BlueChip invokes a new technique, unused circuit identification (UCI), to identify suspicious circuitry{\{}{\&}{\}}amp;{\{}{\#}{\}}x02014;those circuits not used or otherwise activated by any of the design verification tests. BlueChip removes the suspicious circuitry and replaces it with exception generation hardware. The exception handler software is responsible for providing forward progress by emulating the effect of the exception generating instruction in software, effectively providing a detour around suspicious hardware. In our experiments, BlueChip is able to prevent all hardware attacks we evaluate while incurring a small runtime overhead.},
author = {Hicks, Matthew and Finnicum, Murph and King, Samuel T and Martin, M M K and Smith, Jonathan M},
doi = {10.1109/SP.2010.18},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
pages = {159--172},
title = {{Overcoming an untrusted computing base: Detecting and removing malicious hardware automatically}},
year = {2010}
}
@article{Bursztein2010,
abstract = {Captchas are designed to be easy for humans but hard for machines. However, most recent research has focused only on making them hard for machines. In this paper, we present what is to the best of our knowledge the first large scale evaluation of captchas from the human perspective, with the goal of assessing how much friction captchas present to the average user. For the purpose of this study we have asked workers from Amazon{\{}{\&}{\}}amp;{\{}{\#}{\}}x02019;s Mechanical Turk and an underground captchabreaking service to solve more than 318 000 captchas issued from the 21 most popular captcha schemes (13 images schemes and 8 audio scheme). Analysis of the resulting data reveals that captchas are often difficult for humans, with audio captchas being particularly problematic. We also find some demographic trends indicating, for example, that non-native speakers of English are slower in general and less accurate on English-centric captcha schemes. Evidence from a week{\{}{\&}{\}}amp;{\{}{\#}{\}}x02019;s worth of eBay captchas (14,000,000 samples) suggests that the solving accuracies found in our study are close to real-world values, and that improving audio captchas should become a priority, as nearly 1{\{}{\&}{\}}amp;{\{}{\#}{\}}x025; of all captchas are delivered as audio rather than images. Finally our study also reveals that it is more effective for an attacker to use Mechanical Turk to solve captchas than an underground service.},
author = {Bursztein, Elie and Bethard, Steven and Fabry, Celine and Mitchell, John C and Jurafsky, Dan},
doi = {10.1109/SP.2010.31},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
pages = {399--413},
title = {{How good are humans at solving CAPTCHAs? A large scale evaluation}},
year = {2010}
}
@article{Rocha2010,
abstract = {Simple non-interference is too restrictive for specifying and enforcing information flow policies in most programs. Exceptions to non-interference are provided using declassification policies. Several approaches for enforcing declassification have been proposed in the literature. In most of these approaches, the declassification policies are embedded in the program itself or heavily tied to the variables in the program being analyzed, thereby providing little separation between the code and the policy. Consequently, the previous approaches essentially require that the code be trusted, since to trust that the correct policy is being enforced, we need to trust the source code. In this paper, we propose a novel framework in which declassification policies are related to the source code being analyzed via its I/O channels. The framework supports many of the of declassification policies identified in the literature. Based on flow-based static analysis, it represents a first step towards a new approach that can be applied to untrusted and legacy source code to automatically verify that the analyzed program complies with the specified declassification policies. The analysis works by constructing a conservative approximation of expressions over input channel values that could be output by the program, and by determining whether all such expressions satisfy the declassification requirements stated in the policy. We introduce a representation of such expressions that resembles tree automata. We prove that if a program is considered safe according to our analysis then it satisfies a property we call Policy Controlled Release, which formalizes information-flow correctness according to our notion of declassification policy. We demonstrate, through examples, that our approach works for several interesting and useful declassification policies, including one involving declassification of the average of several confidential values.},
author = {Rocha, Bruno P S and Bandhakavi, Sruthi and {Den Hartog}, Jerry and Winsborough, William H and Etalle, Sandro},
doi = {10.1109/SP.2010.14},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
keywords = {Data flow analysis,Languages,Security,Software verification and validation},
pages = {93--108},
title = {{Towards static flow-based declassification for legacy and untrusted programs}},
year = {2010}
}
@article{Fredrikson2010,
abstract = {Fueled by an emerging underground economy, malware authors are exploiting vulnerabilities at an alarming rate. To make matters worse, obfuscation tools are commonly available, and much of the malware is open source, leading to a huge number of variants. Behavior-based detection techniques are a promising solution to this growing problem. However, these detectors require precise specifications of malicious behavior that do not result in an excessive number of false alarms. In this paper, we present an automatic technique for extracting optimally discriminative specifications, which uniquely identify a class of programs. Such a discriminative specification can be used by a behavior-based malware detector. Our technique, based on graph mining and concept analysis, scales to large classes of programs due to probabilistic sampling of the specification space. Our implementation, called Holmes, can synthesize discriminative specifications that accurately distinguish between programs, sustaining an 86{\{}{\%}{\}} detection rate on new, unknown malware, with 0 false positives, in contrast with 55{\{}{\%}{\}} for commercial signature-based antivirus (AV) and 62-64{\{}{\%}{\}} for behavior-based AV (commercial or research).},
author = {Fredrikson, Matt and Jha, Somesh and Christodorescu, Mihai and Sailer, Reiner and Yan, Xifeng},
doi = {10.1109/SP.2010.11},
isbn = {978-1-4244-6894-2},
issn = {1081-6011},
journal = {s{\&}p},
keywords = {Art,Computer hacking,Computer science,Computer security,Credit cards,Detectors,Humans,Internet,Malware,Privacy,Probabilistic Optimization,Sampling methods,Software Security,Specification},
pages = {45--60},
title = {{Synthesizing Near-Optimal Malware Specifications from Suspicious Behaviors}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5504788},
year = {2010}
}
@article{Chen2010,
abstract = {With software-as-a-service becoming mainstream, more and more applications are delivered to the client through the Web. Unlike a desktop application, a web application is split into browser-side and server-side components. A subset of the application{\{}{\&}{\}}amp;{\{}{\#}{\}}x02019;s internal information flows are inevitably exposed on the network. We show that despite encryption, such a side-channel information leak is a realistic and serious threat to user privacy. Specifically, we found that surprisingly detailed sensitive information is being leaked out from a number of high-profile, top-of-the-line web applications in healthcare, taxation, investment and web search: an eavesdropper can infer the illnesses/medications/surgeries of the user, her family income and investment secrets, despite HTTPS protection; a stranger on the street can glean enterprise employees' web search queries, despite WPA/WPA2 Wi-Fi encryption. More importantly, the root causes of the problem are some fundamental characteristics of web applications: stateful communication, low entropy input for better interaction, and significant traffic distinctions. As a result, the scope of the problem seems industry-wide. We further present a concrete analysis to demonstrate the challenges of mitigating such a threat, which points to the necessity of a disciplined engineering practice for side-channel mitigations in future web application developments.},
author = {Chen, Shuo and Wang, Rui and Wang, XiaoFeng and Zhang, Kehuan},
doi = {10.1109/SP.2010.20},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
keywords = {Ambiguity set,Encrypted traffic,Padding,Side-channel-leak,Software-as-a-Service (saas),Web application},
pages = {191--206},
title = {{Side-channel leaks in web applications: A reality today, a challenge tomorrow}},
year = {2010}
}
@article{Garg2010,
abstract = {We present the design and implementation of PCFS, a file system that adapts proof-carrying authorization to provide direct, rigorous, and efficient enforcement of dynamic access policies. The keystones of PCFS are a new authorization logic BL that supports policies whose consequences may change with both time and system state, and a rigorous enforcement mechanism that combines proof verification with conditional capabilities. We prove that our enforcement using capabilities is correct, and evaluate our design through performance measurements and a case study.},
author = {Garg, Deepak and Pfenning, Frank},
doi = {10.1109/SP.2010.28},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
keywords = {Access control,File system,Logic,Proof-carrying authorization},
pages = {349--364},
title = {{A proof-carrying file system}},
year = {2010}
}
@article{Meyerovich2010,
abstract = {Much of the power of modern Web comes from the ability of a Web page to combine content and JavaScript code from disparate servers on the same page. While the ability to create such mash-ups is attractive for both the user and the developer because of extra functionality, code inclusion effectively opens the hosting site up for attacks and poor programming practices within every JavaScript library or API it chooses to use. In other words, expressiveness comes at the price of losing control. To regain the control, it is therefore valuable to provide means for the hosting page to restrict the behavior of the code that the page may include. This paper presents ConScript, a client-side advice implementation for security, built on top of Internet Explorer 8. ConScript allows the hosting page to express fine-grained application-specific security policies that are enforced at runtime. In addition to presenting 17 widely-ranging security and reliability policies that ConScript enables, we also show how policies can be generated automatically through static analysis of server-side code or runtime analysis of client-side code. We also present a type system that helps ensure correctness of ConScript policies. To show the practicality of ConScript in a range of settings, we compare the overhead of ConScript enforcement and conclude that it is significantly lower than that of other systems proposed in the literature, both on micro-benchmarks as well as large, widely-used applications such as MSN, GMail, Google Maps, and Live Desktop.},
author = {Meyerovich, Leo a. and Livshits, Benjamin},
doi = {10.1109/SP.2010.36},
isbn = {978-1-4244-6894-2},
issn = {10816011},
journal = {s{\&}p},
keywords = {-javascript,1,an included library might,aspects,browsers,drastically redefining the behavior,hijacking attack,instance,language security,of,perform a prototype,security policies,web and client-side programming},
pages = {481--496},
title = {{ConScript: Specifying and Enforcing Fine-Grained Security Policies for JavaScript in the Browser}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5504806},
year = {2010}
}
@article{Saxena2010,
abstract = {As AJAX applications gain popularity, client-side JavaScript code is becoming increasingly complex. However, few automated vulnerability analysis tools for JavaScript exist. In this paper, we describe the first system for exploring the execution space of JavaScript code using symbolic execution. To handle JavaScript code{\{}{\&}{\}}amp;{\{}{\#}{\}}x02019;s complex use of string operations, we design a new language of string constraints and implement a solver for it. We build an automatic end-to-end tool, Kudzu, and apply it to the problem of finding client-side code injection vulnerabilities. In experiments on 18 live web applications, Kudzu automatically discovers 2 previously unknown vulnerabilities and 9 more that were previously found only with a manually-constructed test suite.},
author = {Saxena, Prateek and Akhawe, Devdatta and Hanna, Steve and Mao, Feng and McCamant, Stephen and Song, Dawn},
doi = {10.1109/SP.2010.38},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
keywords = {String decision procedures,Symbolic execution,Web security},
pages = {513--528},
title = {{A symbolic execution framework for JavaScript}},
year = {2010}
}
@article{Singh2010,
author = {Singh, Kapil and Moshchuk, Alexander and Wang, Helen J H J and Lee, Wenke},
doi = {10.1109/SP.2010.35},
isbn = {978-1-4244-6894-2},
journal = {s{\&}p},
pages = {463--478},
title = {{On the Incoherencies in Web Browser Access Control Policies}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/SP.2010.35$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5504803},
year = {2010}
}
@article{Koscher2010,
abstract = {Modern automobiles are no longer mere mechanical devices; they are pervasively monitored and controlled by dozens of digital computers coordinated via internal vehicular networks. While this transformation has driven major advancements in efficiency and safety, it has also introduced a range of new potential risks. In this paper we experimentally evaluate these issues on a modern automobile and demonstrate the fragility of the underlying system structure. We demonstrate that an attacker who is able to infiltrate virtually any Electronic Control Unit (ECU) can leverage this ability to completely circumvent a broad array of safety-critical systems. Over a range of experiments, both in the lab and in road tests, we demonstrate the ability to adversarially control a wide range of automotive functions and completely ignore driver inputdash including disabling the brakes, selectively braking individual wheels on demand, stopping the engine, and so on. We find that it is possible to bypass rudimentary network security protections within the car, such as maliciously bridging between our car's two internal subnets. We also present composite attacks that leverage individual weaknesses, including an attack that embeds malicious code in a car's telematics unit and that will completely erase any evidence of its presence after a crash. Looking forward, we discuss the complex challenges in addressing these vulnerabilities while considering the existing automotive ecosystem.},
author = {Koscher, Karl and Czeskis, Alexei and Roesner, Franziska and Patel, Shwetak and Kohno, Tadayoshi and Checkoway, Stephen and McCoy, Damon and Kantor, Brian and Anderson, Danny and Snach{\'{a}}m, Hovav and Savage, Stefan},
doi = {10.1109/SP.2010.34},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
keywords = {Automobiles,Communication standards,Communication system security,Computer security,Data buses},
pages = {447--462},
title = {{Experimental security analysis of a modern automobile}},
year = {2010}
}
@article{Osadchy2010,
abstract = {We introduce SCiFI, a system for Secure Computation of Face Identification. The system performs face identification which compares faces of subjects with a database of registered faces. The identification is done in a secure way which protects both the privacy of the subjects and the confidentiality of the database. A specific application of SCiFI is reducing the privacy impact of camera based surveillance. In that scenario, SCiFI would be used in a setting which contains a server which has a set of faces of suspects, and client machines which might be cameras acquiring images in public places. The system runs a secure computation of a face recognition algorithm, which identifies if an image acquired by a client matches one of the suspects, but otherwise reveals no information to neither of the parties. Our work includes multiple contributions in different areas: A new face identification algorithm which is unique in having been specifically designed for usage in secure computation. Nonetheless, the algorithm has face recognition performance comparable to that of state of the art algorithms. We ran experiments which show the algorithm to be robust to different viewing conditions, such as illumination, occlusions, and changes in appearance (like wearing glasses). A secure protocol for computing the new face recognition algorithm. In addition, since our goal is to run an actual system, considerable effort was made to optimize the protocol and minimize its online latency. A system - SCiFI, which implements a secure computation of the face identification protocol. Experiments which show that the entire system can run in near real-time: The secure computation protocol performs a preprocessing of all public-key cryptographic operations. Its online performance therefore mainly depends on the speed of data communication, and our experiments show it to be extremely efficient.},
author = {Osadchy, Margarita and Pinkas, Benny and Jarrous, Ayman and Moskovich, Boaz},
doi = {10.1109/SP.2010.39},
isbn = {9780769540351},
issn = {10816011},
journal = {s{\&}p},
keywords = {Face recognition,Privacy,Secure computation},
pages = {239--254},
title = {{SCiFI - A System for Secure Face Identification}},
year = {2010}
}
@article{Goodrich2009,
abstract = {In this paper, we study the degree to which a genomic string, {\{}{\$}{\}}Q{\{}{\$}{\}}, leaks$\backslash$ndetails about itself any time it engages in comparison protocols with a genomic$\backslash$nquerier, Bob, even if those protocols are cryptographically guaranteed to$\backslash$nproduce no additional information other than the scores that assess the degree$\backslash$nto which {\{}{\$}{\}}Q{\{}{\$}{\}} matches strings offered by Bob. We show that such scenarios allow$\backslash$nBob to play variants of the game of Mastermind with {\{}{\$}{\}}Q{\{}{\$}{\}} so as to learn the$\backslash$ncomplete identity of {\{}{\$}{\}}Q{\{}{\$}{\}}. We show that there are a number of efficient$\backslash$nimplementations for Bob to employ in these Mastermind attacks, depending on$\backslash$nknowledge he has about the structure of {\{}{\$}{\}}Q{\{}{\$}{\}}, which show how quickly he can$\backslash$ndetermine {\{}{\$}{\}}Q{\{}{\$}{\}}. Indeed, we show that Bob can discover {\{}{\$}{\}}Q{\{}{\$}{\}} using a number of$\backslash$nrounds of test comparisons that is much smaller than the length of {\{}{\$}{\}}Q{\{}{\$}{\}}, under$\backslash$nvarious assumptions regarding the types of scores that are returned by the$\backslash$ncryptographic protocols and whether he can use knowledge about the distribution$\backslash$nthat {\{}{\$}{\}}Q{\{}{\$}{\}} comes from, e.g., using public knowledge about the properties of human$\backslash$nDNA. We also provide the results of an experimental study we performed on a$\backslash$ndatabase of mitochondrial DNA, showing the vulnerability of existing real-world$\backslash$nDNA data to the Mastermind attack.},
archivePrefix = {arXiv},
arxivId = {0904.4458v1},
author = {Goodrich, Michael T},
doi = {10.1109/SP.2009.4},
eprint = {0904.4458v1},
isbn = {978-0-7695-3633-0},
issn = {10816011},
journal = {s{\&}p},
keywords = {attacks,genomic databases,mastermind,mitochondrial dna,privacy},
pages = {1--24},
title = {{The Mastermind Attack on Genomic Data}},
year = {2009}
}
@article{Clarkson2009,
abstract = {We develop a novel technique for authenticating physical documents by using random, naturally occurring imperfections in paper texture. To this end, we devised a new method for measuring the three-dimensional surface of a paper without modifying the document in any way, using only a commodity scanner. From this physical feature, we generate a concise fingerprint that uniquely identifies the document. Our method is secure against counterfeiting, robust to harsh handling, and applicable even before any content is printed on a page. It has a wide range of applications, including detecting forged currency and tickets, authenticating passports, and halting counterfeit goods. On a more sinister note, document identification could be used to de-anonymize printed surveys and to compromise the secrecy of paper ballots.},
author = {Clarkson, William and Weyrich, Tim and Finkelstein, Adam and Heninger, Nadia and Halderman, J Alex and Felten, Edward W},
doi = {10.1109/SP.2009.7},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
number = {May},
pages = {301--314},
title = {{Fingerprinting blank paper using commodity scanners}},
year = {2009}
}
@article{Barth2009,
abstract = {Cross-site scripting defenses often focus on HTML documents, neglecting attacks involving the browser's content-sniffing algorithm, which can treat non-HTML content as HTML. Web applications, such as the one that manages this conference, must defend themselves against these attacks or risk authors uploading malicious papers that automatically submit stellar self-reviews. In this paper, we formulate content-sniffing XSS attacks and defenses. We study content-sniffing XSS attacks systematically by constructing high-fidelity models of the content-sniffing algorithms used by four major browsers. We compare these models with Web site content filtering policies to construct attacks. To defend against these attacks, we propose and implement a principled content-sniffing algorithm that provides security while maintaining compatibility. Our principles have been adopted, in part, by Internet Explorer 8 and, in full, by Google Chrome and the HTML 5 working group.},
author = {Barth, Adam and Caballero, Juan and Song, Dawn},
doi = {10.1109/SP.2009.3},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
pages = {360--371},
title = {{Secure content sniffing for web browsers, or how to stop papers from reviewing themselves}},
year = {2009}
}
@article{Datta2009,
author = {Datta, Anupam and Franklin, Jason},
doi = {10.1109/SP.2009.16},
journal = {s{\&}p},
title = {{A Logic of Secure Systems and its Application to Trusted Computing}},
year = {2009}
}
@article{Zanella-Beguelin2009,
abstract = {We present two machine-checked proofs of the existential unforgeability under adaptive chosen-message attacks of the full domain hash signature scheme. These proofs formalize the original argument of Bellare and Rogaway, and an optimal reduction by Coron that provides a tighter bound on the probability of a forgery. Both proofs are developed using CertiCrypt, a general framework to formalize exact security proofs of cryptographic systems in the computational model. Since CertiCrypt is implemented on top of theCoq proof assistant, the proofs are highly trustworthy and can beverified independently and fully automatically.},
author = {Zanella-B{\'{e}}guelin, Santiago and Barthe, Gilles and Gr{\'{e}}goire, Benjamin and Olmedo, Federico},
doi = {10.1109/SP.2009.17},
isbn = {978-0-7695-3633-0},
issn = {10816011},
journal = {s{\&}p},
pages = {237--250},
title = {{Formally Certifying the Security of Digital Signature Schemes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5207648},
year = {2009}
}
@article{Simoens2009,
abstract = {The increasing use of biometrics has given rise to new privacy concerns. Biometric encryption systems have been proposed in order to alleviate such concerns: rather than comparing the biometric data directly, a key is derived from these data and subsequently knowledge of this key is proved. One specific application of biometric encryption is the use of biometric sketches: in this case biometric template data are protected with biometric encryption. We address the question whether one can undermine a user's privacy given access to biometrically encrypted documents, and more in particular, we examine if an attacker can determine whether two documents were encrypted using the same biometric. This is a particular concern for biometric sketches that are deployed in multiple locations: in one scenario the same biometric sketch is deployed everywhere; in a second scenario the same biometric data is protected with two different biometric sketches. We present attacks on template protection schemes that can be described as fuzzy sketches based on error-correcting codes. We demonstrate how to link and reverse protected templates produced by code-offset and bit-permutation sketches.},
author = {Simoens, Koen and Tuyls, Pim and Preneel, Bart},
doi = {10.1109/SP.2009.24},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
pages = {188--203},
title = {{Privacy weaknesses in biometric sketches}},
year = {2009}
}
@article{Comparetti2009,
abstract = {Protocol reverse engineering is the process of extracting application-level specifications for network protocols. Such specifications are very useful in a number of security-related contexts, for example, to perform deep packet inspection and black-box fuzzing, or to quickly understand custom botnet command and control (C{\{}{\&}{\}}amp;amp;C) channels. Since manual reverse engineering is a time-consuming and tedious process, a number of systems have been proposed that aim to automate this task. These systems either analyze network traffic directly or monitor the execution of the application that receives the protocol messages. While previous systems show that precise message formats can be extracted automatically, they do not provide a protocol specification.The reason is that they do not reverse engineer the protocol state machine. In this paper, we focus on closing this gap by presenting a system that is capable of automatically inferring state machines. This greatly enhances the results of automatic protocol reverse engineering, while further reducing the need for human interaction. We extend previous work that focuses on behavior-based message format extraction, and introduce techniques for identifying and clustering different types of messages not only based on their structure, but also according to the impact of each message on server behavior. Moreover, we present an algorithm for extracting the state machine. We have applied our techniques to a number of real-world protocols, including the command and control protocol used by a malicious bot. Our results demonstrate that we are able to extract format specifications for different types of messages and meaningful protocol state machines. We use these protocol specifications to automatically generate input for a stateful fuzzer, allowing us to discover security vulnerabilities in real-world applications.},
author = {Comparetti, Paolo Milani and Wondracek, Gilbert and Kruegel, Christopher and Kirda, Engin},
doi = {10.1109/SP.2009.14},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
pages = {110--125},
title = {{Prospex: Protocol specification extraction}},
year = {2009}
}
@article{Backes2009,
abstract = {Information-flow analysis is a powerful technique for reasoning about the sensitive information exposed by a program during its execution. We present the first automatic method for information-flow analysis that discovers what information is leaked and computes its comprehensive quantitative interpretation. The leaked information is characterized by an equivalence relation on secret artifacts, and is represented by a logical assertion over the corresponding program variables. Our measurement procedure computes the number of discovered equivalence classes and their sizes. This provides a basis for computing a set of quantitative properties, which includes all established information-theoretic measures in quantitative information-flow. Our method exploits an inherent connection between formal models of qualitative information-flow and program verification techniques. We provide an implementation of our method that builds upon existing tools for program verification and information-theoretic analysis. Our experimental evaluation indicates the practical applicability of the presented method.},
author = {Backes, Michael and K{\"{o}}pf, Boris and Rybalchenko, Andrey},
doi = {10.1109/SP.2009.18},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
keywords = {information flow,information theory,program analysis},
pages = {141--153},
title = {{Automatic discovery and quantification of information leaks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5207642},
year = {2009}
}
@article{Sharif2009,
abstract = {Malware authors have recently begun using emulation technology to obfuscate their code. They convert native malware binaries into bytecode programs written in a randomly generated instruction set and paired with a native binary emulator that interprets the bytecode. No existing malware analysis can reliably reverse this obfuscation technique. In this paper, we present the first work in automatic reverse engineering of malware emulators. Our algorithms are based on dynamic analysis. We execute the emulated malware in a protected environment and record the entire x86 instruction trace generated by the emulator. We then use dynamic data-flow and taint analysis over the trace to identify data buffers containing the bytecode program and extract the syntactic and semantic information about the bytecode instruction set. With these analysis outputs, we are able to generate data structures, such as control-flow graphs, that provide the foundation for subsequent malware analysis. We implemented a proof-of-concept system called Rotalume and evaluated it using both legitimate programs and malware emulated by VMProtect and code virtualizer. The results show that Rotalume accurately reveals the syntax and semantics of emulated instruction sets and reconstructs execution paths of original programs from their bytecode representations.},
author = {Sharif, Monirul and Lanzi, Andrea and Giffin, Jonathon and Lee, Wenke},
doi = {10.1109/SP.2009.27},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
pages = {94--109},
title = {{Automatic reverse engineering of malware emulators}},
year = {2009}
}
@article{Chen2009,
abstract = {14회 인용},
author = {Chen, Shuo and Mao, Ziqing and Wang, Yi-Min and Zhang, Ming},
doi = {10.1109/SP.2009.12},
isbn = {978-0-7695-3633-0},
issn = {10816011},
journal = {s{\&}p},
pages = {347--359},
title = {{Pretty-Bad-Proxy; An Overlooked Adversary in Browsers' HTTPS Deployments}},
year = {2009}
}
@article{Coppens2009,
abstract = {This paper studies and evaluates the extent to which automated compiler techniques can defend against timing-based side-channel attacks on modern x86 processors. We study how modern x86 processors can leak timing information through side-channels that relate to control flow and data flow. To eliminate key-dependent control flow and key-dependent timing behavior related to control flow, we propose the use of if-conversion in a compiler backend, and evaluate a proof-of-concept prototype implementation. Furthermore, we demonstrate two ways in which programs that lack key-dependent control flow and key-dependent cache behavior can still leak timing information on modern x86 implementations such as the Intel Core 2 Duo, and propose defense mechanisms against them.},
author = {Coppens, Bart and Verbauwhede, Ingrid and {De Bosschere}, Koen and {De Sutter}, Bjorn},
doi = {10.1109/SP.2009.19},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
pages = {45--60},
title = {{Practical mitigations for timing-based side-channel attacks on modern x86 processors}},
year = {2009}
}
@article{Krohn2009,
abstract = {The Flume system is an implementation of decentralized information flow control (DIFC) at the operating system level. Prior work has shown Flume can be implemented as a practical extension to the Linux operating system, allowing real Web applications to achieve useful security guarantees. However, the question remains if the Flume system is actually secure. This paper compares Flume with other recent DIFC systems like Asbestos, arguing that the latter is inherently susceptible to certain wide-bandwidth covert channels, and proving their absence in Flume by means of a noninterference proof in the communicating sequential processes formalism.},
author = {Krohn, M and Tromer, E},
doi = {10.1109/SP.2009.23},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
pages = {61--76},
title = {{Noninterference for a Practical DIFC-Based Operating System}},
url = {http://ieeexplore.ieee.org/search/freesrchabstract.jsp?tp={\{}{\&}{\}}arnumber=5207637$\backslash$npapers2://publication/doi/10.1109/SP.2009.23},
year = {2009}
}
@article{Cai2009,
abstract = {We defeat two proposed Unix file-system race condition defense mechanisms. First, we attack the probabilistic defense mechanism of Tsafrir, et al., published at USENIX FAST 2008. We then show that the same attack breaks the kernel-based dynamic race detector of Tsyrklevich and Yee, published at USENIX Security 2003. We then argue that all kernel-based dynamic race detectors must have a model of the programs they protect or provide imperfect protection. The techniques we develop for performing these attacks work on multiple Unix operating systems, on uni- and multi-processors, and are useful for exploiting most Unix file-system races. We conclude that programmers should use provably-secure methods for avoiding race conditions when accessing the file-system.},
author = {Cai, Xiang and Gui, Yuwei and Johnson, Rob},
doi = {10.1109/SP.2009.10},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
pages = {27--41},
title = {{Exploiting unix file-system races via algorithmic complexity attacks}},
year = {2009}
}
@article{Parno2009,
abstract = {Providing online access to sensitive data makes Web servers lucrative targets for attackers. A compromise of any of the Web server's scripts, applications, or operating system can leak the sensitive data of millions of customers. Unfortunately, many systems for stopping data leaks require considerable effort from application developers, hindering their adoption. In this work, we investigate how such leaks can be prevented with minimal developer effort. We propose CLAMP, an architecture for preventing data leaks even in the presence of Web server compromises or SQL injection attacks. CLAMP protects sensitive data by enforcing strong access control on user data and by isolating code running on behalf of different users. By focusing on minimizing developer effort, we arrive at an architecture that allows developers to use familiar operating systems, servers, and scripting languages, while making relatively few changes to application code - less than 50 lines in our applications.},
author = {Parno, Bryan and McCune, J M},
doi = {10.1109/SP.2009.21},
isbn = {978-0-7695-3633-0},
issn = {1081-6011},
journal = {s{\&}p},
pages = {154--169},
title = {{CLAMP: Practical prevention of large-scale data leaks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5207643$\backslash$nhttp://ieeexplore.ieee.org/xpls/abs{\{}{\_}{\}}all.jsp?arnumber=5207643},
year = {2009}
}
@article{Yu2009,
author = {Yu, Haifeng and Shi, Chenwei and Kaminsky, Michael and Gibbons, Phillip B and Xiao, Feng},
doi = {10.1109/SP.2009.26},
isbn = {978-0-7695-3633-0},
journal = {s{\&}p},
pages = {283--298},
title = {{DSybil: Optimal Sybil-Resistance for Recommendation Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5207651},
year = {2009}
}
@article{Yee2009,
abstract = {This paper describes the design, implementation and evaluation of Native Client, a sandbox for untrusted x86 native code. Native Client aims to give browser-based applications the computational performance of native applications without compromising safety. Native Client uses software fault isolation and a secure runtime to direct system interaction and side effects through interfaces managed by Native Client. Native Client provides operating system portability for binary code while supporting performance-oriented features generally absent from Web application programming environments, such as thread support, instruction set extensions such as SSE, and use of compiler intrinsics and hand-coded assembler. We combine these properties in an open architecture that encourages community review and 3rd-party tools.},
author = {Yee, Bennet and Sehr, David and Dardyk, Gregory and Chen, J Bradley and Muth, Robert and Ormandy, Tavis and Okasaka, Shiki and Narula, Neha and Fullagar, Nicholas},
doi = {10.1109/SP.2009.25},
isbn = {978-0-7695-3633-0},
issn = {10816011},
journal = {s{\&}p},
pages = {79--93},
title = {{Native Client: A Sandbox for Portable, Untrusted x86 Native Code}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5207638},
year = {2009}
}
@article{Garcia2009,
abstract = {The Mifare Classic is the most widely used contactless smartcard on the market. The stream cipher CRYPTO1 used by the classic has recently been reverse engineered and serious attacks have been proposed. The most serious of them retrieves a secret key in under a second. In order to clone a card, previously proposed attacks require that the adversary either has access to an eavesdropped communication session or executes a message-by-message man-in-the-middle attack between the victim and a legitimate reader. Although this is already disastrous from a cryptographic point of view, system integrators maintain that these attacks cannot be performed undetected.This paper proposes four attacks that can be executed by an adversary having only wireless access to just a card (and not to a legitimate reader). The most serious of them recovers a secret key in less than a second on ordinary hardware. Besides the cryptographic weaknesses, we exploit other weaknesses in the protocol stack. A vulnerability in the computation of parity bits allows an adversary to establish a side channel. Another vulnerability regarding nested authentications provides enough plaintext for a speedy known-plaintext attack.},
author = {Garcia, Flavio D and {Van Rossum}, Peter and Verdult, Roel and Schreur, Ronny Wichers},
doi = {10.1109/SP.2009.6},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
pages = {3--15},
title = {{Wirelessly pickpocketing a Mifare Classic card}},
year = {2009}
}
@article{Backes2009a,
abstract = {Reflecting objects such as tea pots and glasses, but also diffusely reflecting objects such as a user's shirt, can be used to spy on confidential data displayed on a monitor. First, we show how reflections in the user's eye can be exploited for spying on confidential data. Second, we investigate to what extent monitor images can be reconstructed from the diffuse reflections on a wall or the user's clothes, and provide information-theoretic bounds limiting this type of attack. Third, we evaluate the effectiveness of several countermeasures. This substantially improves previous work (Backes et al., IEEE Symposium on Security {\{}{\&}{\}}amp;amp; Privacy, 2008).},
author = {Backes, Michael and Chen, Tongbo and D{\"{u}}rmuth, Markus and Lensch, Hendrik P a and Welk, Martin},
doi = {10.1109/SP.2009.20},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
number = {ii},
pages = {315--327},
title = {{Tempest in a teapot: Compromising reflections revisited}},
year = {2009}
}
@article{Albrecht2009,
abstract = {This paper presents a variety of plaintext-recovering attacks against SSH. We implemented a proof of concept of our attacks against OpenSSH, where we can verifiably recover 14 bits of plaintext from an arbitrary block of ciphertext with probability 2-14 and 32 bits of plaintext from an arbitrary block of ciphertext with probability 2-18. These attacks assume the default configuration of a 128-bit block cipher operating in CBC mode. The paper explains why a combination of flaws in the basic design of SSH leads implementations such as OpenSSH to be open to our attacks, why current provable security results for SSH do not cover our attacks, and how the attacks can be prevented in practice.},
author = {Albrecht, Martin R and Paterson, Kenneth G and Watson, Gaven J},
doi = {10.1109/SP.2009.5},
isbn = {978-0-7695-3633-0},
issn = {10816011},
journal = {s{\&}p},
pages = {16--26},
title = {{Plaintext Recovery Attacks against SSH}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5207634},
year = {2009}
}
@article{Schechter2009,
abstract = {All four of the most popular webmail providers - AOL, Google, Microsoft, and Yahoo! - rely on personal questions as the secondary authentication secrets used to reset account passwords. The security of these questions has received limited formal scrutiny, almost all of which predates webmail. We ran a user study to measure the reliability and security of the questions used by all four webmail providers. We asked participants to answer these questions and then asked their acquaintances to guess their answers. Acquaintances with whom participants reported being unwilling to share their webmail passwords were able to guess 17{\{}{\%}{\}} of their answers. Participants forgot 20{\{}{\%}{\}} of their own answers within six months. What's more, 13{\{}{\%}{\}} of answers could be guessed within five attempts by guessing the most popular answers of other participants, though this weakness is partially attributable to the geographic homogeneity of our participant pool.},
author = {Schechter, Stuart and Brush, a. J Bernheim and Egelman, Serge},
doi = {10.1109/SP.2009.11},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
pages = {375--390},
title = {{It's no secret Measuring the security and reliability of authentication via 'secret' questions}},
year = {2009}
}
@article{Louw2009,
abstract = {As social networking sites proliferate across the World Wide Web, complex user-created HTML content is rapidly becoming the norm rather than the exception. User-created web content is a notorious vector for cross-site scripting (XSS) attacks that target websites and confidential user data. In this threat climate, mechanisms that render web applications immune to XSS attacks have been of recent research interest. A challenge for these security mechanisms is en- abling web applications to accept complex HTML input from users, while disallowing malicious script content. This challenge is made difficult by anomalous web browser behaviors, which are often used as vectors for successful XSS attacks. Motivated by this problem, we present a new XSS defense strategy designed to be effective in widely deployed existing web browsers, despite anomalous browser behavior. Our approach seeks to minimize trust placed on browsers for interpreting untrusted content. We implemented this approach in a tool called BLUEPRINT that was integrated with several popular web applications. We evaluated BLUEPRINT against a barrage of stress tests that demonstrate strong re- sistance to attacks, excellent compatibility with web browsers and reasonable performance overheads.},
author = {Louw, Mike Ter and Venkatakrishnan, V N},
doi = {10.1109/SP.2009.33},
isbn = {978-0-7695-3633-0},
issn = {10816011},
journal = {s{\&}p},
pages = {331--346},
title = {{Blueprint: Robust Prevention of Cross-site Scripting Attacks for Existing Browsers}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/SP.2009.33$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5207654},
year = {2009}
}
@article{Kusters2009,
abstract = {Coercion resistance is an important and one of the most intricate security requirements of electronic voting protocols. Several definitions of coercion-resistance have been proposed in the literature,including definitions based on symbolic models.However, existing definitions in such models are rather restricted in their scope and quite complex.In this paper, we therefore propose a new definition of coercion resistance in a symbolic setting, based on an epistemic approach. Our definition is relatively simple and intuitive. It allows for a fine-grained formulation of coercion resistance and can be stated independently of a specific, symbolic protocol and adversary model. As a proof of concept,we apply our definition to three voting protocols. In particular, we carry out the first rigorous analysis of the recently proposed Civitas system. We precisely identify those conditions under which this system guarantees coercion resistance or fails to be coercion resistant. We also analyze protocols proposed by Lee et al. and Okamoto.},
archivePrefix = {arXiv},
arxivId = {0903.0802},
author = {K{\"{u}}sters, Ralf and Truderung, Tomasz},
doi = {10.1109/SP.2009.13},
eprint = {0903.0802},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
pages = {251--266},
title = {{An epistemic approach to coercion-resistance for electronic voting protocols}},
year = {2009}
}
@article{Borders2009,
abstract = {As the Internet grows and network bandwidth continues to increase, administrators are faced with the task of keeping confidential information from leaving their networks. Todaypsilas network traffic is so voluminous that manual inspection would be unreasonably expensive. In response, researchers have created data loss prevention systems that check outgoing traffic for known confidential information. These systems stop naive adversaries from leaking data, but are fundamentally unable to identify encrypted or obfuscated information leaks. What remains is a high-capacity pipe for tunneling data to the Internet. We present an approach for quantifying information leak capacity in network traffic. Instead of trying to detect the presence of sensitive data-an impossible task in the general case--our goal is to measure and constrain its maximum volume. We take advantage of the insight that most network traffic is repeated or determined by external information, such as protocol specifications or messages sent by a server. By filtering this data, we can isolate and quantify true information flowing from a computer. In this paper, we present measurement algorithms for the Hypertext Transfer Protocol (HTTP), the main protocol for Web browsing. When applied to real Web browsing traffic, the algorithms were able to discount 98.5{\{}{\%}{\}} of measured bytes and effectively isolate information leaks.},
author = {Borders, Kevin and Prakash, Atul},
doi = {10.1109/SP.2009.9},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
pages = {129--140},
title = {{Quantifying information leaks in outbound web traffic}},
year = {2009}
}
@article{Narayanan2009,
archivePrefix = {arXiv},
arxivId = {0903.3276},
author = {Narayanan, Arvind and Shmatikov, Vitaly},
doi = {10.1109/SP.2009.22},
eprint = {0903.3276},
isbn = {978-0-7695-3633-0},
journal = {s{\&}p},
pages = {173--187},
title = {{De-anonymizing Social Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5207644},
year = {2009}
}
@article{Danezis2009,
abstract = {Sphinx is a cryptographic message format used to relay anonymized messages within a mix network. It is more compact than any comparable scheme, and supports a full set of security features: indistinguishable replies, hiding the path length and relay position, as well as providing unlinkability for each leg of the message's journey over the network. We prove the full cryptographic security of Sphinx in the random oracle model, and we describe how it can be used as an efficient drop-in replacement in deployed remailer systems.},
author = {Danezis, George and Goldberg, Ian},
doi = {10.1109/SP.2009.15},
isbn = {9780769536330},
issn = {10816011},
journal = {s{\&}p},
pages = {269--282},
title = {{Sphinx: A compact and provably secure mix format}},
year = {2009}
}
